<!DOCTYPE html>
<!--
    Plain-Academic by Vasilios Mavroudis
    Released under the  Simplified BSD License/FreeBSD (2-clause) License.
    https://github.com/mavroudisv/plain-academic
-->

<html lang="en">
  <head>
    <title>Achu Wilson</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link
      rel="stylesheet"
      href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.css"
    />
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.0/jquery.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.js"></script>
    
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@700&display=swap" 
          rel="stylesheet"
          type="text/css"
          >

    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-44991178-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-44991178-2');
</script>


  </head>
  <body>
    <!-- Navigation -->
    <nav class="navbar navbar-inverse">
      <div class="container">
        <ul class="nav navbar-nav" style="font-size:22px" >
          <li><a href="#">Home</a></li>
          <li><a href="#education">Education</a></li>
          <li><a href="#experience">Experience</a></li>
          <li><a href="#publications">Publications</a></li>
          <li><a href="#projects">Projects</a></li>
          <li><a href="#blog">Blog</a></li>
          <li><a href="#notes">Notes</a></li>
        </ul>
      </div>
    </nav>

    <!-- Page Content -->
    <div class="container">
      <div class="row">
        <!-- Entries Column -->
        <div class="col-md-8">
          <!-- Main Image -->
          <img class="img-responsive" src="photo_12.jpg" alt="" /><br />

          <div style="margin-top: 3%; text-align: justify" style="font-size:16px">
            
          </div>
        </div>

        <!-- Contact Info on the Sidebar -->
        <div class="col-md-4">
          <div style="font-family: 'Roboto', sans-serif; font-size: 32px">
            <b>Achu Wilson</b>
          </div>
          <br />
          <p style=" font-size:17px">
            <b>achuwilson [at] gmail [dot] com</b><br />
          </p>

        </div>

        <!-- Links on the Sidebar -->
        <div class="col-md-4" style="margin-top: 2%; font-size:17px">
          <dd>
            <a href="https://github.com/achuwilson" target="_blank">GitHub</a>
          </dd>
          <dd>
            <a href="https://www.linkedin.com/in/achuwilson/" target="_blank"
              >LinkedIn</a
            >
          </dd>
          <dd>
            <a
              href="https://scholar.google.com/citations?user=EQnhhaQAAAAJ&hl=en"
              target="_blank"
              >Google Scholar</a
            >
          </dd>
          <dd>
            <a href="https://twitter.com/achuwilson" target="_blank">Twitter</a>
          </dd>
        </div>
      </div>

      
      <div class="row">
        <div class="col-md-12"  style="text-align: justify; font-size:17px">

          <p >
            &emsp; 
              I'm a Roboticist and Applied Scientist at the <a href="https://rai-inst.com/" > Boston Dynamics Robotcs & AI Institute </a>, where I focus on developing advanced robot hands, 
              tactile sensors, and manipulation algorithms.
              Previously I was a graduate student at Carnegie Mellon's RoboTouch Lab, under the guidance of <a href="https://www.ri.cmu.edu/ri-faculty/wenzhen-yuan" >Dr.Wenzhen Yuan</a>.
              My research there centered on using tactile sensing to enable robots to perform complex industrial assembly tasks, such as handling flexible components like cables and inserting connectors.
         </p>

          <p >
            &emsp; Prior to that I was at MIT, where I was advised by <a href="https://www.csail.mit.edu/person/ted-adelson" >Dr.Edward Adelson</a> in the Perception Science Group at CSAIL. My work at MIT focused on developing GelSight sensorized robot hands.
              I've also done research at the Robert Bosch Centre for Cyber-Physical Systems (<a href=" https://cps.iisc.ac.in/" >RBCCPS</a>) at the Indian Institute of Science (<a href="https://iisc.ac.in/">IISc</a>), Bangalore, 
              under the mentorship of <a href=" https://eecs.iisc.ac.in/people/bharadwaj-amrutur/" >Dr.Bharadwaj Amrutur</a>. Here, I led the joint IISc-MIT team in the Kuka Innovation AI Challenge 2021. 
          </p>

          <p >
           I am also the co-founder of <a href="https://sastrarobotics.com/" >Sastra Robotics</a>, which focusses on building robots for automated functional testing of devices. At Sastra, I led the R&D and production teams, designing hardware prototypes, 
              developing real-time embedded systems, and writing robotics and computer vision software that continue to be commercially utilized.
          </p>

          <p>
            &emsp; My expertise includes:

               <ul>
                   <li> Developing controllers for robotic manipulation</li>
                    <li> Real-time embedded systems and low-level hardware controllers </li>
                    <li> Computer vision and machine learning</li>
                    <li>  Python/C++ application development</li>
                    <li> ROS (Robot Operating System), DRAKE</li>
                    <li> Industrial robot programming for UR, Kuka, and Motoman robots</li>
               </ul>
    

When I am not tinkering on my hobby robot projects, I could be found hiking, biking, snowshoeing, reading a book, playing videogames, brewing some wine or experimenting with a new dish in the kitchen!,

          </p>

        </div>
      </div>

      <div class="row">
        <div class="col-md-12">
          <h2 id="news">News</h2>

          <ul style="font-size:17px">
              <li><strong>[March 2025]</strong> Workshop on Acoustic sensing for robots at ICRA-2025 :  <strong><a href = "https://sites.google.com/view/roboacoustics" target="_blank" > RoboAcoustics </strong></a> </li>
              <li><strong>[Feb 2025]</strong> Work on tactile sensor design using physically accurate light simulator published in Nature  </li>
            <li><strong>[July 2023] </strong> I have joined the Boston Dynamics Robotics & AI Institute as a Roboticist/Applied Scientist! </li>
            <li><strong>[Jan 2023] </strong> Our new paper (Cable Routing and Assembly using Tactile-driven Motion Primitives) is accepted for ICRA 2023.</li>
            <li><strong>[Jan 2023] </strong> First stable release of <strong> <a href="https://github.com/achuwilson/easy_ur" target="_blank" > Easy_UR</a></strong>  -  a library to control UR series of robots</li>
          </ul> 

        </div>
      </div>


      <div class="row">
        <div class="col-md-12">
          <h2 id="education">Education</h2>

         <u><strong style="font-size: 16px">Graduate Student, MechE- Robotics and Controls</strong></u>
          <p style="font-size: 16px">
            <strong  style="font-size: 15px"> Carnegie Mellon University (Sep 2021 – May 2023, Pittsburgh, US)</strong>
            <br>Courses: Machine Learning (24-787, also TA'ed), Mechanics of Manipulation (16-741), AI for Manipulation (16-740), Computer Vision (24-678),
            Statistical Techniques/Reinforcement Learning for Robotics(16-831), Robot Cognition for Manipulation(16-890), Optimization (18-660), Bio-Inspired Robotics(24-775).

          </p>

          
          <p style="font-size: 16px">
            <strong  style="font-size: 15px">Massachusetts Institute of Technology (Oct 2018 – Oct 2019, Cambridge, USA)</strong>
            <br>Courses: Introduction to Robotics (2.12), Robotic Manipulation (6.881)
          </p>

          <u><strong style="font-size: 16px" >Bachelors, Electronics and Communication Engineering</strong></u>
          <p style="font-size: 16px">
            <strong  style="font-size: 15px">Govt. Engineering College, Sreekrishnapuram (Calicut University) (Sep 2008 – May 2012, India)</strong>
            <br>Courses: Electronic Circuits, Control Systems, Embedded Systems, Communication Systems
          </p>
        </div>
      </div>

      <div class="row">
        <div class="col-md-12" style="font-size: 17px">
          <h2 id="experience">Experience</h2>
          
          <strong style="font-size: 18px">Graduate Research Assistant </strong>
          <p>
            <strong  style="font-size: 16px"> <i>RoboTouch Group, Robotics Institute, CMU, Sept 2021 – Present, Pittsburgh, US</strong> </i>
            <ul style="margin-top:-10px; line-height:1.3;" >
              <li > Developed tactile driven manipulation primitives and skills for cable routing and assembly tasks.</li>
              <li> Developed fabrication and calibration techniques for curved GelSight fingertip sensors aided by simulation.</li>      
              <li> Implemented tactile (GelSight) based hybrid force - position controllers for UR5 Robots.</li>
              <li> Developed and maintained the manipulation pipeline (including easy_ur) used for various other projects in the lab.</li>      
            </ul>
          </p>

          <strong style="font-size: 18px" >Research Associate</strong>
          <p>
            <strong  style="font-size: 16px"> <i>Robert Bosch Center for Cyber-Physical Systems, Indian Institute of Science, 
              Sept 2020 – Sept 2021, Bangalore, India. </i></strong>
              <ul style="margin-top:-10px; line-height:1.3;" >
              <li> Led Team MIT-IISc on the Kuka Innovation Award - AI Challenge 2021, and made into one of the five finalists worldwide</li>
              <li> Designed and fabricated a two-finger robotic hand featuring GelSight tactile sensing on the inner surface 
                and a close-range proximity sensor array on the outer surfaces. 
                The hand was capable of performing in-hand object rolling, enabling the gathering of tactile data over extended object surfaces.</li>
              <li> Explored reinforcement learning algorithms to explore and pick up
                objects in vision denied environments using tactile & proximity sensing
                data. </li>
              <li>Implemented model free non-prehensile object manipulation techniques on Kuka IIWA robot. </li>
              <li> Trained graduate students on programming Kuka, Motoman robot manipulators and on GelSight 
              fabrication. </li>
            </ul>
          </p>

          <strong style="font-size: 18px">Visiting Research Student/Associate</strong>
          <p>
            <strong  style="font-size: 16px"> <i>Perceptual Science group, CSAIL, Massachusetts Institute of Technology, Oct 2018 – Oct 2019, Cambridge, USA
              </i></strong>
            <ul style="margin-top:-10px; line-height:1.3;" >
            <li> Developed a fully actuated robotic hand with multi-sensory feedback, featuring several GelSight tactile sensors.
               The design encompassed the mechanical system, electronics, and software for data capture and processing. The hand was capable of limited in-hand manipulation.</li>
            </ul>
          </p>

          <strong style="font-size: 18px">Co-Founder & Chief Technology Officer</strong>
          <p>
            <strong  style="font-size: 16px"> <i>Sastra Robotics, Aug 2013 – Sep 2018, Cochin, India
            </i></strong>
            <ul style="margin-top:-10px; line-height:1.3;" >
              <li>  As the technical co-founder, designed and built the drive systems,
                motion controllers and user software for SCARA and 6 axis robots for
                  device testing applications</li>
              <li> Developed  robot force controllers to guarantee the safety of tested 
                devices and a sensor-equipped finger to test the haptic feedback felt on touchscreens.</li>
              <li> Created computer vision systems to analyze device states from LCD
              screens, dials, switches, knobs etc</li>
              <li> These robots reduced device testing time for OEM clients by up to 70%.</li>
              
              <li> Hired, trained, built and managed a team of engineers and technicians who were involved in R&D, product design and production.</li>
              <li> Designed scalable hardware and software architectures.</li>
              <li> Created product roadmaps incorporating market demands through collaboration with customers and incorporating their feedback.</li>
            </ul>
          </p>

          <strong style="font-size: 18px">Robotics Software Engineer</strong>
          <p>
            <strong  style="font-size: 16px"> <i>Asimov Robotics (Energid Technologies), June 2012 – July 2013, Cochin, India
            </i></strong>
            <ul style="margin-top:-10px; line-height:1.3;" >
              <li>Developed ROS & LabVIEW interfaces for Cyton Gamma 7DOF
                manipulator through Energid Actin framework. </li>
              <li> Designed electrical subsystems, motor controllers and sensor interfaces
                for a humanoid social robot.</li>
              <li> Implemented ROS based manipulation pipeline for a custom bi-manual
                dexterous manipulator.</li>
            </ul>
          </p>

          <strong style="font-size: 20px">Service</strong>
          <p>
          <ul style="margin-top:-10px; line-height:1.3;" >
            <li> Reviewer RA-L, IROS, IEEE-RAM, AIR-2019</li>
            <li> Program Committee Member - Advances in Robotics (AIR-2019)</li>
            <li> Voting Member - Bureau of Indian Standards: Automation Systems & Robotics Committee.</li>
            <li> Industry Member in the drafting the Robotics & Automation Roadmap for India, IEEE-SA Industry Connections Workshop 2017.</li>
            <li> Numerous introductory talks/workshops on robotics in colleges in Kerala, India</li>
          </ul>
          </p>


        </div>
      </div>

      <div class="row">
        <div class="col-md-12">
          <h2 id="publications">Publications</h2>

          <!--strong>Selected peer-reviewed papers</strong><br /-->
          <ul  style="font-size: 17px">
              <li class="paper">
                  <strong><a href="https://www.nature.com/articles/s44172-025-00350-4" target="_blank"
                 >Vision-based tactile sensor design using physically based rendering</a></strong>
                  Arpit Agarwal , Achu Wilson , Timothy Man , Edward Adelson , Ioannis Gkioulekas & Wenzhen Yuan - in
                Nature Communications Engineering volume 4, Article number: 21 (2025)
              </li>
            <li class="paper">
              <strong><a href="https://ieeexplore.ieee.org/document/10161069/" target="_blank"
                >Cable Routing and Assembly using Tactile-driven Motion Primitives. </a> </strong>
              Achu Wilson, Helen Jiang, Wenzhao Lian, Wenzhen Yuan - (accepted for
               IEEE International Conference on Robotics & Automation (ICRA), London, 2023)
               <a href="https://youtu.be/_F0kCjBgxD0" target="_blank" > <strong style="color:green;">VIDEO </strong> </a>
            </li>
            <li class="paper">
              <strong> <a href="papers/2019-gelsighthand-iros.pdf" target="_blank"
                > Design of a Fully Actuated Robotic Hand With Multiple Gelsight Tactile Sensors. </a></strong>
               Achu Wilson, Shaoxiong Wang, Branden Romero, Edward Adelson, RoboTac Workshop,
               International Conference on Intelligent Robotics and Systems (IROS), Macau, 2019
               <a href="https://youtu.be/4hxsZ9nHJWI" target="_blank" > <strong style="color:green;">VIDEO </strong> </a>
            </li>
            <li class="paper">
              <strong> <a href="papers/2019-sim2real-icra.pdf" target="_blank"
                > GelSight Simulation for Sim2Real Learning.</a></strong>
              
               Daniel Fernandes Gomes, Achu Wilson, Shan Luo, ViTac Workshop,
               IEEE International Conference on Robotics & Automation (ICRA), Montreal, 2019.
            </li>
            <li class="paper">
              <strong><a href="papers/2017-mr-clutch-icra.pdf" target="_blank"
                > Design and Development of a Magneto-Rheological Linear Clutch for Force
                controlled Human Safe Robots. </a></strong>
              
              Achu Wilson, IEEE International Conference on Robotics & Automation (ICRA), Singapore, 2017.
               <a href="https://youtu.be/ES0q4eb1TYo" target="_blank" >  <strong style="color:green;">VIDEO </strong> </a>
            </li>
            <li class="patent">
              <strong><a href="https://patents.google.com/patent/WO2017051263A2/en" target="_blank"
                > Robot arm for testing of touchscreen applications</a></strong>
              
               Achu Wilson, Aronin P, Akhil A, International Patent, WO2017051263A3
            </li>
          </ul>
        </div>
      </div>

      <div class="row">

        <div class="col-md-12">
          <h2 id="projects">Projects</h2>

          <div class="col-md-12">
            <div class="col-md-4">
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source src="images/pivotmanip.mp4" type="video/mp4"  />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong> Non-Prehensile Pivoting for Manipulation of Heavy Objects</Strong>
                  Inspired by the way humans move big and heavy objects, this project investigated methods for the manipulation of objects heavier than the
                   robot manipulator's payload. Objects were manipulated through pivoting actions that maintain edge or line contacts with the surface.
                  Analytical solution as well as a sampling based MPC controller were developed as part of the CMU 16-741 Mechanics of Manipulation class while reinforcement learning based solutions
                  were studied as a part of the CMU 16-845 RoboStats class. The video shows the robot moving using an analytical solution.
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>


  

          <div class="col-md-12" >
            <div class="col-md-4" >          
            <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
              <source src="images/cableroute.mp4" type="video/mp4"  />
            </video>
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong>Cable Routing and Assembly using Tactile-driven Motion Primitives</strong>

                In this project, we combined tactile-guided low-level motion techniques with high-level
                 vision-based task parsing for routing and assembling cables on a reconfigurable task board.
                  We developed a library of library of tactile-guided
                  motion primitives using a fingertip GelSight sensor. Each primitive 
                  reliably accomplished operations such as following along a cable, wraping it around pivots, weaving through slots, 
                  or inserting a connector.The overall task was inferred through visual perception using a given goal configuration image 
                  which was used to generate the primitive sequence.  <a href="https://youtu.be/_F0kCjBgxD0" target="_blank" > <strong style="color:green;">VIDEO </strong> </a>
                 
                

              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>


          <div class="col-md-12">
            <div class="col-md-4">
              <img class="img-responsive" src="images/easygui.png" alt="" /><br />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <br>
                <strong> <a href="https://github.com/achuwilson/easy_ur" target="_blank" > Easy_UR</a></strong>

                easy_ur is a library that provides a straightforward way to control Universal Robots while enabling flexible control.
                 It offers TCP and joint level position, velocity, acceleration, servoing, and freedrive control. 
                 <br>
                An intuitive GUI is also developed using Kivy.
                <br><br>
                code:  <a href="https://github.com/achuwilson/easy_ur" target="_blank" >https://github.com/achuwilson/easy_ur </a>

              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>


          <div class="col-md-12">
            <div class="col-md-4">
              <a href="posts/edgepivot.html"  >
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source  src="images/boxpivot.mp4" type="video/mp4"  />
              </a>
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">

                <strong> <a href="posts/edgepivot.html"  >Non-Prehensile manipulation for pivoting around corners without slipping. </a> </strong>
                
                <br>
                Inspired by human ability to slide an object over a surface and pivot around corners without dropping it, this project examined non-prehensile manipulation methods to achieve similar results.
                 In the MIT  Fall 2020<a href="https://manipulation.csail.mit.edu/" target="_blank" > 6.881 Manipulation class</a>, offered entirely online,I chose the class project to tackle this challenge. 
                The robot does not have the model of the object nor the environment.  It kept a constant vertical force while sliding the box across the table. At the edge, 
                the controller estimated the box's pose and applied horizontal and vertical forces to avoid slipping.
                 The controller was also able to return the box from a vertical position to a horizontal position on the table.
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="col-md-12">
            <div class="col-md-4">
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source src="images/rollfinger.mp4" type="video/mp4"  />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong> Gripper with in-hand rolling, tactile and proximity sensing</strong><br>
                Designed for the Kuka Innovation Award - AI Challenge 2021, this 3-DOF gripper features two GelSight tactile 
                sensors on the inner surface and an array of proximity sensors on the outer surface. 
                The fingers can be independently extended or retracted, giving the hand dexterity to perform simple in-hand rolling of objects. 
                This enhances the capture of rich tactile data over a larger surface not possible with a single contact. 
                The proximity sensors aid the robot in avoiding obstacles in low-visibility, cluttered environments. 
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="col-md-12">
            <div class="col-md-4">
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source src="images/fingervision_fgseg.mp4" type="video/mp4"  />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong> Background Segmentation for FingerVision tactile sensor</strong>
                FingerVision is an optical tactile sensor that delivers RGB images of objects within the fingers along with contact forces from optical markers. 
                To separate objects from the background captured by the sensor, a deep learning approach was applied. 
                The process involved collecting and labeling training data, then training a U-Net-based network. 
                The video demonstrates the RGB input on the left and the segmented objects on the right.
                 The training achieved high accuracy within just a few tens of minutes and demonstrated good generalization.
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="col-md-12">
            <div class="col-md-4">
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source src="images/chero.mp4" type="video/mp4"  />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong>Quasi Direct Drive Manipulator</strong>
                A personal project which provided me with lots of interesting challenges and learning oppertunities. The attempt to build a backdrivable quasi direct drive robot arm from grounds up was motivated by the easy
                availability of high torque BLDC motors, controllers and projects such as Berkeley Blue robot arm. The 7DOF robot has a spherical shoulder and a spherical wrist (not shown in video) making it dexterous. The project 
                also received partial funding from MIT ProjX 2019.  


              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="col-md-12">
            <div class="col-md-4">
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source src="images/gelsighthand.mp4" type="video/mp4"  />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong>Fully Actuated Robotic Hand With Multiple Gelsight Tactile Sensors</strong>
                This work details the design of a novel two finger robot gripper with multiple Gelsight based optical-tactilesensors covering the inner surface of the hand. The multiple
                Gelsight sensors can gather the surface topology of the object
                from multiple views simultaneously as well as can track the
                shear and tensile stress. In addition, other sensing modalities
                enable the hand to gather the thermal, acoustic and vibration
                information from the object being grasped. The force controlled
                gripper is fully actuated so that it can be used for various grasp
                configurations and can also be used for in-hand manipulation
                tasks.
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="col-md-12">
            <div class="col-md-4">
              <img class="img-responsive" src="images/sim2real.jpg" alt="" /><br />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong>GelSight Simulation for Sim2Real Learning.</strong>
                We introduce a novel approach for
                  simulating a GelSight tactile sensor in the commonly used Gazebo
                  simulator. Similar to the real GelSight sensor, the simulated
                  sensor can produce high-resolution images by an optical sensor
                  from the interaction between the touched object and an opaque
                  soft membrane. It can indirectly sense forces, geometry, texture
                  and other properties of the object and enables the research of
                  Sim2Real learning with tactile sensing.
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="col-md-12">
            <div class="col-md-4">
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source src="images/gelsightsim.mp4" type="video/mp4"  />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong> Simulation of GelSight 3D reconstruction </strong>
                This is one of the earliest works in simulating the behaviour of GelSight in a robotics simulator.
                The contact surface reconstruction capabilities of GelSight is simulated using raytracing techniques.
                An array of rays shooting from a rectangular sensor patch calculates and generates a PointCloud data of the contact surface.
                The video shows a robot gripper with the simulated sensor in pyBullet simulator grasping a coin and a sphere and generating the corresponding pointclouds which are
                visualized in Rviz.
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="col-md-12">
            <div class="col-md-4">
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source src="images/mrclutch.mp4" type="video/mp4"  /> 
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong>Magneto-Rheological Linear Clutch</strong>
                This paper
                  introduces a MR clutch which can control the force transmitted
                  by a linear actuator. The electromechanical model of the linear
                  clutch has been developed, implemented in hardware, and tested
                  using a prototype one Degree of Freedom arm. The design of the
                  clutch is detailed and the performance is characterized thorough
                  a series of experiments. The results suggest that the linear clutch
                  serves well for the precise force control of a linear actuator.
              </p>
            </div>
           
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="col-md-12">
            <div class="col-md-4">
              <a href="papers/2021-cable_flexibility.pdf" target="_blank"  > 
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source src="images/cable_classifier.mp4" type="video/mp4"  />
              </a>
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong> <a href="papers/2021-cable_flexibility.pdf" target="_blank" >Predicting the Flexibility of Electric Cables using Robot Tactile Sensing and
                  Push Primitives </a> </strong>
                  This project, which was done as a part of the CMU-24787 AI&ML class explored various ML techniques 
                  to learn and classify different cable types from the tactile data. The tactile data was captured from GelSight sensors when 
                  the robot grasped a cable and executed a predefined push primitive action. The classification performance of numerous
                   shallow machine learning approaches was compared against that of a CNN. We also implemented
                  PCA to investigate how these methods change with lower dimensional data.
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="col-md-12">
            <div class="col-md-4">
              <img class="img-responsive" src="images/delta_rob.jpg" alt="" /><br />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong>Improving the accuracy of DeltaZ robot using Machine Learning</strong>
                DeltaZ is an affordable, compliant, delta style, centimeter scale robot developed to teach the ideas of manipulation.
                This project developed methods to improve the accuracy of the robot by modelling for the errors using a neural network.
                The left part of image shows the robot touching on a touchscreen ( blue dots) with green dots as the goal positions. The 
                left part shows the robot performing with the correction applied. 
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>


          <div class="col-md-12">
            <div class="col-md-4">
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source src="images/2048.mp4" type="video/mp4"  />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong>Superhuman scores on the 2048 game using a Robot</strong>
                This was a weekend project where I tried to use a sr-scara robot, computer vision, and AI techniques to play the popular 2048 game on a smartphone.
                The smartphone screenshot was captured using ADB, analyzed using OpenCV and a min-max algorithm was used to predict the next move. The robot then executed the move on
                the real device then.
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="col-md-12">
            <div class="col-md-4">
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source src="images/sr_6d.mp4" type="video/mp4"  />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong> SR-6D </strong>
                SR-6D is a lightweight manipulator aimed for small parts picking and research purposes developed at Sastra Robotics. I helped design the mechanics, developed the low level electronics,
                control software, high level kinematics, trajectory generation, GUI and API interfaces. 
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>


          <div class="col-md-12">
            <div class="col-md-4">
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source src="images/sr_scara.mp4" type="video/mp4"  />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong> SR-SCARA.</strong>
                SR-SCARA was developed for high speed functional testing
                of devices with HMI interfaces such as touchscreens, button setc. I helped design the mechanics, developed the low level electronics,
                control software, high level kinematics, API interfaces, computer vision systems to estimate the test device state and spent time with customers to refine the product. These robots helped the 
                customers ( including many global automotive OEMs) to reduce the testing time by upto 70%. I also designed a force controlled stylus which can apply arbitrary forces and is safe to use on the device under test.
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="col-md-12">
            <div class="col-md-4">
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source src="images/chippu.mp4" type="video/mp4"  />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong> My first robot:  </strong>
                Inspired after attending a talk on ROS and watching Wall-E, I built this robot around 2010 as a way to get started with ROS. Learned how to make low level hardware
                communicate with ROS and got familiar with the high level tools in ROS. Trained voice control using CMU Sphinx and later implemented autonomous indoor SLAM navigation using the 
                ROS navigation stack.
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>





     <!-- End of projects div-->
      </div>


       <div class="row">
        <div class="col-md-12">
          <h2 id="blog">Blog</h2>

          <ul  style="font-size: 18px">
            <li>[June 2017]  <a href="posts/icra17.html"  > Writing my first research paper and presenting it at ICRA 17</a> </li>
            
            <li>[June 2016]  <a href="posts/icra16.html"  > Attending ICRA for the fist time and winning the HRATC challenge</a> </li>
            
          </ul> 

        </div>
      </div>

      <div class="row">
        <div class="col-md-12">
          <h2 id="notes">Notes</h2>
          <p align="justify" style="font-size:17px">
            These are the tips, tricks and documentation that are automatically updated from my Obsidian notebooks and Github Readme files.
            </p>

          <ul  style="font-size: 18px">
            
            
            <li> <a href="notes/controls.html" > Control Systems</a></li>
            <li> <a href="notes/cv.html" > Computer Vision</a></li>
            <li> <a href="notes/pydrake_manipulator.html"  > Drake, python bindings and setting up a Manipulation System.</a> </li>
            <li> <a href="notes/linux.html" > Linux </a></li>
            <li> <a href="notes/math.html" >Math </a></li>
            <li> <a href="notes/productivity.html" > Productivity</a></li>
            <li> <a href="notes/robotics.html" > Robotics </a></li>
            <li> <a href="notes/rpi.html" > Raspberry Pi - General </a></li>
            <li> <a href="notes/rpi_stream.html" > Raspberry Pi - Streaming the camera </a></li>
            <li> <a href="notes/stm32_bluepill.html" > STM32 (Bluepill) </a></li>
            <li> <a href="notes/webpages.html" > Website Tips </a></li>
            <li> <a href="notes/writing_presentations.html" >Writing, Presentations and Email </a></li>

          </ul> 

        </div>
      </div>


    <!-- End of main contents div-->
    </div>


      <div class="row">
        <div class="col-md-10"></div>
        <br />
        <a
          href="https://github.com/mavroudisv/plain-academic"
          target="_blank"
          style="color: gray"
          >Thanks to Vasilios Mavroudis for website template</a
        >
        <br />
        <br />
        <br />
      </div>
    </div>
    <!-- /.container -->
  </body>
</html>
