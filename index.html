<!DOCTYPE html>
<!--
    Plain-Academic by Vasilios Mavroudis
    Released under the  Simplified BSD License/FreeBSD (2-clause) License.
    https://github.com/mavroudisv/plain-academic
-->

<html lang="en">
  <head>
    <title>Achu Wilson</title>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <link
      rel="stylesheet"
      href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.css"
    />
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.12.0/jquery.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.js"></script>
    
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@700&display=swap" 
          rel="stylesheet"
          type="text/css"
          >

    <!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-44991178-2"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-44991178-2');
</script>


  </head>
  <body>
    <!-- Navigation -->
    <nav class="navbar navbar-inverse">
      <div class="container">
        <ul class="nav navbar-nav" style="font-size:22px" >
          <li><a href="#">Home</a></li>
          <li><a href="#education">Education</a></li>
          <li><a href="#experience">Experience</a></li>
          <li><a href="#publications">Publications</a></li>
          <li><a href="#projects">Projects</a></li>
          <li><a href="#blog">Blog</a></li>
          <li><a href="#projects">Notes</a></li>
        </ul>
      </div>
    </nav>

    <!-- Page Content -->
    <div class="container">
      <div class="row">
        <!-- Entries Column -->
        <div class="col-md-8">
          <!-- Main Image -->
          <img class="img-responsive" src="photo.jpg" alt="" /><br />

          <div style="margin-top: 3%; text-align: justify" style="font-size:16px">
            
          </div>
        </div>

        <!-- Contact Info on the Sidebar -->
        <div class="col-md-4">
          <div style="font-family: 'Roboto', sans-serif; font-size: 32px">
            <b>Achu Wilson</b>
          </div>
          <br />
          <p>
            <b>achuwilson [at] gmail [dot] com</b><br />
          </p>

        </div>

        <!-- Links on the Sidebar -->
        <div class="col-md-4" style="margin-top: 2%">
          <dd>
            <a href="https://github.com/achuwilson" target="_blank">GitHub</a>
          </dd>
          <dd>
            <a href="https://www.linkedin.com/in/achuwilson/" target="_blank"
              >LinkedIn</a
            >
          </dd>
          <dd>
            <a
              href="https://scholar.google.com/citations?user=EQnhhaQAAAAJ&hl=en"
              target="_blank"
              >Google Scholar</a
            >
          </dd>
          <dd>
            <a href="https://twitter.com/achuwilson" target="_blank">Twitter</a>
          </dd>
        </div>
      </div>

      
      <div class="row">
        <div class="col-md-12"  style="text-align: justify; font-size:17px">

          <p >
            &emsp; I am a roboticist with both an academic and industrial background. In academia, I focus my research on robot manipulation of flexible materials such as cables, using tactile sensing. On the industrial side, I have co-founded a robotics company, where I built and led R&D and production teams. I have also designed hardware and software which are in current commercial use.
          </p>

          <p >
            &emsp; I am currently pursuing a graduate research program at the RoboTouch Lab at Carnegie Mellon University, under Dr.Wenzhen Yuan. My research focuses on using tactile sensing to make robots learn how to manipulate flexible components such as cables.
          </p>

          <p >
            &emsp; Previously, I did research at the Robert Bosch Centre for Cyber Physical Systems in Indian Institute of Science (IISc), Bangalore, under Dr.Bharadwaj Amrutur, where I led the joint IISc-MIT team for the Kuka Innovation AI Challenge 2021. Before IISc, I did research under Dr.Edward Adelson on building sensorized robot hands at the Perception Science Group at CSAIL MIT.
          </p>

          <p>
            &emsp; My skills include research, programming - real time embedded systems, computer vision, machine learning , Python/C++ application development, ROS, industrial robot programming of UR, Kuka and Motoman robots. I am also well versed with mechanical design, love iterative prototyping and enjoy getting my hands dirty in the machine shop.
          </p>

        </div>
      </div>

      <div class="row">
        <div class="col-md-12">
          <h2 id="news">News</h2>

          <ul style="font-size:17px">
            <li>I'm on the job market and am looking for positions starting in mid-2023. Please let me know if you interested in having a discussion!
            </li>
            <li><strong>[Jan 2023] </strong> Our new paper (Cable Routing and Assembly using Tactile-driven Motion Primitives) is accepted for ICRA 2023. Preprints coming out soon!</li>
            <li><strong>[Jan 2023] </strong> First stable release of <strong> <a href="https://github.com/achuwilson/easy_ur" target="_blank" > Easy_UR</a></strong>  -  a library to control UR series of robots</li>
          </ul> 

        </div>
      </div>


      <div class="row">
        <div class="col-md-12">
          <h2 id="education">Education</h2>

         <u><strong style="font-size: 16px">MS Research, MechE- Robotics and Controls</strong></u>
          <p style="font-size: 16px">
            <strong  style="font-size: 15px"> Carnegie Mellon University (Sep 2021 – May 2023, Pittsburgh, US)</strong>
            <br>Courses: Machine Learning (24-787), Mechanics of Manipulation (16-741), AI for Manipulation (16-740), Computer Vision (24-678),
            Statistical Techniques/Reinforcement Learning for Robotics(16-831), Robot Cognition for Manipulation(16-890), Optimization (18-660), Bio-Inspired Robotics(24-775).

          </p>

          <u><strong style="font-size: 16px" >Visiting Graduate Student, EECS</strong></u>
          <p style="font-size: 16px">
            <strong  style="font-size: 15px">Massachusetts Institute of Technology (Oct 2018 – Oct 2019, Cambridge, USA)</strong>
            <br>Courses: Introduction to Robotics (2.12), Robotic Manipulation (6.881)
          </p>

          <u><strong style="font-size: 16px" >B.Tech in Electronics and Communication Engineering</strong></u>
          <p style="font-size: 16px">
            <strong  style="font-size: 15px">University of Calicut (Sep 2008 – May 2012, India)</strong>
            <br>Courses: Electronic Circuits, Control Systems, Embedded Systems, Communication Systems
          </p>
        </div>
      </div>

      <div class="row">
        <div class="col-md-12" style="font-size: 17px">
          <h2 id="experience">Experience</h2>
          
          <strong style="font-size: 18px">Graduate Research Assistant </strong>
          <p>
            <strong  style="font-size: 16px"> <i>RoboTouch Group, Robotics Institute, CMU, Sept 2021 – Present, Pittsburgh, US</strong> </i>
            <ul style="margin-top:-10px; line-height:1.3;" >
              <li > Developed tactile driven manipulation primitives and skills for cable routing and assembly tasks.</li>
              <li> Optimized fabrication techniques on curved GelSight Sensors.</li>      
              <li> Implemented tactile based hybrid force - position controllers for UR5 Robots.</li>
              <li> Developed and maintained the manipulation pipeline (including easy_ur) used for various other projects in the lab.</li>      
            </ul>
          </p>

          <strong style="font-size: 18px" >Research Associate</strong>
          <p>
            <strong  style="font-size: 16px"> <i>Robert Bosch Center for Cyber-Physical Systems, Indian Institute of Science, 
              Sept 2020 – Sept 2021, Bangalore, India. </i></strong>
              <ul style="margin-top:-10px; line-height:1.3;" >
              <li> Led Team MIT-IISc on the Kuka Innovation Award - AI Challenge 2021.</li>
              <li> Designed and fabricated a two finger robot hand which has GelSight tactile sensing on
                the inner surface and close range proximity sensor array on outer
                surfaces. The hand could perform in-hand object rolling thereby
                gathering tactile data over extended object surfaces.</li>
              <li> Explored reinforcement learning algorithms to explore and pick up
                objects in vision denied environments using tactile & proximity sensing
                data. </li>
              <li>Implemented model free non-prehensile object manipulation techniques on Kuka IIWA robot. </li>
              </li> Trained graduate students on programming Kuka, Motoman robot manipulators and on GelSight 
              fabrication. </li>
            </ul>
          </p>

          <strong style="font-size: 18px">Visiting Research Student/Associate</strong>
          <p>
            <strong  style="font-size: 16px"> <i>Perceptual Science group, CSAIL, Massachusetts Institute of Technology, Oct 2018 – Oct 2019, Cambridge, USA
              </i></strong>
            <ul style="margin-top:-10px; line-height:1.3;" >
            <li> Designed a fully actuated robot hand with multi-sensory feedback,
                including multiple GelSight tactile sensors. It involved design of the
                mechanical system, electronics and software for data capture and
                processing. </li>
            </ul>
          </p>

          <strong style="font-size: 18px">Co-Founder & Chief Technology Officer</strong>
          <p>
            <strong  style="font-size: 16px"> <i>Sastra Robotics, Aug 2013 – Sep 2018, Cochin, India
            </i></strong>
            <ul style="margin-top:-10px; line-height:1.3;" >
              <li>  As the technical co-founder, designed and built the drive systems,
                motion controllers and user software for SCARA and 6 axis robots for
                  device testing applications</li>
              <li> Developed robot force controllers to ensure safety of devices under test
                and a sensorized finger to analyze the haptic feedback generated on
                touchscreens.</li>
              <li> Created computer vision systems to analyze device states from LCD
              screens, dials, switches, knobs etc</li>
              <li> These robots helped OEM clients in reduction
              of device testing time by upto 70%</li>
              </ul>
          </p>

          <strong style="font-size: 18px">Robotics Software Engineer</strong>
          <p>
            <strong  style="font-size: 16px"> <i>Asimov Robotics (Energid technologies), June 2012 – July 2013, Cochin, India
            </i></strong>
            <ul style="margin-top:-10px; line-height:1.3;" >
              <li>Developed ROS & LabVIEW interfaces for Cyton Gamma 7DOF
                manipulator through Energid Actin framework. </li>
              <li> Designed electrical subsystems, motor controllers and sensor interfaces
                for a humanoid social robot.</li>
              <li> Implemented ROS based manipulation pipeline for a custom bi-manual
                dexterous manipulator.</li>
            </ul>
          </p>


        </div>
      </div>

      <div class="row">
        <div class="col-md-12">
          <h2 id="publications">Publications</h2>

          <!--strong>Selected peer-reviewed papers</strong><br /-->
          <ul  style="font-size: 17px">
            <li class="paper">
              <strong><a href="" target="_blank"
                >Cable Routing and Assembly using Tactile-driven Motion Primitives. </a> </strong>
              Achu Wilson, Helen Jiang, Wenzhao Lian, Wenzhen Yuan (accepted for
               IEEE International Conference on Robotics & Automation (ICRA), London, 2023)
               <a href="https://youtu.be/_F0kCjBgxD0" target="_blank" > <strong style="color:green;">VIDEO </strong> </a>
            </li>
            <li class="paper">
              <strong> <a href="papers/2019-gelsighthand-iros.pdf" target="_blank"
                > Design of a Fully Actuated Robotic Hand With Multiple Gelsight Tactile Sensors. </a></strong>
               Achu Wilson, Shaoxiong Wang, Branden Romero, Edward Adelson, RoboTac Workshop,
               International Conference on Intelligent Robotics and Systems (IROS), Macau, 2019
               <a href="https://youtu.be/4hxsZ9nHJWI" target="_blank" > <strong style="color:green;">VIDEO </strong> </a>
            </li>
            <li class="paper">
              <strong> <a href="papers/2019-sim2real-icra.pdf" target="_blank"
                > GelSight Simulation for Sim2Real Learning.</a></strong>
              
               Daniel Fernandes Gomes, Achu Wilson, Shan Luo, ViTac Workshop,
               IEEE International Conference on Robotics & Automation (ICRA), Montreal, 2019.
            </li>
            <li class="paper">
              <strong><a href="papers/2017-mr-clutch-icra.pdf" target="_blank"
                > Design and Development of a Magneto-Rheological Linear Clutch for Force
                controlled Human Safe Robots. </a></strong>
              
              Achu Wilson, IEEE International Conference on Robotics & Automation (ICRA), Singapore, 2017.
               <a href="https://youtu.be/ES0q4eb1TYo" target="_blank" >  <strong style="color:green;">VIDEO </strong> </a>
            </li>
            <li class="patent">
              <strong><a href="https://patents.google.com/patent/WO2017051263A2/en" target="_blank"
                > Robot arm for testing of touchscreen applications</a></strong>
              
               Achu Wilson, Aronin P, Akhil A, International Patent, WO2017051263A3
            </li>
          </ul>
        </div>
      </div>

      <div class="row">

        <div class="col-md-12">
          <h2 id="projects">Projects</h2>

          <div class="col-md-12">
            <div class="col-md-4">
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source src="images/pivotmanip.mp4" type="video/mp4"  />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong> Non-Prehensile Pivoting for Manipulation of Heavy Objects</Strong>
                  Inspired by the way humans move big and heavy objects, this project explored techniques for the manipulation of objects heavier than the
                  payload of robot manipulators. Objects could be manipulated using pivoting actions which maintain edge or line contacts with the surface.  Analytical solutions were developed as part of the CMU 16-741 Mechanics of Manipulation class while reinforcement learning based solutions
                  were explored as a part of the CMU 16-845 RoboStats class. The video shows the robot moving
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>


  

          <div class="col-md-12" >
            <div class="col-md-4" >          
            <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
              <source src="images/cableroute.mp4" type="video/mp4"  />
            </video>
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong>Cable Routing and Assembly using Tactile-driven Motion Primitives</strong>

                In this project we integrated tactile-guided low-level
                motion primitives with high-level vision-based task parsing for a
                the routing and assembly of cables on a reconfigurable
                 task board. A library of tactile-guided
                  motion primitives using a fingertip GelSight sensor was developed. Each primitive 
                  reliably accomplished operations such as following along a cable, wraping cable around pivots
            , weaving through slots or inserting a connector.The overall task is inferred via visual perception given
                   a goal configuration image, and then used to generate the primitive sequence.  <a href="https://youtu.be/_F0kCjBgxD0" target="_blank" > <strong style="color:green;">VIDEO </strong> </a>
                 
                

              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>


          <div class="col-md-12">
            <div class="col-md-4">
              <img class="img-responsive" src="images/easygui.png" alt="" /><br />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <br>
                <strong> <a href="https://github.com/achuwilson/easy_ur" target="_blank" > Easy_UR</a></strong>

                easy_ur is a library to control Universal Robots in a very simple way, 
                yet allowing for very flexible control. It allows for the TCP and joint level position, velocity, acceleration, servoing and freedrive control. 
                 <br>
                An intuitive GUI is also developed using Kivy.
                <br><br>
                code:  <a href="https://github.com/achuwilson/easy_ur" target="_blank" >https://github.com/achuwilson/easy_ur </a>

              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>


          <div class="col-md-12">
            <div class="col-md-4">
              <a href="posts/edgepivot.html"  >
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source  src="images/boxpivot.mp4" type="video/mp4"  />
              </a>
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">

                <strong> <a href="posts/edgepivot.html"  >Non-Prehensile manipulation for pivoting around corners without slipping. </a> </strong>
                
                <br>
                Inspired by the capability of humans to slide an object over a surface and pivot around the corners without dropping, this project explored non-prehensile manipulation
                techniques to do the same. The MIT  Fall 2020<a href="https://manipulation.csail.mit.edu/" target="_blank" > 6.881 Manipulation class</a>, which was offered entirely online, had a class project and I decided to try this. 
                The robot does not have the model of the object nor the environment. The robot maintains a constant vertical force when sliding the box over the table. At the edge, the controller is able to estimate the pose of the box
                and applies both horizontal and vertical forces to prevent slipping. The controller is also able to bring back the box from the vertical position to horizontal position on the table. 
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="col-md-12">
            <div class="col-md-4">
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source src="images/rollfinger.mp4" type="video/mp4"  />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong> Gripper with in-hand rolling, tactile and proximity sensing</strong><br>
                Developed for the Kuka Innovation Award, AI Challenge 2021, this 3 DOF gripper has two GelSight tactile sensors on the inner surface and an array of proximity sensors on the outser surface. Both the fingers 
                can be independently extracted/retracted thereby making the hand dexterous enough to do simple in hand-rolling of objects. This is used to capture a rich tactile data over a larger surface which cannot be done using a single contact.
                The proximity sensors are used by the robot to avoid obstacles in low visibility, cluttered scenes.  
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="col-md-12">
            <div class="col-md-4">
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source src="images/fingervision_fgseg.mp4" type="video/mp4"  />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong> Background Segmentation for FingerVision tactile sensor</strong>
                FingerVision is an optical tactile sensor which provides the RGB image of the object within the fingers in addition to the contact forces from the optical markers. Since the sensor captures the 
                background also in addition to the object, there exists the need to remove the background. A deep learning approach is used to segment the objects and the background. Training data is collected,
                 labelled and a U-Net based network is trained. The left part of the video shows the RGB input and the left part shows the segmented objects. The training acheives good accuracy in a couple of tens of minutes and generalizes well.
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="col-md-12">
            <div class="col-md-4">
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source src="images/chero.mp4" type="video/mp4"  />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong>Quasi Direct Drive Manipulator</strong>
                Another personal project which provided me with lots of interesting challenges and learning oppertunities. The attempt to build a backdrivable quasi direct drive robot arm from grounds up was motivated by the easy
                availability of high torque BLDC motors, controllers and projects such as Berkeley Blue robot arm. The 7DOF robot has a spherical shoulder and a spherical wrist (not shown in video) making it dexterous. The project 
                also received partial funding from MIT ProjX 2019.  


              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="col-md-12">
            <div class="col-md-4">
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source src="images/gelsighthand.mp4" type="video/mp4"  />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong>Fully Actuated Robotic Hand With Multiple Gelsight Tactile Sensors</strong>
                This work details the design of a novel two fin ger robot gripper with multiple Gelsight based optical-tactilesensors covering the inner surface of the hand. The multiple
                Gelsight sensors can gather the surface topology of the object
                from multiple views simultaneously as well as can track the
                shear and tensile stress. In addition, other sensing modalities
                enable the hand to gather the thermal, acoustic and vibration
                information from the object being grasped. The force controlled
                gripper is fully actuated so that it can be used for various grasp
                configurations and can also be used for in-hand manipulation
                tasks.
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="col-md-12">
            <div class="col-md-4">
              <img class="img-responsive" src="images/sim2real.jpg" alt="" /><br />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong>GelSight Simulation for Sim2Real Learning.</strong>
                We introduce a novel approach for
                  simulating a GelSight tactile sensor in the commonly used Gazebo
                  simulator. Similar to the real GelSight sensor, the simulated
                  sensor can produce high-resolution images by an optical sensor
                  from the interaction between the touched object and an opaque
                  soft membrane. It can indirectly sense forces, geometry, texture
                  and other properties of the object and enables the research of
                  Sim2Real learning with tactile sensing.
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="col-md-12">
            <div class="col-md-4">
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source src="images/gelsightsim.mp4" type="video/mp4"  />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong> Simulation of GelSight 3D reconstruction </strong>
                This is one of the earliest works in simulating the behaviour of GelSight in a robotics simulator.
                The contact surface reconstruction capabilities of GelSight is simulated using raytracing techniques.
                An array of rays shooting from a rectangular sensor patch calculates and generates a PointCloud data of the contact surface.
                The video shows a robot gripper with the simulated sensor in pyBullet simulator grasping a coin and a sphere and generating the corresponding pointclouds which are
                visualized in Rviz.
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="col-md-12">
            <div class="col-md-4">
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source src="images/mrclutch.mp4" type="video/mp4"  /> 
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong>Magneto-Rheological Linear Clutch</strong>
                This paper
                  introduces a MR clutch which can control the force transmitted
                  by a linear actuator. The electromechanical model of the linear
                  clutch has been developed, implemented in hardware, and tested
                  using a prototype one Degree of Freedom arm. The design of the
                  clutch is detailed and the performance is characterized thorough
                  a series of experiments. The results suggest that the linear clutch
                  serves well for the precise force control of a linear actuator.
              </p>
            </div>
           
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="col-md-12">
            <div class="col-md-4">
              <a href="papers/2021-cable_flexibility.pdf" target="_blank"  > 
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source src="images/cable_classifier.mp4" type="video/mp4"  />
              </a>
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong> <a href="papers/2021-cable_flexibility.pdf" target="_blank" >Predicting the Flexibility of Electric Cables using Robot Tactile Sensing and
                  Push Primitives </a> </strong>
                  This project, which was done as a part of the CMU-24787 AI&ML class explored various ML techniques 
                  to learn and classify different cable types from the tactile data. The tactile data was captured from GelSight sensors when 
                  the robot grasped a cable and executed a predefined push primitive action. The classification performance of numerous
                   shallow machine learning approaches was compared against that of a CNN. We also implemented
                  PCA to investigate how these methods change with lower dimensional data.
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="col-md-12">
            <div class="col-md-4">
              <img class="img-responsive" src="images/delta_rob.jpg" alt="" /><br />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong>Improving the accuracy of DeltaZ robot using Machine Learning</strong>
                DeltaZ is an affordable, compliant, delta style, centimeter scale robot developed to teach the ideas of manipulation.
                This project developed methods to improve the accuracy of the robot by modelling for the errors using a neural network.
                The left part of image shows the robot touching on a touchscreen ( blue dots) with green dots as the goal positions. The 
                left part shows the robot performing with the correction applied. 
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>


          <div class="col-md-12">
            <div class="col-md-4">
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source src="images/2048.mp4" type="video/mp4"  />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong>Superhuman scores on the 2048 game using a Robot</strong>
                This was a weekend project where I tried to use a sr-scara robot, computer vision, and AI techniques to play the popular 2048 game on a smartphone.
                The smartphone screenshot was captured using ADB, analyzed using OpenCV and a min-max algorithm was used to predict the next move. The robot then executed the move on
                the real device then.
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="col-md-12">
            <div class="col-md-4">
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source src="images/sr_6d.mp4" type="video/mp4"  />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong> SR-6D </strong>
                SR-6D is a lightweight manipulator aimed for small parts picking and research purposes developed at Sastra Robotics. I helped design the mechanics, developed the low level electronics,
                control software, high level kinematics, trajectory generation, GUI and API interfaces. 
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>


          <div class="col-md-12">
            <div class="col-md-4">
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source src="images/sr_scara.mp4" type="video/mp4"  />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong> SR-SCARA.</strong>
                SR-SCARA was developed for high speed functional testing
                of devices with HMI interfaces such as touchscreens, button setc. I helped design the mechanics, developed the low level electronics,
                control software, high level kinematics, API interfaces, computer vision systems to estimate the test device state and spent time with customers to refine the product. These robots helped the 
                customers ( including many global automotive OEMs) to reduce the testing time by upto 70%. I also designed a force controlled stylus which can apply arbitrary forces and is safe to use on the device under test.
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>

          <div class="col-md-12">
            <div class="col-md-4">
              <video  class="img-responsive " autoplay loop muted playsinline disableRemotePlayback x-webkit-airplay="deny" disablePictureInPicture style="object-fit: fill" >
                <source src="images/chippu.mp4" type="video/mp4"  />
            </div>
            <div class="col-md-8">
              <p align="justify" style="font-size:17px">
                <strong> My first robot:  </strong>
                Inspired after attending a talk on ROS and watching Wall-E, I built this robot around 2010 as a way to get started with ROS. Learned how to make low level hardware
                communicate with ROS and got familiar with the high level tools in ROS. Trained voice control using CMU Sphinx and later implemented autonomous indoor SLAM navigation using the 
                ROS navigation stack.
              </p>
            </div>
          </div>
          <div>
            &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
          </div>





     <!-- End of projects div-->
      </div>


       <div class="row">
        <div class="col-md-12">
          <h2 id="blog">Blog</h2>

          <ul  style="font-size: 18px">
            
            <li>[June 2016]  <a href="posts/icra16.html"  > Attending ICRA for the fist time and winning the HRATC challenge</a> </li>
            
          </ul> 

        </div>
      </div>


    <!-- End of main contents div-->
    </div>


      <div class="row">
        <div class="col-md-10"></div>
        <br />
        <a
          href="https://github.com/mavroudisv/plain-academic"
          target="_blank"
          style="color: gray"
          >Thanks to Vasilios Mavroudis for website template</a
        >
        <br />
        <br />
        <br />
      </div>
    </div>
    <!-- /.container -->
  </body>
</html>
