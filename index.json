[{"authors":["achuwils"],"categories":null,"content":"I am a roboticist, currently working at Robert Bosch Centre for Cyber Physical Systems at Indian Institute of Science (IISc), Bangalore. My research interests involve tactile sensing for robot manipulation, in-hand manipulation and reinforcement learning. I would say that I am more of a full stack roboticist and have experience building mechanical systems, designing electronics and writing code.\nPrior to joining IISc I used to work ( and still collaborate) with the Perception Science Group at CSAIL MIT.\nI am also one of the co-founders and was the CTO of Sastra Robotics. Sastra builds robots to automate the functional testing of devices: ranging from smartphones to cockpit avionic panels. Later I stepped down, as the startup matured, to pursue a career in academia. Since then, I also serve as a technical consultant for Sastra.\n ","date":1610928000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1610928000,"objectID":"8778fa3ebce2f8d51a5f1b013d5a2a50","permalink":"https://achuwilson.github.io/authors/achuwils/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/achuwils/","section":"authors","summary":"I am a roboticist, currently working at Robert Bosch Centre for Cyber Physical Systems at Indian Institute of Science (IISc), Bangalore. My research interests involve tactile sensing for robot manipulation, in-hand manipulation and reinforcement learning.","tags":null,"title":"","type":"authors"},{"authors":null,"categories":["Software Projects"],"content":"  What and why  How  Results  What and Why Humans can manipulate objects in interesting ways without even grasping them. We make use of the properties of the object as well as the environment to push, flip, throw and squeeze objects.\nThe following video shows a human hand pushing a paper box on a table and pivoting it around an edge. We can do such a task just by relying on the forces felt on the finger.\nThe MIT 6.881 Manipulation class Fall 2020, which was offered entirely online, had a class project and I decided to try this. A Kuka LBR iiwa robot, fitted with a simple single finger end effector is used to achieve the task. The robot does not have the model of the object nor the environment (No prior knowledge of the dimensions of the box or the position coordinates of the edge). The workspace limitations of the robot restrict moving the object under the table, but the method could be extended for any such capable robot.\nHow Since the model of the object and environment is not provided and no external vision sensors are used, proprioceptive data alone is to be used. The joint torques and position from the robot are used in combination with the impedance control capabilities of the iiwa to estimate the model parameters. This would be an almost impossible or incredibly difficult task with a conventional position-controlled stiff robot, even if equipped with a Force/Torque sensor. The entire system is implemented in Drake as a state machine. The following paragraphs detail the approach used.\nInitially, the robot moves at a constant velocity in the negative z-axis. Individual joint velocities are calculated from the desired velocity using a PsuedoInverseJacobian controller. Since the iiwa does not have a joint velocity input, the computed joint velocities are integrated to obtain the joint position inputs. While the robot is moving at such a constant velocity, the force on the end effector is being monitored. IIWA provides joint torques estimated at the 7 joints. The external wrench at the end effector is calculated using the PsuedoInverseJacobian method. When the z-axis force is above an experimentally determined threshold, the state machine jumps into the next state.\nOnce the finger touches the box, it applies a downward force and a forward velocity such that the resultant force is outside the friction cone of the contact between the box and table. Since the friction coefficient is unknown, the forces are determined experimentally.\nThe Drake Kuka driver allows us to provide a feedforward torque to each joint. The individual feedforward torque values are calculated from the desired end-effector wrench using the Jacobian transpose methods and commanded.\nAs the finger approaches the edge of the table, interesting things start to happen.\nAs the center of gravity moves past the edge, the gravitational force acting on it causes a torque centered around the edge. This torque generates an upwards force on the fingers (green up arrows). Since the iiwa operates in impedance control mode, it is not very stiff in the z-axis. So, it deviates minutely from the previous positions in the vertical direction, moving in an arc centered around the corner. The derivative of z-axis force is used to detect the beginning of this deviation and is used to transition the state machine to the next state.\nOnce the box starts pivoting, the finger continues moving in the x-axis, until the vertical z-axis reaction force generated by the box on the finger is zero. The gravitational force on the object would be balanced by the horizontal component of the forces on the finger and the corner. The motion of the finger is stopped and any incremental motion will cause the object to fall.\nThe following plot shows the z axis finger coordinates during this time and we can clearly see the arc.\nThe following graph shows the forces and their derivatives on the finger until the box is balanced between the finger and the corner (T3).\n From 0 to T1, the finger moves down at the constant velocity. The force readings are quite noisy when the finger is not in contact with anything and the accuracy is only +/-5N. The spike at T1 is caused by contact with the object. From T1 to T2, The finger moves towards the edge, applying a constant force in the z-axis. At T2, the box starts pivoting due to torque induced by gravity. the derivative of the force is found to be a reliable estimate to detect this rather than the raw noisy force data. From T1 to T3, the z-axis force peaks and then drops. It is this force that causes the compliant z-axis to deviate in position and move in an arc, which is shown in the plot of pZ (z-axis position). At T3, the box is balanced perfectly between the finger and the corner. The force on the z-axis drops while the x-axis force increases. The motion of the finger is stopped at this point.  The points on the arc are observed and fitted onto a model of the circle to estimate the model parameters: the position coordinate of the corner and the size of the object.\nOnce we estimate the center and radius from the arc, we plan a trajectory along the perimeter, starting from the current finger position and finishing at a position when the box is vertical.\nThe trajectory includes the positions of each frame (blue dots) as well as the forces (green arrows) at all these frames. This makes sure that while pivoting, the vector sum of forces direct towards the corner position.\nThe robot then moves along the trajectory. Once the finger reaches the end of the above trajectory, a new trajectory is planned, along the same path to pivot the box back to the horizontal position. While executing this new trajectory, fingers move along the circular path, with tangential velocity and radial force components.\nOnce the box is nearly vertical, the state machine transitions into a new state, in which the finger moves a little up or down to maintain a pre-determined vertical force on the box. This accounts for the accumulated position errors caused during the estimation of the model parameters. Once it is done, the state machine transitions it to the final state where the finger moves horizontally, moving the box to the initial position.\nResults The following video (3x sped up) shows the system in action.\n  \nThe approach works reliably and is tested with boxes of different sizes and weights as well as for different table positions.\nThe following areas could be improved for better performance.\n The system cannot detect the slip that might occur. A tactile sensorized finger (GelSight?) could be used for this. The joint torques reported by iiwa are not very accurate and are also pose dependant. This could be improved using an external F/T sensor. The estimated model may not be always perfectly accurate. The inaccuracy in the estimated edge position causes a jerking motion when we start applying force towards the center. The finger position not only depends on the commanded position but the feed-forward torque also, this makes the transition between feed-forward torque control mode and trajectory execution mode tricky and causes little jerks.   ","date":1611705600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1611705600,"objectID":"e3450c1744c724f8a2b601eca0c98610","permalink":"https://achuwilson.github.io/project/2021-edgepivot/","publishdate":"2021-01-27T00:00:00Z","relpermalink":"/project/2021-edgepivot/","section":"project","summary":"Pushing and pivoting a box around an edge.","tags":["software","wip"],"title":"Nonprehensile Manipulation","type":"project"},{"authors":[""],"categories":["iiwa","drake"],"content":"This document serves as a quick introduction to Kuka IIWA Robot and controlling it using Drake Python API. The examples are entirely in python and is easy to port to any other robot manipulator. Make sure you read the official Kuka documentation and is familiar with the functioning before operating the robot.\nThe codes of the examples used in this document are available at https://github.com/achuwilson/pydrake_iiwa\nContents   Introduction  Kuka System Architecture  Programming the Robot   Kuka Sunrise WorkBench  FRI    Smartpad and Workbench Video Walk-through  Controlling IIWA from ROS  Controlling IIWA from MATLAB/Simulink/Python  Drake   Systems  Diagrams  Context  MultibodyPlant  Tutorials    LCM  Controlling IIWA from Drake   Drake IIWA Java Application  kuka_driver  IIWA-LCM Interface    Controlling Custom Robot from Drake  Manipulation Station  Examples   Joint Control  Visualizing the robot state in Drake visualizers  Adding an end effector  Forward Kinematics  Inverse Kinematics  Estimating Cartesian Velocities  Cartesian velocity control  Estimating Cartesian forces  Hybrid Force-Position control  Motion Planning and Collision Avoidance  Gravity Compensation  Haptic Force Feedback    Introduction Kuka LBR IIWA is a 7 DOF collaborative robot manipulator which has got excellent torque control capabilities in addition to the common position control features. This enables capabilities like impedance control which is much beneficial when the robot has to interact with noisy environment models, where pure position control can break things (or even the robot). The robot has joint torque sensors on all the joints. ( It should be noted that the torque estimation is not as accurate as an external Force-Torque sensor. It is observed that the error depends on robot pose and has an accuracy of approximately 5N)\nKuka System Architecture A typical architecture of the manipulation station will be as follows:\nIIWA manipulator is controlled by Kuka Sunrise Cabinet controller. It has an industrial PC running Kuka\u0026rsquo;s version of Windows CE called Sunrise OS and a realtime OS. The sunrise OS handles the user program, GUI etc and is accessible to the user. The realtime part is hidden from the user and controls the low level hardware interfaces.\nThe Sunrise Cabinet have multiple Ethernet interfaces. Kuka Line Interface (KLI) (Port X66) is the only one enabled by default and is used to connect the Controller to an external PC. Kuka provides Sunrise Workbench IDE with which the IIWA could be programmed in Java. The IDE also helps installing software packages and configuring settings such as network and safety configurations, input-output etc.\nThe other interface called Fast Robot Interface (FRI) enables access to the robot controller from an external computer in real time. It operates over the Kuka Optional Network Interface (KONI) and has to be enabled by installing the FRI package. Drake uses this interface to interface with the IIWA hardware.\nAdditional interfaces like EtherCAT, PROFINET are also available, which can be enabled by installing the corresponding software package from Kuka.\nKuka Smartpad, the handheld controller allows to start and stop the programs loaded into the SunriseCabinet. In fact, it shows the remote desktop view of the Sunrise OS running inside the Cabinet. Connecting an external monitor to the DVI port on the back side of the Sunrise Cabinet also shows the same Smartpad GUI. In addition, it is also possible to access the Smartpad GUI using Remote Desktop tools over the KLI ethernet port.\nThe following are the default IP address of the Ethernet Ports:\n KLI : 172.31.1.147 KONI FRI : 192.170.10.2 Remote Desktop: 172.31.1.147 Username: KukaUser Password: 68kuka1secpw59  The following documents give a detailed overview of the Kuka IIWA Robot systems. It is recommended to get familiarized with the IIWA system from the following documents before operating the robot.\n  KUKA Sunrise.OS 1.16, Operating Instructions for End Users  KUKA Sunrise Cabinet Operating Instructions  System Software KUKA Sunrise.OS 1.16, KUKA Sunrise.Workbench 1.16, Operating and Programming Instructions for System Integrators. This document has Java API documentation  NOTE: the linked documents could not be shared publicly as it would violate Kuka\u0026rsquo;s Copyright notice and require IISc login. You could also create a free account on Kuka Xpert to download these\nProgramming the Robot Kuka basically provides the following two methods to program the robot\n Kuka Sunrise Workbench FRI  Kuka Sunrise Workbench The default programming option provided by Kuka is through its Java APIs using Sunrise Workbench, which is in-fact a customized Eclipse IDE. It is not available for download on the Kuka website, as it has to match the version of the Sunrise OS running on the controller, So request for your copy of Sunrise Workbench to your Kuka robot supplier.\nThe Java APIs may differ slightly depending upon the version of the Sunrise OS and Workbench that is being used. It is available in the Kuka Sunrise Workbench Operating and Programming Instructions\nAfter developing an application in the Sunrise Workbench, the user has to synchronize it with the Sunrise OS in the controller. This just copies the project files to the controller over the KLI port.\nDownload Links\n  Kuka Sunrise Workbench v1.16  FRI Addon  SmartServo Addon  HRC Addon  Sample Sunrise Project  FRI Client SDK C++  NOTE: While creating a new project and synchronizing it with the Controller, after changing any safety related settings, the SmartPad would show a \u0026ldquo;safety configuration not activated\u0026rdquo; error. The default password to activate the safety configuration is ARGUS\nAfter loading them, applications can be selected and executed using the Smartpad interface. The Key on the Smartpad helps switch between AUT - automatic and T1 - reduced velocity mode.\nFRI FRI stands for \u0026ldquo;Fast Robot Interface\u0026rdquo;, which is an addon provided by Kuka, enables real time control of the robot system. This requires control signals be generated in an external computer and sent over the KONI Ethernet port. The FRI is not enabled out of the box and has to be installed and enabled through the Sunrise workbench. The default IP address of the FRI interface is 192.170.10.2\nKuka provides FRI-Client libraries in C++ and Java, which can be found inside the examples directory after the installation of FRI library in Sunrise WorkBench. The C++ libraries can be found in the file named FRI-Client-SDK_Cpp.zip. It can be used to build applications that communicates with Kuka controller over FRI.\nDrake uses the FRI interface to control the IIWA from an external computer.\nSmartpad and Workbench Video Walk-through TODO #1\nControlling IIWA from ROS The iiwa_stack package can be used to interface IIWA from ROS. It uses the Smart Servoing functionality over the KLI network interface.\nThe ROSJava nodes running on the robot controller as a Sunrise RobotApplication sends data and receives commands from a ROS master running on the external PC. The wiki provides detailed instructions on controlling from ROS.\nControlling IIWA from MATLAB/Simulink/Python  The Kuka Sunrise Toolbox for Matlab allows control of the LBR iiwa robot from MATLAB. The [Simulink-iiwa interface] (https://github.com/Modi1987/Simulink-iiwa-interface) for Simulink based control.  iiwaPy can be used for control of the iiwa from Python.  All of the above packages are based on the Kuka Sunrise Toolbox for Matlab.\nDrake  Drake is a toolbox which can model dynamic systems, solve mathematical problems and has built in multibody kinematics and dynamics.\nRunning the examples requires installation of Python bindings of Drake as documented here. These were tested in Ubuntu 18.04 with binary installation of Drake.\nSystems The basic building block of Drake is a System, which has input and output ports as well as an optional state. Multiple systems can be interconnected either as a Diagram or LeafSystem. LeafSystems are the minimum building block and is often used for basic components like sensors, actuators, controllers, planners etc, which has a specific functionality. Drake come with many built-in systems which can be found in the official documentation\nDiagrams Diagrams consists of multiple LeafSystems or even other Diagrams inside and are used to represent a set of interconnected systems that function as a whole.\n example_drake_simplediagram.py creates a simple diagram which looks as follows:\nThe built-in SystemSlider is used to create a GUI with 3 sliders, whose output is fed into a PrintSystem which evaluates and prints the input values to terminal at a specific update rate.\nContext All Diagram and System has a Context which embodies the state and parameters of the system. In addition to the Context of the main diagram, each subsystems and sub diagrams have their own unique context with with we can interact with the internals of the systems. Given the context, all methods called on a Diagram or System is deterministic and repeatable. The Simulator needs the Diagram and its Contextfor running the computations.\nMultibodyPlant MultibodyPlant is one of the most important built-in systems that Drake provides. It is used to represent multiple rigid bodies connected in tree, a common practice with serial robot manipulators. It internally uses rigid body tree algorithms to compute the kinematics. MultibodyPlant also has both inputs and outputs which could be connected to other systems such as controllers or visualizers.\nTutorials Drake provides a set of tutorials\n dynamical_systems.ipynb gives an introduction to modelling systems in Drake\n mathematical_program.ipynb introduces numerical programming capabilities of Drake\nLCM  LCM stands for Lightweight Communications and Marshalling. It is a set of libraries that can provide publish/subscribe message passing capabilities.\nLCM implementatoions are available for all common programming languages and operating systems. Refer to the LCM example for a quick-start.\nPlotting LCM Messages Drake includes drake-lcm-spy in /opt/drake/bin to plot and visualize LCM messages.\nControlling IIWA from Drake The following diagram shows a typical architecture of a Drake based system for controlling the iiwa.\nDrakeFRIPositionDriver and DrakeFRITorqueDriver are Java applications built using the Sunrise Workbench and running inside the Sunrise Controller. They open an FRI connection at a specified network port, to which an external computer can connect to.\nkuka_driver is a C++ application built using the FRI-Client-SDK-Cpp and runs on the external computer. It communicates with the Sunrise Controller over the FRI/KONI interface. It also publishes and subscribes LCM messages which can be used by other programs to read/write data to the iiwa robot.\nDrake has a built in LCMInterfaceSystem which allows drake systems to publish and subscribe to LCM messages. Other Drake systems make use of these systems to communicate with the hardware.\nDrake IIWA Java Application The Java application runs on the on the Sunrise Controller and opens an FRI connection to which the kuka_driver running on an external computer connects to.\nThe detailed documentation and code is available in drake-iiwa-driver There are two applications\n DrakeFRIPositionDriver DrakeFRITorqueDriver  The DrakeFRIPositionDriver, as the name implies allows controlling the robot in position control mode, taking in joint position commands.\nThe DrakeFRITorqueDriver allows for the control of the robot in impedance control mode and takes in joint position as well as joint feed-forward torque commands. We would be using this mode more often.\nBoth the drivers output robot status like joint positions, velocities, torques etc\nkuka_driver The kuka_driver runs on the external computer, connects to the Java application running on the robot and provides an LCM interface to read/write data.\nIt has to be compiled as in this documentation and requires FRI client SDK for compilation.\nAfter compilation, the kuka_driver should be run first, so as to communicate with IIWA\nIIWA-LCM Interface kuka_driver provides read/write interface to the IIWA through LCM messages. It generates three LCM message channels\n IIWA_STATUS of the type lcmt_iiwa_status, defined in lcmt_iiwa_status.lcm IIWA_COMMAND of the type lcmt_iiwa_command, defined in lcmt_iiwa_command.lcm IIWA_STATUS_TELEMETRY of the type lcmt_iiwa_status_telemetry, defined in lcmt_iiwa_status_telemetry.lcm  By default, kuka_driver publishes/ subscribes these messages at 200Hz\nIIWA_STATUS provides the robot joint status which includes joint position, velocities and torques. An example which subscribes to the IIWA_STATUS and prints the output is available in lcm_examples/iiwa-lcm-listener.py\nIIWA_COMMAND is used to command joint positions with an optional feed forward joint torque. An example which subscribes to IIWA_STATUS to estimate the current robot configuration and move joint 7 incrementally is available in lcm_examples/iiwa-lcm-publisher.py\nIIWA_STATUS_TELEMETRY provides timing information, which can be used to estimate the latency in the FRI communication between the external computer and the robot controller.\nControlling Custom Robot from Drake The Kuka iiwa interface in this documentation can be adapted to interface custom robot manipulators. Following would be the minimum requirements:\n a hardware interface program similar to kuka_driver, which reads and writes from the hardware. As a bare minimum, we should be able to write joint positions and read joint positions from the hardware and pass it on as LCM messages. Define custom LCM messages depending on the hardware capabilities. Take a look at lcmt_iiwa_command.lcm and lcmt_iiwa_status.lcm and others defined in lcmtypes Develop Drake systems that parses the custom LCM messages and interfaces them to other Drake systems, similar to iiwa_status_receiver.py and iiwa_command_sender.py. URDF/SDF model of the robot. This will be used to create the MultibodyPlant which is used to compute the kinematics and dynamics of the system. (NOTE: use OBJ files instead of STL/DAE)  Manipulation Station The manipulation station consists of the IIWA robot, the Drake systems required to communicate and parse the data with the IIWA as well as other optional hardware such as cameras, grippers etc\nThe IiwaManipulationStation, defined in iiwa_manipulation_station.py is a Diagram which has to be included and initialized in the diagram created by the user, so as to communicate with the IIWA. It consists of the following systems:\n - LcmInterfaceSystem - LcmSubscriberSystem - LcmPublisherSystem - IiwaCommandSender - IiwaStatusReceiver - MultibodyPlant  The diagram of the manipulation system looks as follows:\nThe LcmInterfaceSystem has no inputs nor outputs nor state nor parameters; it declares only an update event that pumps LCM messages into their subscribers if the LCM stack has message(s) waiting. The subscribers will then update their outputs using their own declared events\nThe LcmSubscriberSystem subscribes to the LCM data stream and outputs the received data through a single output port.\nThe LCMPublisherSystem has a single input port and outputs the received data to the LCM data stream at a specified update rate.\nThe IiwaStatusReceiver, defined in iiwa_status_receiver.py parses the IIWA_STATUS LCM message into the following vector valued outputs:\n - position_commanded - position_measured - velocity_estimated - torque_commanded - torque_measured - torque_external  An example on using IiwaStatusReceiver is available in example_iiwa_status_receiver.py\nThe IiwaCommandSender, defined in iiwa_command_sender.py encodes the input into IIWA_COMMAND LCM message which is published by the LcmSubscriberSystem. It has the following two vector valued inputs accepting vectors of size 7.\n - position - torque  An example usage of IiwaCommandSender is available in example_iiwa_command_sender.py. Be EXTREMELY CAUTIOUS before running this example, as it would instantaneously command 0 positions to joints. Don\u0026rsquo;t run this example unless all joints are near to zero position.\nThe MultibodyPlant models the external plant that has to be controlled and helps in the computation of its kinematics and dynamics.\nThe input and output ports of the individual systems inside inside a diagram has to be exported to outside so that other drake systems can interface with the inner systems. The ExportOutput and ExportInput methods of DiagramBuilder are used for this.\nExamples The codes of the examples used in this document are available at https://github.com/achuwilson/pydrake_iiwa\nTo run the examples, follow these steps\n STEP1: Make sure that the Kuka is in AUT  mode in SmartPad STEP2: Select and run DrakeFRITorqueDriver Application from the Smartpad STEP3: run kuka_driver in the PC. This will cause the brakes on IIWA to release STEP4: (optional) Start drake-visualizer or meshcat-server for examples requiring visualization STEP4: run the example  Joint Control  example_joint_slider.py\nIn this simple example, we make use of the drake JointSliders system to control the joint values of the robot. The system diagram of the example is as follows:\nThe output port of the JointSliders system is connected through a FirstOrderLowPassFilter to the iiwa_position port of the IiwaHardwareInterface manipulation station. The FirstOrderLowPassFilter helps to smoothen the motion by filtering out high frequency changes in position which may cause jerk.\nVisualizing the robot state in Drake visualizers  example_iiwa_visualize.py\nDrake has multiple visualizers and uses the SceneGraph system to output the visualizations. By default, Drake comes with a VTK based visualizer which is located in /opt/drake/bin/drake-visualizer. We have to launch the visualizer before running the simulation.\nDrake also has a Meshcat based visualizer which can display the output in a browser window. Run meshcat-server present in the same directory. Meshcat visualizer is greatly helpful when running Drake as IPython notebooks in Google Colab\nThe system diagram of the example is as follows:\nIn the example_iiwa_visualize.py example, the MultibodyPositionToGeometryPose system takes in the joint positions of the robot and outputs the pose output required by the SceneGraph system. The DrakeVisualizer and meshcat_visualizer queries the scenegraph system and updates the rendering.\nThe DrakeVisualizer looks as follows:\nAdding an end effector  example_iiwa_endeffector.py\nThis example demonstrates how to add a custom end-effector/gripper to the IiwaManipulationStation. End effector models, either in URDF or SDF format could be imported and added to the MultibodyPlant before finalizing it. It also needs to be welded to the last link of IIWA.\nFollowing image shows IIWA attached with the small blue finger defined in models/onefinger.urdf:\nOnce the end-effector is added to the MultibodyPlant, we can refer to it by the name defined in the URDF file.\nForward Kinematics  example_FK.py\nForward Kinematics calculates the position of end effector/gripper in the world given the joint position values.\nIn this example, the low pass filtered values from the slider is used to set the joint positions of the IIWA hardware. The iiwa_position_measured output port of the manipulation station is connected to the FKSystem, where the joint values of the multibodyplant are updated. The EvalBodyPoseInWorld function can be used to evaluate the position of the body in the world.\nInverse Kinematics  example_IK.py\nInverse Kinematics solves for the joint positions required to reach a particular end-effector pose.\nDrake has a numeric InverseKinematics solver which formulates IK as a nonlinear optimization problem. We can specify non linear inequality constraints like minimum distance between bodies, position/orientation constraints, target gaze constraints etc. Refer to MIT 6.881 Lecture 15, Motion Planning, Part 1 for more insights on Inverse kinematics and declaring constraints. Corresponding IPython Notebook\nDrake also has a Differential Inverse Kinematics solver, which calculates joint velocities using Jacobian and integrates it to calculate the joint position. The example_IK.py uses the differential IK method implemented in differential_ik.py\nThe system diagram of the example_IK.py is as follows:\nEstimating Cartesian Velocities  example_velocity_estimate.py\nEnd effector velocities can be estimated by multiplying the robot Jacobian with joint velocities. Drake MultibodyPlant has the CalcJacobianSpatialVelocity method, which could be used to calculate the Spatial Jacobian.\nThe system diagram of the example is as follows:\nIn example_velocity_estimate.py, The output of EndEffectorTeleop is used to control the cartesian end effector position through the DifferentialIK system. The iiwa_velocity_estimated and iiwa_position_measured outputs of the manipulation station are used by the velocityEstimator system to calculate the Jacobian and corresponding end effector velocities.\nCartesian velocity control  example_velocity_control.py\nWARNING: Pay close attention when running this example on the real robot. The end effector keeps on moving at the commanded velocity, until the slider is moved back to zero.\nJoint velocities required to move the end effector at a desired velocity in cartesian space are computed using an inverse Jacobian controller.\nThe system diagram is as follows:\nThe desired end effector velocity from the slider and the current joint position is fed as inputs to the PseudoInverseVelocityController. It calculates the required joint velocities, which are integrated and fed as iiwa_position input.\nEstimating Cartesian forces  example_force_estimate.py\nIIWA has joint torque sensors at all the 7 joints. This example uses jacobian transpose to estimate the forces and in cartesian space at the end effector from the measured joint torques.\nThe system diagram of the example is as follows:\nIt looks similar to the velocity estimation system, except that in this case, there is a forceEstimator system that takes in iiwa_position_measured and iiwa_torque_external\nHybrid Force-Position control  example_force_feedforward.py\nIn Hybrid force-position demo, the end effector is able to move to/maintain a position as well exert force in arbitrary directions.\nWe make use of the iiwa_feedforward_torque input to provide additional joint torques. These torques are calculated using Jacobian transpose pseudo-inverse in the FeedForwardForceController system. while running the demo, two windows will pop up, one for controlling the cartesian position and the other for the cartesian wrench.\nThe system diagram of the example is as follows:\nMotion Planning and Collision Avoidance TODO #2\nGravity Compensation TODO #3\nHaptic Force Feedback TODO #4\n","date":1610928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1610928000,"objectID":"75a3de2b094b030443445e7542b3c970","permalink":"https://achuwilson.github.io/post/iiwadrake-intro/","publishdate":"2021-01-18T00:00:00Z","relpermalink":"/post/iiwadrake-intro/","section":"post","summary":"A Gentle Introduction to Kuka iiwa and Drake in Python","tags":["iiwa","drake","tutorials"],"title":"A Gentle Introduction to Kuka iiwa and Drake in Python","type":"post"},{"authors":null,"categories":["Software Projects"],"content":"  What and why  Previous works  Status  Going Further  What and Why This is quite recent and an exploratory side project. Recently for another project, I trained a CNN to classify the objects being grasped by a hand which has GelSight based tactile sensors. It worked pretty good (~90% accuracy) on my test objects. Analysing the false classifications indicated that the tactile data may not be perfect during all the grasps. We humans also gets confused in the same way occasionally, if we grab objects with just two fingers. We would then either proceed to close the fingers to make more contact surface area with the object or roll the object between our fingers to classify it. This would then give us more data and increases our belief probability.\nPrevious Works Much research has been done in tactile object recognition as well as in hand manipulation [1], [2]. Unlike those approaches. this work explores on learning a finger movement repertoire, that could maximize the in-hand object recognition/localization capabilities.\nStatus The following video shows the prototype gripper classifying two test objects (geodesic spheres with hexagonal and triangular faces, that can be better felt by touch)\n We can see that it falsely classifies objects once in a while.\nA modular 3rd axis is inserted in between the finger and the gripper, which can rotate the object in hand.\n The classification probabilities during this motion are averaged to get a more accurate estimate of the object.\n(Note: it has been tested only with symmetric objects, which are easy to roll and is still an ongoing project)\nGoing Further More experimentation, 3D reconstruction using techniques using ICP.\n ","date":1605398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1605398400,"objectID":"62d0046664c3460d52ddf9e988fb174e","permalink":"https://achuwilson.github.io/project/2020-in-hand-rolling/","publishdate":"2020-11-15T00:00:00Z","relpermalink":"/project/2020-in-hand-rolling/","section":"project","summary":"In-Hand manipulation for better tactile object detection and shape reconstruction.","tags":["software","wip"],"title":"In-Hand Rolling","type":"project"},{"authors":null,"categories":["Software Projects"],"content":"  What and why  Previous works  How  Status  What and Why BlindGrasp, as the name implies, aims at grasping objects when the robot\u0026rsquo;s eyes are blindfolded. This idea had been lingering in my mind, ever since I came across the GelSight optical tactile sensor. It is a tough and challenging problem and I have been making slow, yet good progress. This is the main project that I am spending my time now and has helped us to be the finalist in the Kuka Innovation Award 2021\nHumans have excellent grasping capabilities in unstructured environments which are still unmatched by robots. This can be attributed to the dexterous human hand, robust visual and tactile sensing capabilities as well as to the intelligent brain which makes conscious and subconscious decisions.\nVisual sensing is traditionally used extensively in robotics when compared to tactile sensing. Tactile sensing is harder, mainly due to the difficulty in interpreting and making sense of the complex tactile signals. The recent developments in machine learning - particularly Deep Reinforcement Learning has opened new possibilities in the making usage of such complex data.\nWe humans can effortlessly put a hand inside a bag, search by moving it around and pick up an item or we can use our hands to dig or sieve through granular medium like sand and pick up an object. This project explores building a system which learns manipulation and grasping skills in such vision denied environments.\nPrevious Works Grasping objects, using tactile sensing alone as well as in granular media are explored more recently. Following are some of the early works.\n  Reactive Grasping Using Optical Proximity Sensors by Kaijen Hsaio et al.  A Deep Learning Approach to Grasping the Invisible, by Yang Yang et al.  Vibro-Tactile Foreign Body Detection in Granular Objects based on Squeeze-Induced Mechanical Vibrations by Togzhan Syrymova et al.  MAT: Multi-Fingered Adaptive Tactile Grasping via Deep Reinforcement Learning by Bohan Wu et al.  Learning to Grasp without Seeing by Adithyavairavan Murali et al.  Multimodal Haptic Perception within Granular Media via Recurrent Neural Networks by S.Jia et al.  There are almost no works combining tactile sensing- exploration and grasping objects in granular media\nHow Bringing such novel manipulation capabilities to robots calls for better robot hands as well as algorithms. The stated goals of the project are\n Building a novel tactile sensing capable gripper Develop AI techniques to make the robot learn how to explore, detect and grasp objects in unstructured-vision denied environments where tactile sensing only could be used.  The Tactile Sensing Gripper consists of a two finger parallel jaw gripper, with tactile sensing capabilities. Similar to the human fingers, the inner surface of the fingers would be having a high spatial resolution tactile sensing capability and the outer surface comparatively lower resolution. The highly sensitive inner surface of the finger helps in recognizing objects from their shape, texture as well as in fine manipulation skills. The lower sensitive outer surface helps in detecting the presence of nearby objects via contact.\nThe AI System consists of a Deep Reinforcement Learning Agent that learns how to explore the environment and to recognize/differentiate objects and pick them up. The agent uses data from the two types of tactile sensors on each of the fingers, proprioception and force sensor data from the robot arm. The agent decides the direction of motion of the end effector and the control of the gripper.\nAs a relatively simple task to start with, grasping specific objects from an environment similar to the following setup will have to be acheived using the tactile sensing alone.\nUltimately, the robot is expected to learn how to pick up objects under the granular media as in the following simulation.\n  Status A novel gripper, with tactile sensing around the finger is built and we are collecting data from the real world environment. This would be augmented with data from the simulated environment and human demonstrations to train the RL agent for the task.\nMore updates coming soon, as it is an ongoing project..!\n ","date":1595808000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1595808000,"objectID":"68c1814dc9ba1cf757693b50cf469717","permalink":"https://achuwilson.github.io/project/2020-blindgrasp/","publishdate":"2020-07-27T00:00:00Z","relpermalink":"/project/2020-blindgrasp/","section":"project","summary":"Grasping in vision denied environments using Tactile Sensing.","tags":["software","wip"],"title":"BlindGrasp","type":"project"},{"authors":["alison","Desirée De Leon"],"categories":["talk"],"content":" This webinar was designed to help educators who needed to quickly transition to remote teaching due to COVID-19.\n Educators create a lot of files for teaching- slides, exercises, solutions, assignments, data, figures- that all ultimately need to be shared with other people. Having a link for sharing your teaching materials can save you time and pain, but it is hard to get started if you’ve never shared your resources online before. In this webinar, we’ll give a tour of the R Markdown ecosystem for educators that you can start to use right away. We’ll show how it can help you make your teaching more shareable, reproducible, and resilient.\nRead the accompanying Q\u0026amp;A blog post on the RStudio Education blog.\n","date":1585645200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1585645200,"objectID":"10e49beba4816cc4d35e850e1344ec41","permalink":"https://achuwilson.github.io/talk/2020-sharing-short-notice/","publishdate":"2020-03-31T09:00:00Z","relpermalink":"/talk/2020-sharing-short-notice/","section":"talk","summary":"How to get your materials online on short notice.","tags":["rmarkdown"],"title":"Sharing on Short Notice","type":"talk"},{"authors":null,"categories":["Software Projects"],"content":"  What and why  Previous works  How  Result  What and Why FingerVision is an optical tactile sensor [1], which provides the RGB image of the object within the fingers in addition to the contact forces from the optical markers.\nSince the sensor captures the image of the background also in addition to the object, we need to seperate out the object and the background.\nPrevious Works to update later\nHow A deep learning approach is used to segment the objects and the background. Training data is collected, manually labelled and a U-Net based network is trained. The training acheives good accuracy in a couple of tens of minutes. Transfer learning methods could be used to quickly adapt to new environments.\nResult It works, as expected.\n  The left side shows the raw sensor image and the right one is the segmented image output.\n ","date":1580083200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1580083200,"objectID":"bdab21f37ee85dd72ddef1d8d997ffe4","permalink":"https://achuwilson.github.io/project/2020-fingervision-seg/","publishdate":"2020-01-27T00:00:00Z","relpermalink":"/project/2020-fingervision-seg/","section":"project","summary":"Background Segmentation for Fingervision Tactile Sensor","tags":["software"],"title":"Background segmentation","type":"project"},{"authors":null,"categories":["Hardware Projects"],"content":"  What and why  Previous works  What makes it special  Status  Going Further  What and Why Chero is my acronym for Chef-Robot, an ongoing project motivated by my desire to have a personal robot arm to experiment with. My long term dream of such a robot was triggered into action by the MIT Collaborative Intelligence Challenge 2019, in which I gave a try. In addition, it as an interesting challenge and learning oppertunity to build such a system from grounds up - mechanical design-electronics controls.\nPrevious Works Direct drive robots are, as the name implies, are robots in which the motors directly drive the shaft without using a gearbox. It was introduced by Asada et al as early as early 80s [1], [2]. Direct drive robots have many inherent advantages like better torque control, dynamic response, zero backlash and friction etc. Despite all these advantages, the main disadvantage that prevented direct drive robots from becoming mainstream is mainly the low torque density of electrical motors. However, recent advancements in technology has brought us high torque BLDC motors, which are used either used directly or in combination with a low gear ratio (\u0026lt;10) reducer. Notable robots include berkeley openarms [3], and numerous quadrupeds like MIT Cheetah 4 and works by Gavin Kenneally et al [5].\nWhat makes it special  Quasi-Direct Drive Spherical shoulder and wrist joints Redundant Robot - 7DOF Cheap and affordable  The Quasi- Direct drive system consists of timing belt driven axes with a reduction ratio of 3. High torque density gimbal BLDC motors are used to actuate these joints.\nThe robot also has spherical joints at both the shoulder and the wrist, which matches closely with human arms. It is shown that such a wrist design can be quite helpful in dexterous manipulation applications [6]\nStatus The design is almost complete, parts machined-3dprinted and a video of first 4 DOF can be seen below   A 3DOF spherical wrist is also designed and assembled. It will be attached soon.\nGoing Further Implementing controllers, integrating with Drake\n ","date":1574812800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1574812800,"objectID":"71b462b6eca5ace9554d5c701eccdd68","permalink":"https://achuwilson.github.io/project/2019-chero/","publishdate":"2019-11-27T00:00:00Z","relpermalink":"/project/2019-chero/","section":"project","summary":"Affordable, Quasi-Direct Drive Robot Manipulator","tags":["hardware","wip"],"title":"Chero","type":"project"},{"authors":["Achu Wilson","Shaoxiong Wang","Branden Romero","Edward Adelson"],"categories":null,"content":"","date":1572912000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572912000,"objectID":"64aa17ced4f7eed8be8be3db101a115a","permalink":"https://achuwilson.github.io/publication/2019-gelsighthand/","publishdate":"2019-11-05T00:00:00Z","relpermalink":"/publication/2019-gelsighthand/","section":"publication","summary":"This work details the design of a novel two finger robot gripper with multiple Gelsight based optical-tactile sensors covering the inner surface of the hand. The multiple Gelsight sensors can gather the surface topology of the object from multiple views simultaneously as well as can track the shear and tensile stress. In addition, other sensing modalities enable the hand to gather the thermal, acoustic and vibration information from the object being grasped. The force controlled gripper is fully actuated so that it can be used for various grasp configurations and can also be used for in-hand manipulation tasks. Here we present the design of such a gripper.","tags":null,"title":"Design of a Fully Actuated Robotic Hand With Multiple Gelsight Tactile Sensors","type":"publication"},{"authors":null,"categories":["Hardware Projects"],"content":"   What and why\n   Results\n   Going Further\n  What and Why This work details the design of a novel two fin-ger robot gripper with multiple Gelsight based optical-tactilesensors covering the inner surface of the hand. The multipleGelsight sensors can gather the surface topology of the objectfrom multiple views simultaneously as well as can track theshear and tensile stress. In addition, other sensing modalitiesenable the hand to gather the thermal, acoustic and vibrationinformation from the object being grasped. The force controlledgripper is fully actuated so that it can be used for various graspconfigurations and can also be used for in-hand manipulationtasks. Here we present the design of such a gripper.\nResults Workshop Paper Video   Going Further In hand manipulation tasks, object localisation\n ","date":1558915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558915200,"objectID":"219266d5c0459f179cb3f5f3b741028d","permalink":"https://achuwilson.github.io/project/2019-gelsighthand/","publishdate":"2019-05-27T00:00:00Z","relpermalink":"/project/2019-gelsighthand/","section":"project","summary":"Fully Actuated Robotic hand with Multiple GelSight Tactile Sensors","tags":["hardware"],"title":"Gelsight Gripper","type":"project"},{"authors":["Daniel Fernandes Gomes","Achu Wilson","Shan Luo"],"categories":null,"content":"","date":1558742400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558742400,"objectID":"ee7d647c10249e5744c859499789bfc8","permalink":"https://achuwilson.github.io/publication/2019-getsight-sim2real/","publishdate":"2019-05-25T00:00:00Z","relpermalink":"/publication/2019-getsight-sim2real/","section":"publication","summary":"Grasping and manipulation of objects are common both in domestic and industrial environments. Recent works exploring learning based solutions have shown promising results on robotic manipulation tasks. One efficient approach for training such learning agents is to train them within a simulated environment, followed by their deployment on real robots (Sim2Real). Most current works leverage camera vision to facilitate such manipulation tasks. However, camera vision might be significantly occluded by robot hands during the manipulation. Tactile sensing is another important sensing modality that offers complementary information to vision and can make up the information loss caused by the occlusion. However, the use of tactile sensing is restricted in the Sim2Real research due to no simulated tactile sensors available in the current simulation platforms. To mitigate the gap, we introduce a novel approach for simulating a GelSight tactile sensor in the commonly used Gazebo simulator. Similar to the real GelSight sensor, the simulated sensor can produce high-resolution images by an optical sensor from the interaction between the touched object and an opaque soft membrane. It can indirectly sense forces, geometry, texture and other properties of the object and enables the research of Sim2Real learning with tactile sensing. Preliminary experiment results have shown that the simulated sensor could generate realistic outputs similar to ones captured by a real GelSight sensor","tags":null,"title":"GelSight Simulation for Sim2Real Learning","type":"publication"},{"authors":null,"categories":["Software Projects"],"content":"  What and why  Previous works  What makes it special  Result  What and Why This work explores simulating Gelsight so that agents can be trained in simulation and transferred to real hardware systems with minimal/no difficulties. This would require high fidelity simulation of the GelSight sensor.\nPrevious Works My initial approach [1] in simulating Gelsight used raytracing techniques to recreate the 3d pointcloud of the contact surface geometry. In reality, a Poisson solver based surface reconstruction is used to reconstruct the depth image and normals from the gradient image.\nWhat makes it special This approach simulates the raw gradient RGB image coming from the sensor.\nResult The following image shows a comparison of the simulated image and the image acquired from the real sensor on similar objects.\nThe top row of images are acquired from the hardware and the bottom one is the simulated one.\nHere is a paper detailing the approach, and presented at ICRA 2019 ViTac Workshop.\n ","date":1556323200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556323200,"objectID":"762ac21ca78ab18f6855a43e2cf66a25","permalink":"https://achuwilson.github.io/project/2019-gelsight_sim2real/","publishdate":"2019-04-27T00:00:00Z","relpermalink":"/project/2019-gelsight_sim2real/","section":"project","summary":"Simulating Gelsight Sensors for sim2 real","tags":["software"],"title":"Gelsight for Sim2Real","type":"project"},{"authors":null,"categories":["hardware Projects"],"content":"  What and why  Previous works  What makes it special  Status 5  What and Why PulleyDrive is a side project exploring speed reduction/torque amplification using the principle of pulleys. This project was supported by the \u0026ldquo;ProjXpo\u0026rdquo; program under MIT Innovation Initiative.\nPrevious Works This work was mainly inspired by the cable-pulley based actuation of the Ambidex robot [1]\nWhat makes it special  Zero Backlash LightWeight High Stiffness Not Expensive  Status The following video shows a prototype in operation.\n      ","date":1553644800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1553644800,"objectID":"705a44de6e527625f1b3a13ddb31aed7","permalink":"https://achuwilson.github.io/project/2019-pulleydrive/","publishdate":"2019-03-27T00:00:00Z","relpermalink":"/project/2019-pulleydrive/","section":"project","summary":"Speed reduction using the principle of pulleys.","tags":["hardware"],"title":"PulleyDrive","type":"project"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"6087c0ef875554f4409ac52928d79279","permalink":"https://achuwilson.github.io/projects/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/projects/","section":"","summary":"See some of the projects I have worked on","tags":null,"title":"Projects","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"8576ec274c98b3831668a172fa632d80","permalink":"https://achuwilson.github.io/about/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/about/","section":"","summary":"A little more about me and how to get in touch","tags":null,"title":"Resume","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"3960dd3bdc6f629fb800d1d2aaa7224f","permalink":"https://achuwilson.github.io/resume/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/resume/","section":"","summary":"A little more about me and how to get in touch","tags":null,"title":"Resume","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"65de3680a280f6bf29dc34fe1adad5a6","permalink":"https://achuwilson.github.io/talks/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/talks/","section":"","summary":"Upcoming and recent talks / workshops","tags":null,"title":"Talks \u0026 Workshops","type":"widget_page"},{"authors":null,"categories":null,"content":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n   ","date":1530140400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530140400,"objectID":"53e892b8b41cc4caece1cfd5ef21d6e7","permalink":"https://achuwilson.github.io/license/","publishdate":"2018-06-28T00:00:00+01:00","relpermalink":"/license/","section":"","summary":"My blog posts are released under a Creative Commons Attribution-ShareAlike 4.0 International License.\n   ","tags":null,"title":"LICENSE: CC-BY-SA","type":"page"},{"authors":["Achu Wilson"],"categories":null,"content":"","date":1527206400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527206400,"objectID":"fe6acd94ee7d2bbbc7aa695e462ce38a","permalink":"https://achuwilson.github.io/publication/2018-finger-icra/","publishdate":"2018-05-25T00:00:00Z","relpermalink":"/publication/2018-finger-icra/","section":"publication","summary":"This work explores a modular gripper with tactile sensing capabilities all-round the surface and not just limited to the inner fingertip. The system makes use of commonly available components to achieve optical tactile sensing by detecting deformations on a soft transparent silicone elastomer skin. Whole body tactile sensing around the gripper not only provides information on the object being grasped, but also of the immediate environment. These contact information can aid in the exploration of unstructured and cluttered environment","tags":null,"title":"Modular, Whole Finger Tactile Sensing Gripper","type":"publication"},{"authors":null,"categories":["Hardware Projects"],"content":"  What and why  Previous works  What makes it special  Status  Going Further  Subsequent Works  What and Why Robot fingers are almost always flat and rectangular. This is owing to their simplicity in design, control and better contact surface area, thereby giving a better grip.\nHowever, human fingers are round and more streamlined. It helps hand to navigate/explore in cluttered environments without much difficulty. In addition, the tactile sensing capability all around the human finger surface helps in making better sense of the environment contacts.\nPrevious Works #to be update later\nWhat makes it special  Round the surface tactile sensing Simple construction Cheap and affordable  The cross sectional view of the finger is as shown below It consists of a cylindrical glass tube coated with a layer of transparent silicone elastomer. The deformable elastomer has colored markers on its outer surface, which is then tracked by a wide angle camera placed at one end of the glass tube. The camera is able to get a 360 degree image of the surface of the finger, which is then de-warped to rectangular images for better understanding and processing. The other end of the tube has a spherical mirror. The reflected image of the markers from the spherical mirror provides view from another angle and is intended to be used in future to reconstruct the three dimensional position of the markers using stereo reconstruction.\nUsing off the shelf components ensured that the cost of the modular tactile finger is kept low. A 75 x 12 mm laboratory test tube is used as the cylindrical tube. The elastomer has Shore A hardness of 15 and is made using high transparent platinum cure silicone, which is molded into a thin sheet and wrapped over the test tube. The parabolic mirror is made using a chrome coated metal ball bearing. The finger is compatible with commonly available webcams and better results were obtained using a wide angle Raspberry Pi Camera.\nA two finger parallel gripper configuration can be easily achieved using such a finger Status The following image shows the raw output from the camera and then it is pre-processed and the dots are tracked for its position Going Further The next steps would be attaching it to a gripper and collecting real world data.\nSubsequent Works These are some subsequent works by others and could be references for future works.\n  GelTip: A Finger-shaped Optical Tactile Sensor for Robotic Manipulation by Daniel Gomes et al. [website]  A Sensorized Multicurved Robot Finger with Data-driven Touch Sensing via Overlapping Light Signals by Pedro Placenza et al. [video]   ","date":1519689600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1519689600,"objectID":"b699a0191abd15c152e0d7218f9a1d6f","permalink":"https://achuwilson.github.io/project/2018-roundfinger/","publishdate":"2018-02-27T00:00:00Z","relpermalink":"/project/2018-roundfinger/","section":"project","summary":"Cylindrical finger with tactile sensing around the surface.","tags":["software","hardware"],"title":"Round Tactile Sensing Finger","type":"project"},{"authors":null,"categories":["Software Projects"],"content":"  What and why  How  Result  What and Why This is my initial approach in modelling the behaviour of GelSight tactile sensors in simulation. I resorted to simulation as I did not have access to the real sensor or resources to build one.\nHow I used the physics simulator Bullet. It had softbody simulation, which I thought would be useful for simulating the elastomer of GelSight. But it turned out that the softbody simulation was a basic and needed much more development. So, I indirectly modelled Gelsight using the raytest functionality in bullet. It returns the depth at which a ray makes contact with a solid body.\nResult Following is the video of the simulated gelsight gripper. The Kuka iiwa has a WSG 50 gripper fitted with a gelsight sensor. The sensor has a sensing area of 24x24mm and has a resolution of 256x256 pixels in the planar sensing area. It outputs standard ROS 3D pointcloud data, which is displayed in RViz.The simulation was run in my laptop and the pointcloud could be generated at 5Hz   Even though I could simulate the sensor, I soon ran into comptational limits of my laptop. \n","date":1511740800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1511740800,"objectID":"c7224c61db76f69714b0c186ab8f3a0f","permalink":"https://achuwilson.github.io/project/2017-gelsight_sim/","publishdate":"2017-11-27T00:00:00Z","relpermalink":"/project/2017-gelsight_sim/","section":"project","summary":"Simulating Gelsight Sensor by generating pointcloud","tags":["software"],"title":"Gelsight Simulation 1","type":"project"},{"authors":["Achu Wilson"],"categories":null,"content":"","date":1496275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496275200,"objectID":"bff335706a87730ac270d47ddb417914","permalink":"https://achuwilson.github.io/publication/2017-magnetorheological-clutch/","publishdate":"2017-06-01T00:00:00Z","relpermalink":"/publication/2017-magnetorheological-clutch/","section":"publication","summary":" This paper proposes a Magneto Rheological linear clutch  for  use  in  human  safe  robotic  applications.  The  force  transmitted to the links of the robot must be precisely controlled for  any  manipulator  if  it  has  to  be  operated  safely  alongside  humans.  The  traditional  approaches  to  this  problem  is  using  various    compliant    actuating    schemes    like    Series    Elastic    Actuators,  Joint  Torque  Control  etc.  Research  on  the  usage  of  smart  materials  that  change  their  properties  on  application  of  electrical or magnetic fields for human safe robots have gained momentum  recently.  Studies  on  the  feasibility  of  Magneto-Rheological   actuators   has   been   done   already.   This   paper   introduces a MR clutch which can control the force transmitted by a linear actuator. The electromechanical model of the linear clutch has been developed, implemented in hardware, and tested using a prototype one Degree of Freedom arm. The design of the clutch is detailed and the performance is characterized thorough a series of experiments. The results suggest that the linear clutch serves well for the precise force control of a linear actuator","tags":null,"title":"Design and Development of a Magneto-Rheological Linear Clutch for Force controlled Human Safe Robots","type":"publication"},{"authors":null,"categories":["Software Projects"],"content":"  What and why  Previous Works  Results  Status  References  What and Why This is a side project to explore whether quadrupeds can be made to land on all four legs safely after they are dropped/thrown in random orientations.\nPrevious Works  Cat righting reflex been extensively studied before. In 1942 US Air Force played with kittens in microgravity. The same lab, in 1962, published a report titled Weightless Man: Self-Rotation Techniques, as a guidline for future astronauts on how to move around in zero gravity. 1960s. The falling cat was also studiedextensively by NASA to prepare the astronauts for Zero-G environments. A 1969 paper titled A Dynamical Explanation of the Falling Cat Phenomenon explains this.\nPast research attributes the cat reflex to law of conservation of momentem. Flexible bodies can generate internal forces and moments, by the motion of limbs. But the rest of the body rotates in opposite direction owing to the conservation of momentum, so that net angular momentum remains zero. However the rate of rotation can be controlled by extending or pulling back the limbs. Wikipedia summarises it to three key steps:\n1.Bend in the middle so that the front half of their body rotates about a different axis from the rear half. 2.Tuck their front legs in to reduce the moment of inertia of the front half of their body and extend their rear legs to increase the moment of inertia of the rear half of their body so that they can rotate their front further (as much as 90°) while the rear half rotates in the opposite direction less (as little as 10°). 3.Extend their front legs and tuck their rear legs so that they can rotate their rear half further while their front half rotates in the opposite direction less.\nDepending on the cat’s flexibility and initial angular momentum, if any, the cat may need to perform steps two and three repeatedly in order to complete a full 180° rotation.\nPast works on using active spine in legged robots specialized rotation in the axis perpendicular to the saggital plane and was aimed at improving the gait. My research focusess on rotation in the axis perpendicular to the transverse plane. I developed an inertial reoreintation controller. It controls the extension and tucking in of the quadruped limbs, so that the cranial and caudal part rotates with different velocities owing to the conserved momentum. The controller thus uses the inertia of the quadruped to reorient itself.\nResults A quadruped was modelled in the Bullet simulator and a simple state machine based controller was implemented. It works beautifully as in the following video\n  Status As the system is verified in simulation, now it has to be implemented in hardware. I am working on a small quadruped during freetime and it will be updated later.\nHere are some photos of it in progress.\n References will be updated later as time permits.\n ","date":1495843200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1495843200,"objectID":"50e4e25118a8cffba198e69dcda79c32","permalink":"https://achuwilson.github.io/project/2017-activespine_quadruped/","publishdate":"2017-05-27T00:00:00Z","relpermalink":"/project/2017-activespine_quadruped/","section":"project","summary":"Mid air inertial re-orientation using an active spine","tags":["software"],"title":"Quadruped with Active spine","type":"project"},{"authors":null,"categories":["Hardware Projects"],"content":"   What and why\n   Result\n  What and Why This work proposes a Magneto Rheological linear clutch for use in human safe robotic applications. The force transmitted to the links of the robot must be precisely controlled for any manipulator if it has to be operated safely alongside humans. The traditional approaches to this problem is using various compliant actuating schemes like Series Elastic Actuators, Joint Torque Control etc. Research on the usage of smart materials that change their properties on application of electrical or magnetic fields for human safe robots have gained momentum recently. Studies on the feasibility of Magneto-Rheological actuators has been done already. This paper introduces a MR clutch which can control the force transmitted by a linear actuator. The electromechanical model of the linear clutch has been developed, implemented in hardware, and tested using a prototype one Degree of Freedom arm. The design of the clutch is detailed and the performance is characterized thorough a series of experiments. The results suggest that the linear clutch serves well for the precise force control of a linear actuator.\nResult Conference Paper Video    ","date":1493251200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493251200,"objectID":"6d65002201f0609ae05d44913073c9f6","permalink":"https://achuwilson.github.io/project/2017-mrclutch/","publishdate":"2017-04-27T00:00:00Z","relpermalink":"/project/2017-mrclutch/","section":"project","summary":"Magneto-Rheological Clutch for safe Human-Robot interaction.","tags":["hardware"],"title":"MR CLutch","type":"project"},{"authors":["Achu Wilson","Aronin P","Akhil A"],"categories":null,"content":"Publication Number WO/2017/051263\nPublication Date 30.03.2017\nInternational Application No. PCT/IB2016/053292\nInternational Filing Date 04.06.2016\nIPC G06F 11/22 2006.01\n","date":1490832000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1490832000,"objectID":"cf0d19d67e20d7fcd7cc6f23aaa3ccd0","permalink":"https://achuwilson.github.io/publication/2017-touchtest-patent/","publishdate":"2017-03-30T00:00:00Z","relpermalink":"/publication/2017-touchtest-patent/","section":"publication","summary":" The present invention pertains to a robotic arm to test the functionality of a touchscreen panel of a computing device. It consists of a stylus which is adapted to move into three dimensional space for emulating various touch based movements on to the touchscreen panels to provide commands to the computing device, and the stylus further comprises a stylus tip. There are one or more than one rotating motor which is adapted to move the stylus in a plane and a linear actuator which moves the stylus in an axis vertical to the plane.","tags":null,"title":"Robot arm for testing of touchscreen applications ","type":"publication"},{"authors":null,"categories":["Software Projects"],"content":"  What and why  How  Result  What and Why 2048 [1] is a single-player sliding block puzzle game. The objective of the game is to slide numbered tiles on a grid to combine them to create a tile with the number 2048.\nThis was a weekend hobby project, to make use of a robot arm, computer vision and AI techniques to solve and play the game.\nHow The state of the game is inferred from the smartphone screen. Screenshots are perodically captured via the ADB interface. The image is pre-processed and an OCR is done to detect the digits on each tile.\nGiven the current game state, an alpha-beta pruning algorithm predicts the next best action. The action space is swipe in Left/Right/Up/Bottom directions. The robot arm then would move towards the screen and make the swipe\nResult See for yourself!.\n  It not only wins the game, but also scores unbelievable scores.\n ","date":1445904000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1445904000,"objectID":"ac73028a982fd6ac604e279359ef3e19","permalink":"https://achuwilson.github.io/project/2015-2048/","publishdate":"2015-10-27T00:00:00Z","relpermalink":"/project/2015-2048/","section":"project","summary":"Solving a 2048 game using AI, computer vision and a real robot","tags":["software"],"title":"2048 Solver Robot","type":"project"},{"authors":null,"categories":["Software Projects"],"content":"  What and why  Results  What and Why This was a small project done for the qualification round of MBZIRC Challenge 2015. The drone would lift off, move in a circular pattern with incrasing readius and searching for the marker using the downward facing camera. Once the marker is localized, a simple path planner is used to plan the trajectory to land the drone on the rover.\nResults Even though we didnt make it to the finals, it was a good learning experience.\nHere is a video of it.    ","date":1430092800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1430092800,"objectID":"0ca24ee3ff58105ab4e41beb6b16e89f","permalink":"https://achuwilson.github.io/project/2015-quadrotor_landing/","publishdate":"2015-04-27T00:00:00Z","relpermalink":"/project/2015-quadrotor_landing/","section":"project","summary":"Landing a drone on static and dynamic targets.","tags":["software"],"title":"Autonomous Drone Landing","type":"project"},{"authors":null,"categories":["Hardware Projects"],"content":"  What and why  What and Why Chippu was a small social robot made as a side project. It could understand speech commands, make conversations, recocnize faces and navigate on his own using SLAM and help me with my emails and calender, like an assistant. All the functionalities was implemented using the publically available packages in ROS. This was an attempt to learn and get familiar with ROS by building something.\nHere are some of the videos of chippu.\nJust moving around   Speech recognition was implemented using Julius and trained with my voice.    ","date":1319673600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1319673600,"objectID":"573c06c8dffd585a63c9a400c645841b","permalink":"https://achuwilson.github.io/project/2010-chippu/","publishdate":"2011-10-27T00:00:00Z","relpermalink":"/project/2010-chippu/","section":"project","summary":"A ROS running, personal service robot","tags":["hardware","software"],"title":"Chippu","type":"project"},{"authors":null,"categories":["Hardware Projects"],"content":"   What and why\n   Results\n  What and Why This is my undergraduate college project.\nA high altitude platfrom based communication system, which can be rapidly deployed using a balloon is designed and implemented. It is similar to the Project Loon by Google, which was later announced.\nThis page will be updated later as time permits and more details are here\nResults We could not deploy it on a real weather balloon as we were constrained by resources. Instead, the developed system was fixed at the top of a high rise building and we were able to make long distance communication beyond line of sight, using the system as a transponder.\nIn addition, we won the Best Project Award from the college for the year of 2012.\n ","date":1303862400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1303862400,"objectID":"174f34189be233f41b05c856891ffcd6","permalink":"https://achuwilson.github.io/project/2011-haps/","publishdate":"2011-04-27T00:00:00Z","relpermalink":"/project/2011-haps/","section":"project","summary":"High Altitude Platform Based Communication System.","tags":["embedded"],"title":"HAPS","type":"project"},{"authors":null,"categories":["Hardware Projects"],"content":"RollerBot is a two wheeled, cylindrical robot, which could be used for remote surveillance applications. It drew inspiration from the Recon Scout ThrowBot [1]\nThe main body is made from a PVC pipe which houses the batteries, motors, Camera+Mic, AV transmitter and the RF remote control receiver. The robot is controlled from a remote handheld controller over an RF link. The wheels are custom made and a small leg provides the reaction force for the wheels.\n ","date":1272326400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1272326400,"objectID":"7875942e85bf7df414ba39f9d76211ca","permalink":"https://achuwilson.github.io/project/2009-rollerbot/","publishdate":"2010-04-27T00:00:00Z","relpermalink":"/project/2009-rollerbot/","section":"project","summary":"A two wheeled rolling surveillance robot.","tags":["Hardware"],"title":"RollerBot","type":"project"},{"authors":null,"categories":["Software Projects"],"content":"  What and why  How  Results  What and Why This is another hobby project. Any display solution with a VGA input can be used a the notice board. Users text the messages to be displayed, along with an access code to a predefined number. The system would them display the messgae on the display\nHow The system is built around an AVR microcontroller and a GSM modem. The GSM modem has a SIM card provided by the netork provider and can receive text messages. The microcontroller controls the modem and reads the incomming messages through AT commands over a serial port. The user has to send the text message prepended with the security access code. This prevents unauthorized people from spamming the system,\nOnce the text to be displayed is received and authenticated. The AVR microcontroller sends it over to the display as a VGA signal. Even though the tiny AVR controller does not have a VGA port, VGA signals are generated by bitbanging through the SPI port.\nResults Here is a video of the system in operation.    ","date":1269648000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1269648000,"objectID":"1c17390c8f7a6ddd99ba2accfe69b89e","permalink":"https://achuwilson.github.io/project/2009-smart_notice_board/","publishdate":"2010-03-27T00:00:00Z","relpermalink":"/project/2009-smart_notice_board/","section":"project","summary":"Notice board which can be updated by texting from a phone","tags":["software","embedded"],"title":"Smart Notice Board","type":"project"},{"authors":null,"categories":["Hardware Projects"],"content":"  What and why  How  Result  What and Why This is a small side project during my undergraduate days, to master embedded systems and robotics skills.\nHow The robot car consists of IR distance sensors on its sides and wheel encoders. These data streams are fed into a state machine, which looks for empty parking spots. If found any, the state machine would initialize the parking subroutine, and executes a parking maneuver.\n The above image shows the setup used.\nResult Some trials!    ","date":1238112000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1238112000,"objectID":"9423d2cdb0f99a22413f4a0b3c5515f8","permalink":"https://achuwilson.github.io/project/2009-self-parking-car/","publishdate":"2009-03-27T00:00:00Z","relpermalink":"/project/2009-self-parking-car/","section":"project","summary":"A simple self parking car usinf IR distance sensors.","tags":["embedded","hardware"],"title":"Self parking car","type":"project"}]