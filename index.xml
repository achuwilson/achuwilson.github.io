<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Achu Wilson</title>
    <link>https://achuwilson.github.io/</link>
      <atom:link href="https://achuwilson.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Achu Wilson</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Achu Wilson 2021</copyright><lastBuildDate>Wed, 27 Jan 2021 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://achuwilson.github.io/img/pom-card.png</url>
      <title>Achu Wilson</title>
      <link>https://achuwilson.github.io/</link>
    </image>
    
    <item>
      <title>Nonprehensile Manipulation</title>
      <link>https://achuwilson.github.io/project/2021-edgepivot/</link>
      <pubDate>Wed, 27 Jan 2021 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2021-edgepivot/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#how&#34;&gt;How&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#results&#34;&gt;Results&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;a href=&#34;#what-and-why&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Humans can manipulate objects in interesting ways without even grasping them. We make use of the properties of the object as well as the environment to push, flip, throw and squeeze objects.&lt;/p&gt;
&lt;p&gt;The following video shows a human hand pushing a paper box on a table and pivoting it around an edge. We can do such a task just by relying on the forces felt on the finger.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;handdemo.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The 
&lt;a href=&#34;http://manipulation.csail.mit.edu&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MIT 6.881 Manipulation class&lt;/a&gt; Fall 2020, which was offered entirely online, had a class project and I decided to try this. A Kuka LBR iiwa robot, fitted with a simple single finger end effector is used to achieve the task. The robot does not have the model of the object nor the environment (No prior knowledge of the dimensions of the box or the position coordinates of the edge). The workspace limitations of the robot restrict moving the object under the table, but the method could be extended for any such capable robot.&lt;/p&gt;



&lt;h2 id=&#34;how&#34;&gt;How&lt;a href=&#34;#how&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Since the model of the object and environment is not provided and no external vision sensors are used, proprioceptive data alone is to be used. The joint torques and position from the robot are used in combination with the impedance control capabilities of the iiwa to estimate the model parameters. This would be an almost impossible or incredibly difficult task with a conventional position-controlled stiff robot, even if equipped with a Force/Torque sensor. The entire system is implemented in Drake as a state machine. The following paragraphs detail the approach used.&lt;/p&gt;
&lt;p&gt;Initially, the robot moves at a constant velocity in the negative z-axis. Individual joint velocities are calculated from the desired velocity using a PsuedoInverseJacobian controller. Since the iiwa does not have a joint velocity input, the computed joint velocities are integrated to obtain the joint position inputs.
&lt;img src=&#34;push0.png&#34; alt=&#34;&#34;&gt;
While the robot is moving at such a constant velocity, the force on the end effector is being monitored. IIWA provides joint torques estimated at the 7 joints. The external wrench at the end effector is calculated using the PsuedoInverseJacobian method. When the z-axis force is above an experimentally determined threshold, the state machine jumps into the next state.&lt;/p&gt;
&lt;p&gt;Once the finger touches the box, it applies a downward force and a forward velocity such that the resultant force is outside the friction cone of the contact between the box and table. Since the friction coefficient is unknown, the forces are determined experimentally.&lt;/p&gt;
&lt;p&gt;The Drake Kuka driver allows us to provide a feedforward torque to each joint. The individual feedforward torque values are calculated from the desired end-effector wrench using the Jacobian transpose methods and commanded.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;push1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;As the finger approaches the edge of the table, interesting things start to happen.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;push2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;As the center of gravity moves past the edge, the gravitational force acting on it causes a torque centered around the edge. This torque generates an upwards force on the fingers (green up arrows). Since the iiwa operates in impedance control mode, it is not very stiff in the z-axis. So, it deviates minutely from the previous positions in the vertical direction, moving in an arc centered around the corner. The derivative of z-axis force is used to detect the beginning of this deviation and is used to transition the state machine to the next state.&lt;/p&gt;
&lt;p&gt;Once the box starts pivoting, the finger continues moving in the x-axis, until the vertical z-axis reaction force generated by the box on the finger is zero. The gravitational force on the object would be balanced by the horizontal component of the forces on the finger and the corner. The motion of the finger is stopped and any incremental motion will cause the object to fall.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;push3.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The following plot shows the z axis finger coordinates during this time and we can clearly see the arc.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;data2.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The following graph shows the forces and their derivatives on the finger until the box is balanced between the finger and the corner (T3).&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;data1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;From 0 to T1, the finger moves down at the constant velocity. The force readings are quite noisy when the finger is not in contact with anything and the accuracy is only +/-5N.&lt;/li&gt;
&lt;li&gt;The spike at T1 is caused by contact with the object.&lt;/li&gt;
&lt;li&gt;From T1 to T2, The finger moves towards the edge, applying a constant force in the z-axis.&lt;/li&gt;
&lt;li&gt;At T2, the box starts pivoting due to torque induced by gravity. the derivative of the force is found to be a reliable estimate to detect this rather than the raw noisy force data.&lt;/li&gt;
&lt;li&gt;From T1 to T3, the z-axis force peaks and then drops. It is this force that causes the compliant z-axis to deviate in position and move in an arc, which is shown in the plot of pZ (z-axis position).&lt;/li&gt;
&lt;li&gt;At T3, the box is balanced perfectly between the finger and the corner. The force on the z-axis drops while the x-axis force increases. The motion of the finger is stopped at this point.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The points on the arc are observed and fitted onto a model of the circle to estimate the model parameters: the position coordinate of the corner and the size of the object.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;push4.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once we estimate the center and radius from the arc, we plan a trajectory along the perimeter, starting from the current finger position and finishing at a position when the box is vertical.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;push5.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The trajectory includes the positions of each frame (blue dots) as well as the forces (green arrows) at all these frames. This makes sure that while pivoting, the vector sum of forces direct towards the corner position.&lt;/p&gt;
&lt;p&gt;The robot then moves along the trajectory.
&lt;img src=&#34;push6.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Once the finger reaches the end of the above trajectory, a new trajectory is planned, along the same path to pivot the box back to the horizontal position. While executing this new trajectory, fingers move along the circular path, with tangential velocity and radial force components.&lt;/p&gt;
&lt;p&gt;Once the box is nearly vertical, the state machine transitions into a new state, in which the finger moves a little up or down to maintain a pre-determined vertical force on the box. This accounts for the accumulated position errors caused during the estimation of the model parameters. Once it is done, the state machine transitions it to the final state where the finger moves horizontally, moving the box to the initial position.&lt;/p&gt;



&lt;h2 id=&#34;results&#34;&gt;Results&lt;a href=&#34;#results&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The following video (3x sped up) shows the system in action.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/hlw0aVvvHLQ&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;&lt;br&gt;
The approach works reliably and is tested with boxes of different sizes and weights as well as for different table positions.&lt;/p&gt;
&lt;p&gt;The following areas could be improved for better performance.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The system cannot detect the slip that might occur. A tactile sensorized finger (GelSight?) could be used for this.&lt;/li&gt;
&lt;li&gt;The joint torques reported by iiwa are not very accurate and are also pose dependant. This could be improved using an external F/T sensor.&lt;/li&gt;
&lt;li&gt;The estimated model may not be always perfectly accurate. The inaccuracy in the estimated edge position causes a jerking motion when we start applying force towards the center.&lt;/li&gt;
&lt;li&gt;The finger position not only depends on the commanded position but the feed-forward torque also, this makes the transition between feed-forward torque control mode and trajectory execution mode tricky and causes little jerks.&lt;/li&gt;
&lt;/ul&gt;
&lt;/DIV&gt;
</description>
    </item>
    
    <item>
      <title>In-Hand Rolling</title>
      <link>https://achuwilson.github.io/project/2020-in-hand-rolling/</link>
      <pubDate>Sun, 15 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2020-in-hand-rolling/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#previous-works&#34;&gt;Previous works&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#status&#34;&gt;Status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#going-further&#34;&gt;Going Further&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;a href=&#34;#what-and-why&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;This is quite recent and an exploratory side project to explore classification of visually similar yet physical objects. The best example would be geodesic polyhedrons of different frequencies, which can trick visual - both color and depth systems as follows.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;rgb-depth.jpg&#34; alt=&#34;&#34;&gt;.&lt;/p&gt;
&lt;p&gt;The surface geometry of the object could be estimated using a robot gripper which has a GelSight sensor. A CNN using MobileNet architecture  was trained to classify the objectsfrom the raw GelSight data. It worked pretty good (~90% accuracy) on the test objects. Analyzing the false classifications indicated that the tactile data may not be perfect during all the grasps. We humans also gets confused in the same way occasionally, if we grab objects with just two fingers. We would then either proceed to close the fingers to make more contact surface area with the object or roll the object between our fingers as in the following video to classify it.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;handroll.gif&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;This would then give us more tactile data and increases our belief probability. This work involves exploring whether such a capability can improve tactile sensing for robots.&lt;/p&gt;



&lt;h2 id=&#34;previous-works&#34;&gt;Previous Works&lt;a href=&#34;#previous-works&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Much research has been done in tactile object recognition as well as in hand manipulation
&lt;a href=&#34;https://core.ac.uk/download/pdf/77000058.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;, 
&lt;a href=&#34;https://ieeexplore.ieee.org/document/7363508&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2]&lt;/a&gt;. Unlike those approaches. this work explores on learning a finger movement repertoire, that could maximize the in-hand object recognition/localization capabilities.&lt;/p&gt;



&lt;h2 id=&#34;status&#34;&gt;Status&lt;a href=&#34;#status&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The following video shows the prototype gripper classifying two test objects (geodesic spheres with hexagonal and triangular faces, that can be better felt by touch)&lt;/p&gt;
&lt;iframe src=&#34;https://drive.google.com/file/d/1bOfoukle06T8DrrSyyc8ingNUB5qe8He/preview&#34; width=&#34;640&#34; height=&#34;360&#34; align =&#34;center&#34; &gt;&lt;/iframe&gt;
&lt;p&gt;We can see that it falsely classifies objects once in a while.&lt;/p&gt;
&lt;p&gt;A modular 3rd axis is inserted in between the finger and the gripper, which can rotate the object in hand.&lt;/p&gt;
&lt;iframe src=&#34;https://drive.google.com/file/d/1L_x2ZVm-HVFxSDLjEirT3ZcQ40vBXKNi/preview&#34; width=&#34;640&#34; height=&#34;360&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;The classification probabilities during this motion are averaged to get a more accurate estimate of the object.&lt;/p&gt;
&lt;p&gt;(Note: it has been tested only with symmetric objects, which are easy to roll and is still an ongoing project)&lt;/p&gt;



&lt;h2 id=&#34;going-further&#34;&gt;Going Further&lt;a href=&#34;#going-further&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;More experimentation, primarily on the following would be explored as time permits.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3D reconstruction using techniques using ICP.&lt;/li&gt;
&lt;li&gt;Recursive Neural network to extract temporal data during rolling.&lt;/li&gt;
&lt;/ul&gt;
&lt;/DIV&gt;
</description>
    </item>
    
    <item>
      <title>BlindGrasp</title>
      <link>https://achuwilson.github.io/project/2020-blindgrasp/</link>
      <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2020-blindgrasp/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#previous-works&#34;&gt;Previous works&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#how&#34;&gt;How&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#status&#34;&gt;Status&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;a href=&#34;#what-and-why&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;BlindGrasp, as the name implies, aims at grasping objects when the robot&amp;rsquo;s eyes are blindfolded. This idea had been lingering in my mind, ever since I came across the GelSight optical tactile sensor. It is a tough and challenging problem and I have been making slow, yet good progress. This is the main project that I am spending my time now and has helped us to be the finalist in the &lt;em&gt;Kuka Innovation Award 2021&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Humans have excellent grasping capabilities in unstructured environments which are still unmatched by robots. This can be attributed to the dexterous human hand, robust visual and tactile sensing capabilities as well as to the intelligent brain which makes conscious and subconscious decisions.&lt;/p&gt;
&lt;p&gt;Visual sensing is traditionally used extensively in robotics when compared to tactile sensing. Tactile sensing is harder, mainly due to the difficulty in interpreting and making sense of the complex tactile signals. The recent developments in machine learning - particularly Deep Reinforcement Learning has opened new possibilities in the making usage of such complex data.&lt;/p&gt;
&lt;p&gt;We humans can effortlessly put a hand inside a bag, search by moving it around and pick up an item or we can use our hands to dig or sieve through granular medium like sand and pick up an object. This project explores building a system which learns manipulation and grasping skills in such vision denied environments.&lt;/p&gt;



&lt;h2 id=&#34;previous-works&#34;&gt;Previous Works&lt;a href=&#34;#previous-works&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Grasping objects, using tactile sensing alone as well as in granular media are explored more recently. Following are some of the early works.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;http://www.robotics.stanford.edu/~ang/papers/icra09-ProximityGrasping.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reactive Grasping Using Optical Proximity Sensors&lt;/a&gt; by Kaijen Hsaio et al.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://choice.umn.edu/deep-learning-approach-grasping-invisible&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Deep Learning Approach to Grasping the Invisible&lt;/a&gt;, by Yang Yang et al.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9158928&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vibro-Tactile Foreign Body Detection in Granular Objects based on Squeeze-Induced Mechanical Vibrations&lt;/a&gt; by Togzhan Syrymova et al.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://arxiv.org/abs/1909.04787&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MAT: Multi-Fingered Adaptive Tactile Grasping via Deep Reinforcement Learning&lt;/a&gt; by Bohan Wu et al.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://arxiv.org/abs/1805.04201&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning to Grasp without Seeing&lt;/a&gt; by Adithyavairavan Murali et al.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#&#34;&gt;Multimodal Haptic Perception within Granular Media via Recurrent Neural Networks&lt;/a&gt; by S.Jia et al.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are almost no works combining tactile sensing- exploration and grasping objects in granular media&lt;/p&gt;



&lt;h2 id=&#34;how&#34;&gt;How&lt;a href=&#34;#how&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Bringing such novel manipulation capabilities to robots calls for better robot hands as well as algorithms. The stated goals of the project are&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Building a novel tactile sensing capable gripper&lt;/li&gt;
&lt;li&gt;Develop AI techniques to make the robot learn how to explore, detect and grasp objects in unstructured-vision denied environments where tactile sensing only could be used.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The Tactile Sensing Gripper consists of a two finger parallel jaw gripper, with tactile sensing capabilities. Similar to the human fingers, the inner surface of the fingers would be having a high spatial resolution tactile sensing capability and the outer surface comparatively lower resolution. The highly sensitive inner surface of the finger helps in recognizing objects from their shape, texture as well as in fine manipulation skills. The lower sensitive outer surface helps in detecting the presence of nearby objects via contact.&lt;/p&gt;
&lt;p&gt;The AI System consists of a Deep Reinforcement Learning Agent that learns how to explore the environment and to recognize/differentiate objects and pick them up. The agent uses data from the two types of tactile sensors on each of the fingers, proprioception and force sensor data from the robot arm. The agent decides the direction of motion of the end effector and the control of the gripper.&lt;/p&gt;
&lt;p&gt;As a relatively simple task to start with, grasping specific objects from an environment similar to the following setup will have to be acheived using the tactile sensing alone.&lt;/p&gt;





&lt;img src=&#34;https://achuwilson.github.io/project/2020-blindgrasp/pick1_hu5216c571185a4b1be355b78cbb1a0511_203481_450x450_fit_lanczos_2.png&#34; width=&#34;415&#34; height=&#34;450&#34; alt=&#34;picking task 1&#34;&gt;

&lt;p&gt;Ultimately, the robot is expected to learn how to pick up objects under the granular media as in the following simulation.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/a2rk8dN3KsA&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;




&lt;h2 id=&#34;status&#34;&gt;Status&lt;a href=&#34;#status&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;A novel gripper, with tactile sensing around the finger is built and we are collecting data from the real world environment. This would be augmented with data from the simulated environment and human demonstrations to train the RL agent for the task.&lt;/p&gt;
&lt;p&gt;More updates coming soon, as it is an ongoing project..!&lt;/p&gt;
&lt;/DIV&gt;
</description>
    </item>
    
    <item>
      <title>Sharing on Short Notice</title>
      <link>https://achuwilson.github.io/talk/2020-sharing-short-notice/</link>
      <pubDate>Tue, 31 Mar 2020 09:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/talk/2020-sharing-short-notice/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;This webinar was designed to help educators who needed to quickly transition to remote teaching due to COVID-19.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Educators create a lot of files for teaching- slides, exercises, solutions, assignments, data, figures- that all ultimately need to be shared with other people. Having a link for sharing your teaching materials can save you time and pain, but it is hard to get started if you’ve never shared your resources online before. In this webinar, we’ll give a tour of the R Markdown ecosystem for educators that you can start to use right away. We’ll show how it can help you make your teaching more shareable, reproducible, and resilient.&lt;/p&gt;
&lt;p&gt;Read the accompanying Q&amp;amp;A blog post on the 
&lt;a href=&#34;https://education.rstudio.com/blog/2020/04/sharing-on-short-notice/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;RStudio Education blog&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Background segmentation</title>
      <link>https://achuwilson.github.io/project/2020-fingervision-seg/</link>
      <pubDate>Mon, 27 Jan 2020 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2020-fingervision-seg/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#previous-works&#34;&gt;Previous works&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#how&#34;&gt;How&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#result&#34;&gt;Result&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;a href=&#34;#what-and-why&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;FingerVision is an optical tactile sensor
&lt;a href=&#34;https://ieeexplore.ieee.org/document/7803400&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;, which provides the RGB image of the object within the fingers in addition to the contact forces from the optical markers.&lt;/p&gt;
&lt;p&gt;Since the sensor captures the image of the background also in addition to the object, we need to seperate out the object and the background.&lt;/p&gt;



&lt;h2 id=&#34;previous-works&#34;&gt;Previous Works&lt;a href=&#34;#previous-works&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;to update later&lt;/em&gt;&lt;/p&gt;



&lt;h2 id=&#34;how&#34;&gt;How&lt;a href=&#34;#how&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;A deep learning approach is used to segment the objects and the background. Training data is collected, manually labelled and a U-Net based network is trained.
The training acheives good accuracy in a couple of tens of minutes. Transfer learning methods could be used to quickly adapt to new environments.&lt;/p&gt;



&lt;h2 id=&#34;result&#34;&gt;Result&lt;a href=&#34;#result&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;It works, as expected.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/XZOl3pS5oLY&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The left side shows the raw sensor image and the right one is the segmented image output.&lt;/p&gt;
&lt;/DIV&gt;
</description>
    </item>
    
    <item>
      <title>Chero</title>
      <link>https://achuwilson.github.io/project/2019-chero/</link>
      <pubDate>Wed, 27 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2019-chero/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#previous-works&#34;&gt;Previous works&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#what-makes-it-special&#34;&gt;What makes it special&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#status&#34;&gt;Status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#going-further&#34;&gt;Going Further&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;a href=&#34;#what-and-why&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Chero is my acronym for  Chef-Robot, an ongoing project motivated by my desire to have a personal robot arm to experiment with. My long term dream of such a robot was triggered into action by the  MIT Collaborative Intelligence Challenge 2019, in which I gave a try. In addition, it as an interesting challenge and learning oppertunity to build such a system from grounds up - mechanical design-electronics controls.&lt;/p&gt;



&lt;h2 id=&#34;previous-works&#34;&gt;Previous Works&lt;a href=&#34;#previous-works&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Direct drive robots are, as the name implies, are robots in which the motors directly drive the shaft without using a gearbox. It was introduced by Asada et al as early as early 80s 
&lt;a href=&#34;https://www.ri.cmu.edu/pub_files/pub4/asada_haruhiko_1981_1/asada_haruhiko_1981_1.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;,
&lt;a href=&#34;https://mitpress.mit.edu/books/direct-drive-robots&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2]&lt;/a&gt;. Direct drive robots have many inherent advantages like better torque control, dynamic response, zero backlash and friction etc. Despite all these advantages, the main disadvantage that prevented direct drive robots from becoming mainstream is mainly the low torque density of electrical motors. However, recent advancements in technology has brought us high torque BLDC motors, which are used either used directly or in combination with a  low gear ratio (&amp;lt;10) reducer. Notable robots include berkeley openarms
&lt;a href=&#34;https://berkeleyopenarms.github.io/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[3]&lt;/a&gt;,   and numerous quadrupeds like MIT Cheetah 
&lt;a href=&#34;https://ieeexplore.ieee.org/document/6631038&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;4&lt;/a&gt; and works by Gavin Kenneally et al
&lt;a href=&#34;https://ieeexplore.ieee.org/document/7403902&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[5]&lt;/a&gt;.&lt;/p&gt;



&lt;h2 id=&#34;what-makes-it-special&#34;&gt;What makes it special&lt;a href=&#34;#what-makes-it-special&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Quasi-Direct Drive&lt;/li&gt;
&lt;li&gt;Spherical shoulder and wrist joints&lt;/li&gt;
&lt;li&gt;Redundant Robot - 7DOF&lt;/li&gt;
&lt;li&gt;Cheap and affordable&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The Quasi- Direct drive system consists of timing belt driven axes with a reduction ratio of 3. High torque density gimbal BLDC motors are used to actuate these joints.&lt;/p&gt;
&lt;p&gt;The robot also has spherical joints at both the shoulder and the wrist, which matches closely with human arms.  It is shown that such a wrist design can be quite helpful in dexterous manipulation applications 
&lt;a href=&#34;https://ieeexplore.ieee.org/document/8624352&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[6]&lt;/a&gt;&lt;/p&gt;



&lt;h2 id=&#34;status&#34;&gt;Status&lt;a href=&#34;#status&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The design is almost complete, parts machined-3dprinted and a video of first 4 DOF can be seen below

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/R1Q4KODC1f4&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;A 3DOF spherical wrist is also designed and assembled. It will be attached soon.&lt;/p&gt;



&lt;h2 id=&#34;going-further&#34;&gt;Going Further&lt;a href=&#34;#going-further&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Implementing controllers, integrating with Drake&lt;/p&gt;
&lt;/DIV&gt;
</description>
    </item>
    
    <item>
      <title>Design of a Fully Actuated Robotic Hand With Multiple Gelsight Tactile Sensors</title>
      <link>https://achuwilson.github.io/publication/2019-gelsighthand/</link>
      <pubDate>Tue, 05 Nov 2019 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/publication/2019-gelsighthand/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Gelsight Gripper</title>
      <link>https://achuwilson.github.io/project/2019-gelsighthand/</link>
      <pubDate>Mon, 27 May 2019 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2019-gelsighthand/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;#results&#34;&gt;Results&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;#going-further&#34;&gt;Going Further&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;a href=&#34;#what-and-why&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;This  work  details  the  design  of  a  novel  two  fin-ger  robot  gripper  with  multiple  Gelsight  based  optical-tactilesensors  covering  the  inner  surface  of  the  hand.  The  multipleGelsight  sensors  can  gather  the  surface  topology  of  the  objectfrom  multiple  views  simultaneously  as  well  as  can  track  theshear  and  tensile  stress.  In  addition,  other  sensing  modalitiesenable  the  hand  to  gather  the  thermal,  acoustic  and  vibrationinformation from the object being grasped. The force controlledgripper is fully actuated so that it can be used for various graspconfigurations  and  can  also  be  used  for  in-hand  manipulationtasks.  Here  we  present  the  design  of  such  a  gripper.&lt;/p&gt;



&lt;h2 id=&#34;results&#34;&gt;Results&lt;a href=&#34;#results&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;



&lt;h4 id=&#34;workshop-paperpdf2019-gelsighthand-irospdf&#34;&gt;
&lt;a href=&#34;https://achuwilson.github.io/pdf/2019-gelsighthand-iros.pdf&#34;&gt;Workshop Paper&lt;/a&gt;&lt;a href=&#34;#workshop-paperpdf2019-gelsighthand-irospdf&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h4&gt;



&lt;h4 id=&#34;videohttpsyoutube4hxsz9nhjwi&#34;&gt;
&lt;a href=&#34;https://youtu.be/4hxsZ9nHJWI&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Video&lt;/a&gt;&lt;a href=&#34;#videohttpsyoutube4hxsz9nhjwi&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h4&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/4hxsZ9nHJWI&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;




&lt;h2 id=&#34;going-further&#34;&gt;Going Further&lt;a href=&#34;#going-further&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;In hand manipulation tasks, object localisation&lt;/p&gt;
&lt;/DIV&gt;
</description>
    </item>
    
    <item>
      <title>GelSight Simulation for Sim2Real Learning</title>
      <link>https://achuwilson.github.io/publication/2019-getsight-sim2real/</link>
      <pubDate>Sat, 25 May 2019 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/publication/2019-getsight-sim2real/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Gelsight for Sim2Real</title>
      <link>https://achuwilson.github.io/project/2019-gelsight_sim2real/</link>
      <pubDate>Sat, 27 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2019-gelsight_sim2real/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#previous-works&#34;&gt;Previous works&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#what-makes-it-special&#34;&gt;What makes it special&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#result&#34;&gt;Result&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;a href=&#34;#what-and-why&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;This work explores simulating Gelsight so that agents can be trained in simulation and transferred to real hardware systems with minimal/no difficulties. This would require high fidelity simulation of the GelSight sensor.&lt;/p&gt;



&lt;h2 id=&#34;previous-works&#34;&gt;Previous Works&lt;a href=&#34;#previous-works&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;My initial approach
&lt;a href=&#34;#&#34;&gt;[1]&lt;/a&gt; in simulating Gelsight used raytracing techniques to recreate the 3d pointcloud of the contact surface geometry. In reality, a Poisson solver based surface reconstruction is used to reconstruct the depth image and normals from the gradient image.&lt;/p&gt;



&lt;h2 id=&#34;what-makes-it-special&#34;&gt;What makes it special&lt;a href=&#34;#what-makes-it-special&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;This approach simulates the raw gradient RGB image coming from the sensor.&lt;/p&gt;



&lt;h2 id=&#34;result&#34;&gt;Result&lt;a href=&#34;#result&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The following image shows a comparison of the simulated image and the image acquired from the real sensor on similar objects.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;sim2real.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The top row of images are acquired from the hardware and the bottom one is the simulated one.&lt;/p&gt;
&lt;p&gt;Here is a 
&lt;a href=&#34;https://achuwilson.github.io/pdf/2019-sim2real-icra.pdf&#34;&gt;paper&lt;/a&gt; detailing the approach, and presented at ICRA 2019 ViTac Workshop.&lt;/p&gt;
&lt;/DIV&gt;
</description>
    </item>
    
    <item>
      <title>PulleyDrive</title>
      <link>https://achuwilson.github.io/project/2019-pulleydrive/</link>
      <pubDate>Wed, 27 Mar 2019 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2019-pulleydrive/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#previous-works&#34;&gt;Previous works&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#what-makes-it-special&#34;&gt;What makes it special&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#status&#34;&gt;Status&lt;/a&gt;
5&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;a href=&#34;#what-and-why&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;PulleyDrive is a side project exploring speed reduction/torque amplification using the principle of pulleys. This project was supported by the &amp;ldquo;ProjXpo&amp;rdquo; program under MIT Innovation Initiative.&lt;/p&gt;



&lt;h2 id=&#34;previous-works&#34;&gt;Previous Works&lt;a href=&#34;#previous-works&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;This work was mainly inspired by the cable-pulley based actuation of the Ambidex robot 
&lt;a href=&#34;https://ieeexplore.ieee.org/document/8016639&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;&lt;/p&gt;



&lt;h2 id=&#34;what-makes-it-special&#34;&gt;What makes it special&lt;a href=&#34;#what-makes-it-special&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Zero Backlash&lt;/li&gt;
&lt;li&gt;LightWeight&lt;/li&gt;
&lt;li&gt;High Stiffness&lt;/li&gt;
&lt;li&gt;Not Expensive&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 id=&#34;status&#34;&gt;Status&lt;a href=&#34;#status&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The following video shows a prototype in operation.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/e1tY6uB41l4&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;/br&gt;
&lt;/br&gt;
&lt;/br&gt;
&lt;/DIV&gt;
</description>
    </item>
    
    <item>
      <title>Projects</title>
      <link>https://achuwilson.github.io/projects/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/projects/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Resume</title>
      <link>https://achuwilson.github.io/about/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/about/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Resume</title>
      <link>https://achuwilson.github.io/resume/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/resume/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Talks &amp; Workshops</title>
      <link>https://achuwilson.github.io/talks/</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/talks/</guid>
      <description></description>
    </item>
    
    <item>
      <title>LICENSE: CC-BY-SA</title>
      <link>https://achuwilson.github.io/license/</link>
      <pubDate>Thu, 28 Jun 2018 00:00:00 +0100</pubDate>
      <guid>https://achuwilson.github.io/license/</guid>
      <description>&lt;p&gt;My 
&lt;a href=&#34;https://achuwilson.github.io/post/&#34;&gt;blog posts&lt;/a&gt; are released under a 
&lt;a href=&#34;http://creativecommons.org/licenses/by-sa/4.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Creative Commons Attribution-ShareAlike 4.0 International License&lt;/a&gt;.&lt;/p&gt;
&lt;center&gt;
&lt;i class=&#34;fab fa-creative-commons fa-2x&#34;&gt;&lt;/i&gt;&lt;i class=&#34;fab fa-creative-commons-by fa-2x&#34;&gt;&lt;/i&gt;&lt;i class=&#34;fab fa-creative-commons-sa fa-2x&#34;&gt;&lt;/i&gt;
&lt;/center&gt;
</description>
    </item>
    
    <item>
      <title>Modular, Whole Finger Tactile Sensing Gripper</title>
      <link>https://achuwilson.github.io/publication/2018-finger-icra/</link>
      <pubDate>Fri, 25 May 2018 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/publication/2018-finger-icra/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Round Tactile Sensing Finger</title>
      <link>https://achuwilson.github.io/project/2018-roundfinger/</link>
      <pubDate>Tue, 27 Feb 2018 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2018-roundfinger/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#previous-works&#34;&gt;Previous works&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#what-makes-it-special&#34;&gt;What makes it special&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#status&#34;&gt;Status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#going-further&#34;&gt;Going Further&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#subsequent-works&#34;&gt;Subsequent Works&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;a href=&#34;#what-and-why&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Robot fingers are almost always flat and rectangular. This is owing to their simplicity in design, control and better contact surface area, thereby giving a better grip.&lt;/p&gt;
&lt;p&gt;However, human fingers are round and more streamlined. It helps hand to navigate/explore in cluttered environments without much difficulty. In addition, the tactile sensing capability all around the human finger surface helps in making better sense of the environment contacts.&lt;/p&gt;



&lt;h2 id=&#34;previous-works&#34;&gt;Previous Works&lt;a href=&#34;#previous-works&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;#to be update later&lt;/em&gt;&lt;/p&gt;



&lt;h2 id=&#34;what-makes-it-special&#34;&gt;What makes it special&lt;a href=&#34;#what-makes-it-special&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Round the surface tactile sensing&lt;/li&gt;
&lt;li&gt;Simple construction&lt;/li&gt;
&lt;li&gt;Cheap and affordable&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The cross sectional view of the finger is as shown below
&lt;img src=&#34;modfinger1.jpg&#34; alt=&#34;Tactile Round Finger, Sectional View&#34;&gt;&lt;/p&gt;
&lt;p&gt;It consists of a cylindrical glass tube coated with a layer of transparent silicone elastomer. The deformable elastomer has colored markers on its outer surface, which is then tracked by a wide angle camera placed at one end of the glass tube. The camera is able to get a 360 degree image of the surface of the finger, which is then de-warped to rectangular images for better understanding and processing. The other end of the tube has a spherical mirror. The reflected image of the markers from the spherical mirror provides view from another angle and is intended to be used in future to reconstruct the three dimensional position of the markers using stereo reconstruction.&lt;/p&gt;
&lt;p&gt;Using off the shelf components ensured that the cost of the modular tactile finger is kept low. A 75 x 12 mm laboratory test tube is used as the cylindrical tube. The elastomer has Shore A hardness of 15 and is made using high transparent platinum cure silicone, which is molded into a thin sheet and wrapped over the test tube. The parabolic mirror is made using a chrome coated metal ball bearing. The finger is compatible with commonly available webcams and better results were obtained using a wide angle Raspberry Pi Camera.&lt;/p&gt;





&lt;img src=&#34;https://achuwilson.github.io/project/2018-roundfinger/modfinger2_hu8a6b4578a9891b16984e60644ebb7f80_1251968_350x350_fit_q90_lanczos.jpg&#34; width=&#34;281&#34; height=&#34;350&#34; alt=&#34;Modular Finger&#34;&gt;

&lt;p&gt;A two finger parallel gripper configuration can be easily achieved using such a finger





&lt;img src=&#34;https://achuwilson.github.io/project/2018-roundfinger/modfinger4_hud9647231be864811fc59faa85c73d3b7_13259_350x350_fit_lanczos_2.png&#34; width=&#34;208&#34; height=&#34;350&#34; alt=&#34;Modular Finger&#34;&gt;
&lt;/p&gt;



&lt;h2 id=&#34;status&#34;&gt;Status&lt;a href=&#34;#status&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The following image shows the raw output from the camera





&lt;img src=&#34;https://achuwilson.github.io/project/2018-roundfinger/raw_hu2370913120953bb7a5b23bac81215b84_113488_350x350_fit_q90_lanczos.jpg&#34; width=&#34;350&#34; height=&#34;263&#34; alt=&#34;Alternate Text&#34;&gt;

and then it is pre-processed and the dots are tracked for its position





&lt;img src=&#34;https://achuwilson.github.io/project/2018-roundfinger/im_tracking_hu6d075425c3e1aa968eaa100ca478f374_45867_350x350_fit_q90_lanczos.jpg&#34; width=&#34;350&#34; height=&#34;339&#34; alt=&#34;Alternate Text&#34;&gt;
&lt;/p&gt;



&lt;h2 id=&#34;going-further&#34;&gt;Going Further&lt;a href=&#34;#going-further&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The next steps would be attaching it to a gripper and collecting real world data.&lt;/p&gt;



&lt;h2 id=&#34;subsequent-works&#34;&gt;Subsequent Works&lt;a href=&#34;#subsequent-works&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;These are some subsequent works by others and could be references for future works.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;https://arxiv.org/abs/2008.05404&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;GelTip: A Finger-shaped Optical Tactile Sensor for Robotic Manipulation&lt;/a&gt; by Daniel Gomes et al.
&lt;a href=&#34;https://danfergo.github.io/geltip/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[website]&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://arxiv.org/abs/2004.00685&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Sensorized Multicurved Robot Finger with Data-driven Touch Sensing via Overlapping Light Signals&lt;/a&gt; by Pedro Placenza et al. 
&lt;a href=&#34;https://youtu.be/PVw8Qy7BHU0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[video]&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/DIV&gt;
</description>
    </item>
    
    <item>
      <title>Gelsight Simulation 1</title>
      <link>https://achuwilson.github.io/project/2017-gelsight_sim/</link>
      <pubDate>Mon, 27 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2017-gelsight_sim/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#how&#34;&gt;How&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#result&#34;&gt;Result&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;a href=&#34;#what-and-why&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;This is my initial approach in modelling the behaviour of GelSight tactile sensors in simulation. I resorted to simulation as I did not have access to the real sensor or resources to build one.&lt;/p&gt;



&lt;h2 id=&#34;how&#34;&gt;How&lt;a href=&#34;#how&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;I used the physics simulator Bullet. It had softbody simulation, which I thought would be useful for simulating the elastomer of GelSight. But it turned out that the softbody simulation was a basic and needed much more development. So, I indirectly modelled Gelsight using the raytest functionality in bullet. It returns the depth at which a ray makes contact with a solid body.&lt;/p&gt;



&lt;h2 id=&#34;result&#34;&gt;Result&lt;a href=&#34;#result&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Following is the video of the simulated gelsight gripper. The Kuka iiwa has a WSG 50 gripper fitted with a gelsight sensor. The sensor has a sensing area of 24x24mm and has a resolution of 256x256 pixels in the planar sensing area. It outputs standard ROS 3D pointcloud data, which is displayed in RViz.The simulation was run in my laptop and the pointcloud could be generated at 5Hz

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/IO02smLcDQE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Even though I could simulate the sensor, I soon ran into comptational limits of my laptop. &lt;/DIV&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Publishing my first Paper</title>
      <link>https://achuwilson.github.io/post/2017-06-15-icra17/</link>
      <pubDate>Thu, 15 Jun 2017 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/post/2017-06-15-icra17/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;p&gt;About an year ago, I got lucky enough to attend ICRA 2016 at Stockholm for the HRATC robot competition. I was so much inspired after seeing all the research being published there. I decided pursue a research project, and try to publish a paper in the subsequent conference. The fact that ICRA 2017 is organized in Singapore was also another major factor. It is close to India and I need not spend a fortune on the travel and accommodation expenses.&lt;/p&gt;



&lt;h2 id=&#34;why-i-did-it&#34;&gt;Why I did it?&lt;a href=&#34;#why-i-did-it&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;I always wanted to become a scientist and make my little contribution to science/robotics.&lt;/li&gt;
&lt;li&gt;I wanted to get into a Ph.D program and I had no research papers.&lt;/li&gt;
&lt;li&gt;To experience how it was like doing research and writing a technical paper.&lt;/li&gt;
&lt;li&gt;Attend conferences like ICRA and IROS again, meet smart people and their work.&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 id=&#34;my-background-and-constraints&#34;&gt;My Background and Constraints&lt;a href=&#34;#my-background-and-constraints&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Ever since I was rejected from a couple of Ph.D programs from 2013-2015, due to lack of research experience, I have been making plans and trying to do research on my own and write a couple of papers. But nothing moved past making elaborate plans and maybe a little literature study for all the research ideas I had. The stresses of working in a startup consume so much of your mental strength as well as time. Thus I failed in materializing all my research Ideas. My research ideas are interdisciplinary and spans over computer vision for robotics, novel actuators for robots, robotic manipulators and motion planning to name a few.&lt;/p&gt;
&lt;p&gt;My constraints are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Demanding Job&lt;/strong&gt;:
Working in a startup is very much different that a normal 9-5 job. You have got multiple responsibilities and more work. On some days I may have to pull all-nighters to meet the deadlines&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;No research experience&lt;/strong&gt; :
Even though I always wanted to be a scientist, I never had any formal training in research. I am mostly a self taught engineer and all my prior works were on building and hacking some stuff.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;No mentor&lt;/strong&gt; :
Wish I had some professors to guide me. I was alone and my guide was the books and the Internet.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Limited Time&lt;/strong&gt; :
Since one of the goals of undertaking a research project is going to the next ICRA, I had to submit the paper before 15th September 2016. So, it is barely three and half months.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 id=&#34;picking-up-a-research-topic&#34;&gt;Picking up a research topic&lt;a href=&#34;#picking-up-a-research-topic&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The first  challenge was picking up the topic or project to work on. I was clueless and was interested in everything. I started reading through the notes and photos I took at ICRA16. Finally I ended up in the best way possible. The proceedings of the conference were given on a USB drive. It was the first time I had access to this much research papers. It was awesome to go through all of the papers, even though I couldn&amp;rsquo;t understand many of the purely theoretical ones.&lt;/p&gt;
&lt;p&gt;Now I had to read through all of them, make notes and think how to build upon that.  Finding time to read all these was the first problem I faced. I cannot sit and read the entire day. I started using Mendeley app on PC and my smartphone to organize the papers and make notes. Having all the papers in my smartphone helped to make use of every bit of time otherwise spent useless. I read during my daily commute to and from work, during meals, when I am travelling outside etc&lt;/p&gt;
&lt;p&gt;It was very difficult to indulge in studies and research during this time. I was travelling for client demos of the startup and staying at different places- with friends, relatives, in hotels etc. Moreover, I would be often exhausted after the day&amp;rsquo;s toil, that even the possibility of staying up late reading was not imaginable. Still, I managed to read quite a bit by making best usage of time and not wasting any. Often I would end up sleeping reading some papers on my smartphone or in front of the open laptop.&lt;/p&gt;
&lt;p&gt;Even though finding time to read was difficult, there was enough and more time to think. I could think while waiting in traffic blocks, while driving, during meals, and even during boring and useless meetings. My thoughts wandered around various problems associated with direct drive mechanisms and technologies, how animal muscles work and limbs move, tactile and vision feedback, power consumption for these drives etc to name a few.&lt;/p&gt;
&lt;p&gt;I had to travel to Chennai for supporting a custome/pdf/2017-mr-clutch-icra.pdfr of my company. I stayed with my relatives in Chennai and the daily commute to the client was about 70 kms. I was traveling in the crowded suburban trains of Chennai and the hot climate made it worse. I had to start my journey at about 8 AM and would be back only by 11PM. I would be very exhausted  by the time I reached back to my room. So reading at night was beyond consideration. But I could make use of the commute time. In total, it took around 7 hours an average per day for my commute. I spend this time reading through the papers that came in the ICRA USB drive. There were over a thousand papers in my reading list. I skimmed through most of them, stopping and reading only those portions I could understand or was interested in. I came across the paper titled &amp;quot; Design and Development of a Hybrid Magneto-Rheological Clutch for Safe Robotic Applications&amp;quot; by Masoud Moghani and Mehrdad R. Kermani. It captured my attention and I began to study more about Magneto-Rheological fluids and their application in human safe robotics. I found out a couple more papers by the same authors on the fundamentals of MR devices.&lt;/p&gt;



&lt;h2 id=&#34;initial-explorations-failures&#34;&gt;Initial explorations, failures&lt;a href=&#34;#initial-explorations-failures&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;



&lt;h2 id=&#34;more-experimentation-finalizing-the-idea&#34;&gt;More experimentation, finalizing the idea&lt;a href=&#34;#more-experimentation-finalizing-the-idea&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Once I was back at home, I began to delve deep into MR Fluids and their characterization. More reading&amp;hellip; More searching.. I came up with an idea to make use of the properties of MR Fluid to make a Linear clutch -  a clutch which controls force transmitted from input to output through MR fluid.&lt;/p&gt;
&lt;p&gt;MR fluids are expensive and the lead-time will be a couple of weeks. Since I cannot afford both, I resorted to making my own MR fluid. I filed myself a piece of iron to the fine powdered form and used vegetable oil as the medium to make my own MR fluid for the prototype.
Here is a small video with the MR fluid&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/9ZZQn46WvLg&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;Before starting on the construction, I verified that my concept works in simulation. The following figures show the CAD model, rendered image and the FEM analysis&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;cad.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;FEM_clutch.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;After that I started working on building the MR clutch. Following photos shows some random shots during the build process.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;clutch-build.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Now I have to procure a couple of items which includes linear shaft bearings, aluminum rods etc.
Once all the parts are procured, the clutch was assembled and the cavity inside was filled with MR Fluid. Then it had to be tested and characteristics were studied&lt;/p&gt;
&lt;div align=&#34;center&#34;&gt;
&lt;iframe width = &#34;640&#34; height =&#34;380&#34; src=clutch_photo.jpg &gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;In order to demonstrate the usefulness and study more about the MR linear clutch, a single degree of freedom robot joint was designed and fabricated. It used the standard 30x30 Aluminum profiles. The arm was driven by a Firgelli linear actuator.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;arm.jpg&#34; alt=&#34;&#34;&gt;/pdf/2017-mr-clutch-icra.pdf&lt;/p&gt;
&lt;p&gt;More experiments are done with the arm to validate the functionality of MR Linear clutch.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;MR_clutch_result_plot.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/ES0q4eb1TYo&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;




&lt;h2 id=&#34;documentation&#34;&gt;Documentation&lt;a href=&#34;#documentation&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Experiments are completed and data collected. Now I have to write the paper. Since I didn&amp;rsquo;t have any previous experience writing an academic paper, it took some time.  I read a couple of articles online and referred IEEE guides and instructions. Instead of starting with the abstract and introduction, I started directly with the content- the things I did and the results obtained. Once it was done, I had to refer more literature to write the introductory pages. Luckily, due to the local festival of Onam, I got holidays for a couple of days and could work on the paper full time. Even then, I could felt the time running out fast. I had to plot the graphs, make diagrams and pictures and it took a lot more time than expected. I worked on the paper for about 4days clocking about 20 hours of work daily. Just a couple of hours before the deadline, I completed the final proofreading and submitted the paper. I was so exhausted after this that I literally slept an entire day after this.&lt;/p&gt;
&lt;p&gt;The final version of the paper is available 
&lt;a href=&#34;https://achuwilson.github.io/pdf/2017-mr-clutch-icra.pdf&#34;&gt;here&lt;/a&gt;&lt;/p&gt;



&lt;h2 id=&#34;acceptance-and-presentation-at-icra-2017&#34;&gt;Acceptance and Presentation at ICRA 2017&lt;a href=&#34;#acceptance-and-presentation-at-icra-2017&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;No words could describe the joy and happiness I had when I got the decision in January 2017. My very first paper was accepted. I began preparing for attending the conference.&lt;/p&gt;
&lt;p&gt;Later, I travelled to Singapore to attend ICRA 2017, where I presented my paper.
&lt;img src=&#34;presentation.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;In addition, I got to meet numerous researchers and even get an a copy of Handbook of Robotics signed by Dr. Oussama Khatib
&lt;img src=&#34;icra17.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;



&lt;h2 id=&#34;key-takeaways&#34;&gt;Key Takeaways&lt;a href=&#34;#key-takeaways&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;It was an awesome experience. I pushed myself to the limits, learned a lot of things.&lt;/p&gt;
&lt;p&gt;When I started working on the research project, the only thing that mattered to me was doing such an awesome work so that it is accepted at a top conference like ICRA. But as I finished the paper,  I realized that it doesn&amp;rsquo;t matter much whether the paper is accepted or rejected.  The only thing that matters is how much impactful is your work and how much satisfied you are.&lt;/p&gt;
&lt;p&gt;Along the course of working on the paper, I learned much beyond the robotics stuff. The main ones being&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Time Management&lt;/strong&gt; :&lt;/p&gt;
&lt;p&gt;In most cases, scientific research and publication of results take an enormous amount of time and resources. It may takes months or even years of research to make an impactful invention and maybe even more months to author a paper on it.   But in my case, the time was fixed as the paper was to be submitted before the deadline, which is just a couple of months away. On top of that, I am working a full-time job  at a startup, which made finding the time extremely difficult. I had to find extra time after my work and make the best use of time wasted. I often slept less, only about 4-5 hours a day.  After my daytime job, I started fiddling with the research projects in the evening and continued till midnight or occassionally to early mornings.&lt;/p&gt;
&lt;p&gt;Having a smartphone always with me was the greatest tool that I used to make best use of time. It was loaded with plenty of books and papers. I had Mendeley on my phone to keep track of all the papers that I am reading. I could read on the phone irrespective of where I am -  while traveling in a crowded bus, when having meals, when having a haircut at the salon, when stuck in a traffic jam, while waiting for someone, when in a queue or even when I am in the toilet seat. These are the bits of time wasted by not being productive. Somehow I managed to squeeze out every bit of time wasted and use it for productive things.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Narrow down and Focus&lt;/strong&gt; :&lt;br&gt;
My initial ideas was to design a clone of MIT Cheetah using direct drive actuators which is a very broad topic. Studies must be done on the actuators, optimum kinematic structure, gait generation, stability and much more. Each of these requires considerable research effort in itself and has only been possible by the collective effort of many researchers. So concentrate on something that may seem simple and small, go deep and push forward.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Don&amp;rsquo;t re-invent the wheel&lt;/strong&gt; :
This was a lesson that I learned the hard way. Better not to start from scratch, unless your idea is  revolutionary and  novel. Study about and make use of the progress brought about by past researchers.  If some part of your research depends on something that has been previously done, make use of it and build upon it or reuse it. Your time is limited (not just the conference submission deadline), so why to spend time reinventing the wheel, when you can invest it to invent time travel.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;You cannot plan a research project explicitly&lt;/strong&gt; :
Coming from a background doing engineering projects, I used to plan all the minute details and had a roadmap for execution. I knew beforehand the progress I would achieve at particular instances of time. It was possible because the path was clear and the results was predictable. Engineering is the often trodden path and research is the one less traveled. &lt;em&gt;&lt;strong&gt;You may just know the direction but you have to make the path.&lt;/strong&gt;&lt;/em&gt; You may be struck with an idea when you least expect it. The results of the research may often come unexpectedly and maybe even in dreams like the discovery structure of Benzene by Kekule.
So in short &lt;em&gt;&lt;strong&gt;Engineering = predictable&lt;/strong&gt;&lt;/em&gt; ,while &lt;em&gt;&lt;strong&gt;Research = unpredictable.&lt;/strong&gt;&lt;/em&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/DIV&gt;</description>
    </item>
    
    <item>
      <title>Design and Development of a Magneto-Rheological Linear Clutch for Force controlled Human Safe Robots</title>
      <link>https://achuwilson.github.io/publication/2017-magnetorheological-clutch/</link>
      <pubDate>Thu, 01 Jun 2017 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/publication/2017-magnetorheological-clutch/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Quadruped with Active spine</title>
      <link>https://achuwilson.github.io/project/2017-activespine_quadruped/</link>
      <pubDate>Sat, 27 May 2017 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2017-activespine_quadruped/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#previous-works&#34;&gt;Previous Works&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#results&#34;&gt;Results&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#status&#34;&gt;Status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;a href=&#34;#what-and-why&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;This is a side project to explore whether quadrupeds can be made to land on all four legs safely after they are dropped/thrown in random orientations.&lt;/p&gt;



&lt;h2 id=&#34;previous-works&#34;&gt;Previous Works&lt;a href=&#34;#previous-works&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://en.wikipedia.org/wiki/Cat_righting_reflex&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cat righting reflex&lt;/a&gt; been extensively studied before. In 1942 US Air Force played with kittens in microgravity. The same lab, in 1962, published a report titled 
&lt;a href=&#34;http://www.dtic.mil/dtic/tr/fulltext/u2/400354.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Weightless Man: Self-Rotation Techniques&lt;/a&gt;, as a guidline for future astronauts on how to move around in zero gravity. 1960s. The falling cat was also studiedextensively by NASA to prepare the astronauts for Zero-G environments. A 1969 paper titled 
&lt;a href=&#34;http://www.sciencedirect.com/science/article/pii/0020768369900869&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Dynamical Explanation of the Falling Cat Phenomenon&lt;/a&gt; explains this.&lt;/p&gt;
&lt;p&gt;Past research attributes the cat reflex to law of conservation of momentem. Flexible bodies can generate internal forces and moments, by the motion of limbs. But the rest of the body rotates in opposite direction owing to the conservation of momentum, so that net angular momentum remains zero. However the rate of rotation can be controlled by extending or pulling back the limbs. Wikipedia summarises it to three key steps:&lt;/p&gt;
&lt;p&gt;1.Bend in the middle so that the front half of their body rotates about a different axis from the rear half.
2.Tuck their front legs in to reduce the moment of inertia of the front half of their body and extend their rear legs to increase the moment of inertia of the rear half of their body so that they can rotate their front further (as much as 90°) while the rear half rotates in the opposite direction less (as little as 10°).
3.Extend their front legs and tuck their rear legs so that they can rotate their rear half further while their front half rotates in the opposite direction less.&lt;/p&gt;
&lt;p&gt;Depending on the cat’s flexibility and initial angular momentum, if any, the cat may need to perform steps two and three repeatedly in order to complete a full 180° rotation.&lt;/p&gt;
&lt;p&gt;Past works on using active spine in legged robots specialized rotation in the axis perpendicular to the saggital plane and was aimed at improving the gait. My research focusess on rotation in the axis perpendicular to the transverse plane. I developed an inertial reoreintation controller. It controls the extension and tucking in of the quadruped limbs, so that the cranial and caudal part rotates with different velocities owing to the conserved momentum. The controller thus uses the inertia of the quadruped to reorient itself.&lt;/p&gt;



&lt;h2 id=&#34;results&#34;&gt;Results&lt;a href=&#34;#results&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;A quadruped was modelled in the Bullet simulator and a simple state machine based controller was implemented. It works beautifully as in the following video&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/4Hg29l2iCJo&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;




&lt;h2 id=&#34;status&#34;&gt;Status&lt;a href=&#34;#status&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;As the system is verified in simulation, now it has to be implemented in hardware.
I am working on a small quadruped during freetime and it will be updated later.&lt;/p&gt;
&lt;p&gt;Here are some photos of it in progress.&lt;/p&gt;
&lt;p&gt;




&lt;img src=&#34;https://achuwilson.github.io/project/2017-activespine_quadruped/quad1_hub3d4f15a59a31c668d66105d0dde93e6_333603_450x450_fit_q90_lanczos.jpg&#34; width=&#34;450&#34; height=&#34;338&#34; alt=&#34;quadruped image 1&#34;&gt;






&lt;img src=&#34;https://achuwilson.github.io/project/2017-activespine_quadruped/quad2_hu86232e7e403934dbd3ee3c0182a4e2fe_299174_450x450_fit_q90_lanczos.jpg&#34; width=&#34;450&#34; height=&#34;338&#34; alt=&#34;quadruped image 2&#34;&gt;
&lt;/p&gt;



&lt;h2 id=&#34;references&#34;&gt;References&lt;a href=&#34;#references&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;will be updated later as time permits.&lt;/p&gt;
&lt;/DIV&gt;
</description>
    </item>
    
    <item>
      <title>MR Clutch</title>
      <link>https://achuwilson.github.io/project/2017-mrclutch/</link>
      <pubDate>Thu, 27 Apr 2017 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2017-mrclutch/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;#result&#34;&gt;Result&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;a href=&#34;#what-and-why&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;This work proposes a Magneto Rheological linear clutch  for  use  in  human  safe  robotic  applications.  The  force  transmitted to the links of the robot must be precisely controlled for  any  manipulator  if  it  has  to  be  operated  safely  alongside  humans.  The  traditional  approaches  to  this  problem  is  using  various    compliant    actuating    schemes    like    Series    Elastic    Actuators,  Joint  Torque  Control  etc.  Research  on  the  usage  of  smart  materials  that  change  their  properties  on  application  of  electrical or magnetic fields for human safe robots have gained momentum  recently.  Studies  on  the  feasibility  of  Magneto-Rheological   actuators   has   been   done   already.   This   paper   introduces a MR clutch which can control the force transmitted by a linear actuator. The electromechanical model of the linear clutch has been developed, implemented in hardware, and tested using a prototype one Degree of Freedom arm. The design of the clutch is detailed and the performance is characterized thorough a series of experiments. The results suggest that the linear clutch serves well for the precise force control of a linear actuator.&lt;/p&gt;



&lt;h2 id=&#34;result&#34;&gt;Result&lt;a href=&#34;#result&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;



&lt;h4 id=&#34;conference-paperpdf2017-mr-clutch-icrapdf&#34;&gt;
&lt;a href=&#34;https://achuwilson.github.io/pdf/2017-mr-clutch-icra.pdf&#34;&gt;Conference Paper&lt;/a&gt;&lt;a href=&#34;#conference-paperpdf2017-mr-clutch-icrapdf&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h4&gt;



&lt;h4 id=&#34;videohttpsyoutubees0q4eb1tyo&#34;&gt;
&lt;a href=&#34;https://youtu.be/ES0q4eb1TYo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Video&lt;/a&gt;&lt;a href=&#34;#videohttpsyoutubees0q4eb1tyo&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h4&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/ES0q4eb1TYo&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;/DIV&gt;
</description>
    </item>
    
    <item>
      <title>Robot arm for testing of touchscreen applications </title>
      <link>https://achuwilson.github.io/publication/2017-touchtest-patent/</link>
      <pubDate>Thu, 30 Mar 2017 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/publication/2017-touchtest-patent/</guid>
      <description>&lt;p&gt;Publication Number WO/2017/051263&lt;/p&gt;
&lt;p&gt;Publication Date 30.03.2017&lt;/p&gt;
&lt;p&gt;International Application No. PCT/IB2016/053292&lt;/p&gt;
&lt;p&gt;International Filing Date 04.06.2016&lt;/p&gt;
&lt;p&gt;IPC G06F 11/22 2006.01&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Attending ICRA and winning HRATC Challenge</title>
      <link>https://achuwilson.github.io/post/2016-06-12-icra16/</link>
      <pubDate>Sun, 12 Jun 2016 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/post/2016-06-12-icra16/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;p&gt;I heard about the International Conference on Robotics and Automation ( ICRA) for the first time around 2013, mainly from the videos posted by IEEE Spectrum and IEEE Video Friday. Soon,  I realized it is the big name robotics conference where academia as well as industry showcase their flashy demos. Since then, for the next couple of years, I would be always looking forward the ICRA time, when there would be a swarm of updates on the most recent research happening in robotics.&lt;/p&gt;



&lt;h2 id=&#34;a-way-to-attend-the-conference-comes-up&#34;&gt;A way to attend the conference comes up&lt;a href=&#34;#a-way-to-attend-the-conference-comes-up&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;I always wished to attend these conferences, but not having an academic/research background as well as expensive registration as a non academic and expenses related to travel and accommodation held me back. I realized that there are robot competitions associated with the conference. The Humanitarian Robotics and Automation Technology Challenge (HRATC) attracted me, primarily because it related to autonomous navigation and SLAM, which was something I was exploring at my startup. Secondarily the fact that the competition used ROS as the platform and no hardware was necessary. This relieved us from the hassle and expenses of building hardware.&lt;/p&gt;
&lt;p&gt;So, I teamed with two of my friends, Lentin Joseph &amp;amp; Chandykunju Alex. We named the team Autobots.
The challenge involved autonomously searching and mapping an area for buried landmines using a UGV equipped with a metal detector. Teams were rewarded for detecting a landmine, marking it on the map and on the area covered. Penalties were awarded for running over landmines and crashing into obstacles/trees. Initial elimination stages involved teams competing in the Gazebo simulator. In the intermediate stage, teams were provided real data from the hardware as ROSBAG files and the code would be tested on them. The finals were scheduled to be conducted at ICRA 2016, in Stockholm. Teams would be competing on real robot hardware in the finals. To make it more realistic like an inaccessible landmine filled zone, the competition arena including the robot, landmines, obstacles etc were set up in Portugal and  we were only given an initial remote access to upload our code from Stockholm. After that the robot has to operate autonomously. We cleared the initial rounds effortlessly and made into the finals. Hurray..!! So here comes my opportunity to attend ICRA. I was so excited.&lt;/p&gt;
&lt;p&gt;The excitement was huge- it was my first conference, it was my first international trip, and moreover, the excitement of meeting eminent robotics researchers, attending their talks and seeing the cutting edge research happening worldwide. It was indeed a dream came true.&lt;/p&gt;



&lt;h2 id=&#34;the-travel-an-an-unexpected-visit-to-dubai&#34;&gt;The travel, an an unexpected visit to Dubai&lt;a href=&#34;#the-travel-an-an-unexpected-visit-to-dubai&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;So, finally, its time to go to Stockholm. We borrowed money from friends and family, registered for the conference, got the visa, booked flight tickets, and decided to stay in an affordable hostel at a walkable distance from the conference venue.  Our flight was from Kochi to Stockholm with a layover in Dubai. Unfortunately, the flight from Kochi got delayed and we missed our connection flight from Dubai. The airlines shifted us to an airport hotel and re-booked the next days flight for us. Even though this seemed unfortunate, we got the chance to spend a day in Dubai and do sightseeing. We visited Burj Khalifa, Palm Jumeirah etc.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;dubai.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;



&lt;h2 id=&#34;arriving-in-stockholm&#34;&gt;Arriving in Stockholm&lt;a href=&#34;#arriving-in-stockholm&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The next day, we arrived in Stockholm. After clearing the immigration procedures, we heard our names being called out through the PA system of the airport.  We were requested to report to the airlines desk. Soon, we found out that the airlines forgot to pick our baggage from Dubai. They collected the address of our hostel and promised to deliver it next day( Which they did not until the last day of the conference). So, we are now in a foreign country, with just the dress we are in and the laptop in backpack.&lt;/p&gt;



&lt;h2 id=&#34;icra-expo&#34;&gt;ICRA EXPO&lt;a href=&#34;#icra-expo&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The exhibition stalls at the conference were one thing I was most looking forward to visit. I was excited to see the real robots up so close, the ones that I have seen only in YouTube videos. Moreover I got to have interesting conversations with the people exhibiting them.
I met many other startup founders and could even work out some potential future collaborations.
In addition, we got enough free T-shirts which came handy due to our delayed baggage.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;robots.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;



&lt;h2 id=&#34;meeting-my-heros&#34;&gt;Meeting my heros&lt;a href=&#34;#meeting-my-heros&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Another aspect was meeting those pioneers in robotics that you have only read about. These included Dr.Wolfram Burgard, Dr.Vijay Kumar from UPenn, Dr.Henrik Christensen,  Dr. Oussama Khatib, Dr. Magnus Egerstedt,  Dr.Melonee Wise, Dr Peter Corke, Dr. Giorgio Metta etc to name a few. I came to know them either as the professors in the MOOCs classes I took or from cool robot videos/research featured on IEEE Spectrum. Meeting them and talking with them (even though I did not had much to contribute) was a dream come true. They can be said of my role models whom I aspire to be one day. ALl of them were very friendly and approachable and we even took selfies with them.
&lt;img src=&#34;people.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;



&lt;h2 id=&#34;winning-the-challenge&#34;&gt;Winning the challenge&lt;a href=&#34;#winning-the-challenge&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The HRATC challenge was organized over two days.We had to upload our code to the remote robot and video of the robot in operation was live streamed&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;autobots.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Results from the performance on two days were averaged. On the first day, out team scored the lowest. Our navigation stack primarily used GPS coordinates for localization. It was cloudy at the remote robot site and we had trouble getting an accurate GPS fix often. This caused us so much issues in navigation.  After the first day, we pulled an all-nighter to re-write the navigation stack so that it doesn&amp;rsquo;t use GPS always and uses a probabilistic means of localization using the noisy and slippery wheel encoders and the unreliable GPS. We performed  much better the next day. But we were not much hopeful of making it to the top position, given our not that great performance on first day. The winners would only be announced on the last day of the conference. So we spend the rest of the time exploring the conference, attending talks, meeting people, etc.
&lt;img src=&#34;award.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;



&lt;h2 id=&#34;exposure-to-research&#34;&gt;Exposure to Research&lt;a href=&#34;#exposure-to-research&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;It was my first time seeing paper presentation sessions. There was three auditoriums where sessions were held in parallel. It was quite difficult to choose which session to attend, since many of them were interesting to me and to make things worse, they were scheduled in parallel.I was interested in everything, ranging from research on robot manipulation, flying robots, navigation, walking robots, novel hardware design and actuation, computer vision, and what not. I enjoyed almost all the sessions I attended. It was exciting to see the cutting edge research results. Moreover the basic concepts behind most of the research was simple and easy to understand. Even though I couldn&amp;rsquo;t make much sense of many highly mathematical and purely theoretical papers, the other papers presented could be grasped easily. Even though I couldn&amp;rsquo;t make sense of many such papers due to my limited knowledge, it was an awesome learning experience. The papers published at ICRA came from all realms of robotics. Coming from a background more inclined seeing the real-hardware robots, I could classify them into two- purely theoretical/mathematical papers and  papers on real robot hardware.&lt;/p&gt;
&lt;p&gt;Previously my thought was that the research papers/presentations were only understood by someone with a masters or P.hD, due to the complex and advanced topics they handle. I believe keeping myself updated by not missing any IEEE  spectrum video Fridays, reading voraciously every IEEE Robotics &amp;amp; Automation Magazine for the last couple of years helped me a lot in understanding them.&lt;/p&gt;
&lt;p&gt;I addition to getting a broad overview of all the research happening in robotics, I had a firsthand experience of how the academic research and publishing works. The sessions gave me a realization that many of the things that I did as hobby projects could be the basis for writing awesome research papers. It built in me a confidence that, I too can write and publish a research paper.&lt;/p&gt;
&lt;p&gt;The interactive sessions after the paper presentations were much useful than the presentations as such as such. I could meet the authors and ask them more about their paper&lt;/p&gt;



&lt;h2 id=&#34;exploring-stockholm&#34;&gt;Exploring Stockholm&lt;a href=&#34;#exploring-stockholm&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Since this is the first  international trip for the three of us, we had decided to make the best use of it. We stayed for a couple more days after the conference to explore Stockholm. Since it was May, the weather outside was awesome. The city was clean, safe and filled with greenery. We explored the city mostly on foot.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;stockholm.jpeg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;/DIV&gt;</description>
    </item>
    
    <item>
      <title>2048 Solver Robot</title>
      <link>https://achuwilson.github.io/project/2015-2048/</link>
      <pubDate>Tue, 27 Oct 2015 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2015-2048/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#How&#34;&gt;How&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#result&#34;&gt;Result&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;a href=&#34;#what-and-why&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;2048
&lt;a href=&#34;https://en.wikipedia.org/wiki/2048_%28video_game%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;  is a single-player sliding block puzzle game. The objective of the game is to slide numbered tiles on a grid to combine them to create a tile with the number 2048.&lt;/p&gt;
&lt;p&gt;This was a weekend hobby project, to make use of a robot arm, computer vision and AI techniques to solve and play the game.&lt;/p&gt;



&lt;h2 id=&#34;how&#34;&gt;How&lt;a href=&#34;#how&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The state of the game is inferred from the smartphone screen. Screenshots are perodically captured via the ADB interface. The image is pre-processed and an OCR is done to detect the digits on each tile.&lt;/p&gt;
&lt;p&gt;Given the current game state, an alpha-beta pruning algorithm predicts the next best action. The action space is swipe in Left/Right/Up/Bottom directions. The robot arm then would move towards the screen and make the swipe&lt;/p&gt;



&lt;h2 id=&#34;result&#34;&gt;Result&lt;a href=&#34;#result&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;See for yourself!.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/hUNTeZJSUWE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;It not only wins the game, but also scores unbelievable scores.&lt;/p&gt;
&lt;/DIV&gt;
</description>
    </item>
    
    <item>
      <title>Autonomous Drone Landing</title>
      <link>https://achuwilson.github.io/project/2015-quadrotor_landing/</link>
      <pubDate>Mon, 27 Apr 2015 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2015-quadrotor_landing/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#results&#34;&gt;Results&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;a href=&#34;#what-and-why&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;This was a small project done for the qualification round of MBZIRC Challenge 2015. The drone would lift off, move in a circular pattern with incrasing readius and searching for the marker using the downward facing camera. Once the marker is localized, a simple path planner is used to plan the trajectory to land the drone on the rover.&lt;/p&gt;



&lt;h2 id=&#34;results&#34;&gt;Results&lt;a href=&#34;#results&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Even though we didnt make it to the finals, it was a good learning experience.&lt;/p&gt;
&lt;p&gt;Here is a video of it.

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/QIklrzh8k6U&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/DIV&gt;
</description>
    </item>
    
    <item>
      <title>Chippu</title>
      <link>https://achuwilson.github.io/project/2010-chippu/</link>
      <pubDate>Thu, 27 Oct 2011 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2010-chippu/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;a href=&#34;#what-and-why&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Chippu was a small social robot made as a side project. It could understand speech commands, make conversations, recocnize faces and navigate on his own using SLAM and help me with my emails and calender, like an assistant. All the functionalities was implemented using the publically available packages in ROS. This was an attempt to learn and get familiar with ROS by building something.&lt;/p&gt;
&lt;p&gt;Here are some of the videos of chippu.&lt;/p&gt;
&lt;p&gt;Just moving around

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/KoJ7E7-5jwo&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Speech recognition was implemented using Julius and trained with my voice.

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/zO5MvISidJI&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/DIV&gt;
</description>
    </item>
    
    <item>
      <title>HAPS</title>
      <link>https://achuwilson.github.io/project/2011-haps/</link>
      <pubDate>Wed, 27 Apr 2011 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2011-haps/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;
&lt;a href=&#34;#results&#34;&gt;Results&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;a href=&#34;#what-and-why&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;This is my undergraduate college project.&lt;/p&gt;
&lt;p&gt;A high altitude platfrom based communication system, which can be rapidly deployed using a balloon is designed and implemented. It is similar to the Project Loon by Google, which was later announced.&lt;/p&gt;
&lt;p&gt;This page will be updated later as time permits and more details are 
&lt;a href=&#34;https://achuwilson.wordpress.com/2012/05/18/long-range-rf-link-using-nrf24l01-rf-transceiver/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;&lt;/p&gt;



&lt;h2 id=&#34;results&#34;&gt;Results&lt;a href=&#34;#results&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;We could not deploy it on a real weather balloon as we were constrained by resources. Instead, the developed system was fixed at the top of a high rise building and we were able to make long distance communication beyond line of sight, using the system as a transponder.&lt;/p&gt;
&lt;p&gt;In addition, we won the &lt;strong&gt;Best Project Award&lt;/strong&gt; from the college for the year of 2012.&lt;/p&gt;
&lt;/DIV&gt;
</description>
    </item>
    
    <item>
      <title>RollerBot</title>
      <link>https://achuwilson.github.io/project/2009-rollerbot/</link>
      <pubDate>Tue, 27 Apr 2010 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2009-rollerbot/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;p&gt;RollerBot is a two wheeled, cylindrical robot, which could be used for remote surveillance applications. It drew inspiration from the Recon Scout ThrowBot
&lt;a href=&#34;#&#34;&gt;[1]&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;rollerbot-fullsize.jpg&#34; alt=&#34;Roller Bot&#34;&gt;&lt;/p&gt;
&lt;p&gt;The main body is made from a PVC pipe which houses the batteries, motors, Camera+Mic, AV transmitter and the RF remote control receiver. The robot is controlled from a remote handheld controller over an RF link. The wheels are custom made and a small leg provides the reaction force for the wheels.&lt;/p&gt;
&lt;/DIV&gt;
</description>
    </item>
    
    <item>
      <title>Smart Notice Board</title>
      <link>https://achuwilson.github.io/project/2009-smart_notice_board/</link>
      <pubDate>Sat, 27 Mar 2010 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2009-smart_notice_board/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#how&#34;&gt;How&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#results&#34;&gt;Results&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;a href=&#34;#what-and-why&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;This is another hobby project. Any display solution with a VGA input can be used a the notice board. Users text the messages to be displayed, along with an access code to a predefined number. The system would them display the messgae on the display&lt;/p&gt;



&lt;h2 id=&#34;how&#34;&gt;How&lt;a href=&#34;#how&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The system is built around an AVR microcontroller and a GSM modem. The GSM modem has a SIM card provided by the netork provider and can receive text messages. The microcontroller controls the modem and reads the incomming messages through AT commands over a serial port. The user has to send the text message prepended with the security access code. This prevents unauthorized people from spamming the system,&lt;/p&gt;
&lt;p&gt;Once the text to be displayed is received and authenticated. The AVR microcontroller sends it over to the display as a VGA signal. Even though the tiny AVR controller does not have a VGA port, VGA signals are generated by bitbanging through the SPI port.&lt;/p&gt;



&lt;h2 id=&#34;results&#34;&gt;Results&lt;a href=&#34;#results&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Here is a video of the system in operation.

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/sg7snEejMzI&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/DIV&gt;
</description>
    </item>
    
    <item>
      <title>Self parking car</title>
      <link>https://achuwilson.github.io/project/2009-self-parking-car/</link>
      <pubDate>Fri, 27 Mar 2009 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2009-self-parking-car/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#how&#34;&gt;How&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#results&#34;&gt;Result&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;



&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;a href=&#34;#what-and-why&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;This is a small side project during my undergraduate days, to master embedded systems and robotics skills.&lt;/p&gt;



&lt;h2 id=&#34;how&#34;&gt;How&lt;a href=&#34;#how&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;The robot car consists of IR distance sensors on its sides and wheel encoders. These data streams are fed into a state machine, which looks for empty parking spots. If found any, the state machine would initialize the parking subroutine, and executes a parking maneuver.&lt;/p&gt;
&lt;p&gt;




&lt;img src=&#34;https://achuwilson.github.io/project/2009-self-parking-car/parkingcar_hu08f097a3ed191a1b7d6a8778b58680d4_157737_450x450_fit_q90_lanczos.jpg&#34; width=&#34;450&#34; height=&#34;338&#34; alt=&#34;parking robot car&#34;&gt;

The above image shows the setup used.&lt;/p&gt;



&lt;h2 id=&#34;result&#34;&gt;Result&lt;a href=&#34;#result&#34;&gt;&lt;svg class=&#34;anchor-symbol&#34; height=&#34;26&#34; width=&#34;26&#34; viewBox=&#34;0 0 22 22&#34; xmlns=&#34;http://www.w3.org/2000/svg&#34;&gt;
&lt;path d=&#34;M0 0h24v24H0z&#34; fill=&#34;currentColor&#34;&gt;&lt;/path&gt;
&lt;path d=&#34;M3.9 12c0-1.71 1.39-3.1 3.1-3.1h4V7H7c-2.76.0-5 2.24-5 5s2.24 5 5 5h4v-1.9H7c-1.71.0-3.1-1.39-3.1-3.1zM8 13h8v-2H8v2zm9-6h-4v1.9h4c1.71.0 3.1 1.39 3.1 3.1s-1.39 3.1-3.1 3.1h-4V17h4c2.76.0 5-2.24 5-5s-2.24-5-5-5z&#34;&gt;&lt;/path&gt;
&lt;/svg&gt;&lt;/a&gt;
&lt;/h2&gt;
&lt;p&gt;Some trials!

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/GDYHJPsIQ2M&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/DIV&gt;
</description>
    </item>
    
  </channel>
</rss>
