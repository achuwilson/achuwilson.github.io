<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Software Projects | Achu Wilson</title>
    <link>https://achuwilson.github.io/categories/software-projects/</link>
      <atom:link href="https://achuwilson.github.io/categories/software-projects/index.xml" rel="self" type="application/rss+xml" />
    <description>Software Projects</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>© Achu Wilson 2021</copyright><lastBuildDate>Sun, 15 Nov 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://achuwilson.github.io/img/pom-card.png</url>
      <title>Software Projects</title>
      <link>https://achuwilson.github.io/categories/software-projects/</link>
    </image>
    
    <item>
      <title>In-Hand Rolling</title>
      <link>https://achuwilson.github.io/project/2020-in-hand-rolling/</link>
      <pubDate>Sun, 15 Nov 2020 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2020-in-hand-rolling/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#previous-works&#34;&gt;Previous works&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#status&#34;&gt;Status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#going-further&#34;&gt;Going Further&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;/h2&gt;
&lt;p&gt;This is quite recent and an exploratory side project. Recently for another project, I trained a CNN to classify the objects being grasped by a hand which has GelSight based tactile sensors. It worked pretty good (~90% accuracy) on my test objects. Analysing the false classifications indicated that the tactile data may not be perfect during all the grasps. We humans also gets confused in the same way occasionally, if we grab objects with just two fingers. We would then either proceed to close the fingers to make more contact surface area with the object or roll the object between our fingers to classify it. This would then give us more data and increases our belief probability.&lt;/p&gt;
&lt;h2 id=&#34;previous-works&#34;&gt;Previous Works&lt;/h2&gt;
&lt;p&gt;Much research has been done in tactile object recognition as well as in hand manipulation
&lt;a href=&#34;https://core.ac.uk/download/pdf/77000058.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;, 
&lt;a href=&#34;https://ieeexplore.ieee.org/document/7363508&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[2]&lt;/a&gt;. Unlike those approaches. this work explores on learning a finger movement repertoire, that could maximize the in-hand object recognition/localization capabilities.&lt;/p&gt;
&lt;h2 id=&#34;status&#34;&gt;Status&lt;/h2&gt;
&lt;p&gt;The following video shows the prototype gripper classifying two test objects (geodesic spheres with hexagonal and triangular faces, that can be better felt by touch)&lt;/p&gt;
&lt;iframe src=&#34;https://drive.google.com/file/d/1bOfoukle06T8DrrSyyc8ingNUB5qe8He/preview&#34; width=&#34;640&#34; height=&#34;480&#34; align =&#34;center&#34; &gt;&lt;/iframe&gt;
&lt;p&gt;We can see that it falsely classifies objects once in a while.&lt;/p&gt;
&lt;p&gt;A modular 3rd axis is inserted in between the finger and the gripper, which can rotate the object in hand.&lt;/p&gt;
&lt;iframe src=&#34;https://drive.google.com/file/d/1L_x2ZVm-HVFxSDLjEirT3ZcQ40vBXKNi/preview&#34; width=&#34;640&#34; height=&#34;480&#34;&gt;&lt;/iframe&gt;
&lt;p&gt;The classification probabilities during this motion are averaged to get a more accurate estimate of the object.&lt;/p&gt;
&lt;p&gt;(Note: it has been tested only with symmetric objects, which are easy to roll and is still an ongoing project)&lt;/p&gt;
&lt;h2 id=&#34;going-further&#34;&gt;Going Further&lt;/h2&gt;
&lt;p&gt;More experimentation, 3D reconstruction using techniques using ICP.&lt;/p&gt;
&lt;/DIV&gt;
</description>
    </item>
    
    <item>
      <title>BlindGrasp</title>
      <link>https://achuwilson.github.io/project/2020-blindgrasp/</link>
      <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2020-blindgrasp/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#previous-works&#34;&gt;Previous works&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#how&#34;&gt;How&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#status&#34;&gt;Status&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;/h2&gt;
&lt;p&gt;BlindGrasp, as the name implies, aims at grasping objects when the robot&amp;rsquo;s eyes are blindfolded. This idea had been lingering in my mind, ever since I came across the GelSight optical tactile sensor. It is a tough and challenging problem and I have been making slow, yet good progress. This is the main project that I am spending my time now and has helped us to be the finalist in the &lt;em&gt;Kuka Innovation Award 2021&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Humans have excellent grasping capabilities in unstructured environments which are still unmatched by robots. This can be attributed to the dexterous human hand, robust visual and tactile sensing capabilities as well as to the intelligent brain which makes conscious and subconscious decisions.&lt;/p&gt;
&lt;p&gt;Visual sensing is traditionally used extensively in robotics when compared to tactile sensing. Tactile sensing is harder, mainly due to the difficulty in interpreting and making sense of the complex tactile signals. The recent developments in machine learning - particularly Deep Reinforcement Learning has opened new possibilities in the making usage of such complex data.&lt;/p&gt;
&lt;p&gt;We humans can effortlessly put a hand inside a bag, search by moving it around and pick up an item or we can use our hands to dig or sieve through granular medium like sand and pick up an object. This project explores building a system which learns manipulation and grasping skills in such vision denied environments.&lt;/p&gt;
&lt;h2 id=&#34;previous-works&#34;&gt;Previous Works&lt;/h2&gt;
&lt;p&gt;Grasping objects, using tactile sensing alone as well as in granular media are explored more recently. Following are some of the early works.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;http://www.robotics.stanford.edu/~ang/papers/icra09-ProximityGrasping.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reactive Grasping Using Optical Proximity Sensors&lt;/a&gt; by Kaijen Hsaio et al.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://choice.umn.edu/deep-learning-approach-grasping-invisible&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Deep Learning Approach to Grasping the Invisible&lt;/a&gt;, by Yang Yang et al.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;https://ieeexplore.ieee.org/abstract/document/9158928&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vibro-Tactile Foreign Body Detection in Granular Objects based on Squeeze-Induced Mechanical Vibrations&lt;/a&gt; by Togzhan Syrymova et al.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://arxiv.org/abs/1909.04787&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MAT: Multi-Fingered Adaptive Tactile Grasping via Deep Reinforcement Learning&lt;/a&gt; by Bohan Wu et al.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;http://arxiv.org/abs/1805.04201&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Learning to Grasp without Seeing&lt;/a&gt; by Adithyavairavan Murali et al.&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#&#34;&gt;Multimodal Haptic Perception within Granular Media via Recurrent Neural Networks&lt;/a&gt; by S.Jia et al.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;There are almost no works combining tactile sensing- exploration and grasping objects in granular media&lt;/p&gt;
&lt;h2 id=&#34;how&#34;&gt;How&lt;/h2&gt;
&lt;p&gt;Bringing such novel manipulation capabilities to robots calls for better robot hands as well as algorithms. The stated goals of the project are&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Building a novel tactile sensing capable gripper&lt;/li&gt;
&lt;li&gt;Develop AI techniques to make the robot learn how to explore, detect and grasp objects in unstructured-vision denied environments where tactile sensing only could be used.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The Tactile Sensing Gripper consists of a two finger parallel jaw gripper, with tactile sensing capabilities. Similar to the human fingers, the inner surface of the fingers would be having a high spatial resolution tactile sensing capability and the outer surface comparatively lower resolution. The highly sensitive inner surface of the finger helps in recognizing objects from their shape, texture as well as in fine manipulation skills. The lower sensitive outer surface helps in detecting the presence of nearby objects via contact.&lt;/p&gt;
&lt;p&gt;The AI System consists of a Deep Reinforcement Learning Agent that learns how to explore the environment and to recognize/differentiate objects and pick them up. The agent uses data from the two types of tactile sensors on each of the fingers, proprioception and force sensor data from the robot arm. The agent decides the direction of motion of the end effector and the control of the gripper.&lt;/p&gt;
&lt;p&gt;As a relatively simple task to start with, grasping specific objects from an environment similar to the following setup will have to be acheived using the tactile sensing alone.&lt;/p&gt;





&lt;img src=&#34;https://achuwilson.github.io/project/2020-blindgrasp/pick1_hu5216c571185a4b1be355b78cbb1a0511_203481_450x450_fit_lanczos_2.png&#34; width=&#34;415&#34; height=&#34;450&#34; alt=&#34;picking task 1&#34;&gt;

&lt;p&gt;Ultimately, the robot is expected to learn how to pick up objects under the granular media as in the following simulation.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/a2rk8dN3KsA&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;status&#34;&gt;Status&lt;/h2&gt;
&lt;p&gt;A novel gripper, with tactile sensing around the finger is built and we are collecting data from the real world environment. This would be augmented with data from the simulated environment and human demonstrations to train the RL agent for the task.&lt;/p&gt;
&lt;p&gt;More updates coming soon, as it is an ongoing project..!&lt;/p&gt;
&lt;/DIV&gt;
</description>
    </item>
    
    <item>
      <title>Background segmentation</title>
      <link>https://achuwilson.github.io/project/2020-fingervision-seg/</link>
      <pubDate>Fri, 27 Mar 2020 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2020-fingervision-seg/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#previous-works&#34;&gt;Previous works&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#how&#34;&gt;How&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#result&#34;&gt;Result&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;/h2&gt;
&lt;p&gt;FingerVision is an optical tactile sensor
&lt;a href=&#34;https://ieeexplore.ieee.org/document/7803400&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;, which provides the RGB image of the object within the fingers in addition to the contact forces from the optical markers.&lt;/p&gt;
&lt;p&gt;Since the sensor captures the image of the background also in addition to the object, we need to seperate out the object and the background.&lt;/p&gt;
&lt;h2 id=&#34;previous-works&#34;&gt;Previous Works&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;to update later&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&#34;how&#34;&gt;How&lt;/h2&gt;
&lt;p&gt;A deep learning approach is used to segment the objects and the background. Training data is collected, manually labelled and a U-Net based network is trained.
The training acheives good accuracy in a couple of tens of minutes. Transfer learning methods could be used to quickly adapt to new environments.&lt;/p&gt;
&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;
&lt;p&gt;It works, as expected.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/XZOl3pS5oLY&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;The left side shows the raw sensor image and the right one is the segmented image output.&lt;/p&gt;
&lt;/DIV&gt;
</description>
    </item>
    
    <item>
      <title>Gelsight for Sim2Real</title>
      <link>https://achuwilson.github.io/project/2019-gelsight_sim2real/</link>
      <pubDate>Sat, 27 Apr 2019 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2019-gelsight_sim2real/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#previous-works&#34;&gt;Previous works&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#what-makes-it-special&#34;&gt;What makes it special&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#result&#34;&gt;Result&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;/h2&gt;
&lt;p&gt;This work explores simulating Gelsight so that agents can be trained in simulation and transferred to real hardware systems with minimal/no difficulties. This would require high fidelity simulation of the GelSight sensor.&lt;/p&gt;
&lt;h2 id=&#34;previous-works&#34;&gt;Previous Works&lt;/h2&gt;
&lt;p&gt;My initial approach
&lt;a href=&#34;#&#34;&gt;[1]&lt;/a&gt; in simulating Gelsight used raytracing techniques to recreate the 3d pointcloud of the contact surface geometry. In reality, a Poisson solver based surface reconstruction is used to reconstruct the depth image and normals from the gradient image.&lt;/p&gt;
&lt;h2 id=&#34;what-makes-it-special&#34;&gt;What makes it special&lt;/h2&gt;
&lt;p&gt;This approach simulates the raw gradient RGB image coming from the sensor.&lt;/p&gt;
&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;
&lt;p&gt;The following image shows a comparison of the simulated image and the image acquired from the real sensor on similar objects.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;sim2real.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The top row of images are acquired from the hardware and the bottom one is the simulated one.&lt;/p&gt;
&lt;p&gt;Here is a 
&lt;a href=&#34;https://achuwilson.github.io/pdf/2019-sim2real-icra.pdf&#34;&gt;paper&lt;/a&gt; detailing the approach, and presented at ICRA 2019 ViTac Workshop.&lt;/p&gt;
&lt;/DIV&gt;
</description>
    </item>
    
    <item>
      <title>Gelsight Simulation 1</title>
      <link>https://achuwilson.github.io/project/2017-gelsight_sim/</link>
      <pubDate>Mon, 27 Nov 2017 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2017-gelsight_sim/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#how&#34;&gt;How&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#result&#34;&gt;Result&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;/h2&gt;
&lt;p&gt;This is my initial approach in modelling the behaviour of GelSight tactile sensors in simulation. I resorted to simulation as I did not have access to the real sensor or resources to build one.&lt;/p&gt;
&lt;h2 id=&#34;how&#34;&gt;How&lt;/h2&gt;
&lt;p&gt;I used the physics simulator Bullet. It had softbody simulation, which I thought would be useful for simulating the elastomer of GelSight. But it turned out that the softbody simulation was a basic and needed much more development. So, I indirectly modelled Gelsight using the raytest functionality in bullet. It returns the depth at which a ray makes contact with a solid body.&lt;/p&gt;
&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;
&lt;p&gt;Following is the video of the simulated gelsight gripper. The Kuka iiwa has a WSG 50 gripper fitted with a gelsight sensor. The sensor has a sensing area of 24x24mm and has a resolution of 256x256 pixels in the planar sensing area. It outputs standard ROS 3D pointcloud data, which is displayed in RViz.The simulation was run in my laptop and the pointcloud could be generated at 5Hz

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/IO02smLcDQE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;p&gt;Even though I could simulate the sensor, I soon ran into comptational limits of my laptop. &lt;/DIV&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Quadruped with Active spine</title>
      <link>https://achuwilson.github.io/project/2017-activespine_quadruped/</link>
      <pubDate>Sat, 27 May 2017 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2017-activespine_quadruped/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#previous-works&#34;&gt;Previous Works&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#results&#34;&gt;Results&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#status&#34;&gt;Status&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;/h2&gt;
&lt;p&gt;This is a side project to explore whether quadrupeds can be made to land on all four legs safely after they are dropped/thrown in random orientations.&lt;/p&gt;
&lt;h2 id=&#34;previous-works&#34;&gt;Previous Works&lt;/h2&gt;
&lt;p&gt;
&lt;a href=&#34;https://en.wikipedia.org/wiki/Cat_righting_reflex&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Cat righting reflex&lt;/a&gt; been extensively studied before. In 1942 US Air Force played with kittens in microgravity. The same lab, in 1962, published a report titled 
&lt;a href=&#34;http://www.dtic.mil/dtic/tr/fulltext/u2/400354.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Weightless Man: Self-Rotation Techniques&lt;/a&gt;, as a guidline for future astronauts on how to move around in zero gravity. 1960s. The falling cat was also studiedextensively by NASA to prepare the astronauts for Zero-G environments. A 1969 paper titled 
&lt;a href=&#34;http://www.sciencedirect.com/science/article/pii/0020768369900869&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;A Dynamical Explanation of the Falling Cat Phenomenon&lt;/a&gt; explains this.&lt;/p&gt;
&lt;p&gt;Past research attributes the cat reflex to law of conservation of momentem. Flexible bodies can generate internal forces and moments, by the motion of limbs. But the rest of the body rotates in opposite direction owing to the conservation of momentum, so that net angular momentum remains zero. However the rate of rotation can be controlled by extending or pulling back the limbs. Wikipedia summarises it to three key steps:&lt;/p&gt;
&lt;p&gt;1.Bend in the middle so that the front half of their body rotates about a different axis from the rear half.
2.Tuck their front legs in to reduce the moment of inertia of the front half of their body and extend their rear legs to increase the moment of inertia of the rear half of their body so that they can rotate their front further (as much as 90°) while the rear half rotates in the opposite direction less (as little as 10°).
3.Extend their front legs and tuck their rear legs so that they can rotate their rear half further while their front half rotates in the opposite direction less.&lt;/p&gt;
&lt;p&gt;Depending on the cat’s flexibility and initial angular momentum, if any, the cat may need to perform steps two and three repeatedly in order to complete a full 180° rotation.&lt;/p&gt;
&lt;p&gt;Past works on using active spine in legged robots specialized rotation in the axis perpendicular to the saggital plane and was aimed at improving the gait. My research focusess on rotation in the axis perpendicular to the transverse plane. I developed an inertial reoreintation controller. It controls the extension and tucking in of the quadruped limbs, so that the cranial and caudal part rotates with different velocities owing to the conserved momentum. The controller thus uses the inertia of the quadruped to reorient itself.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;A quadruped was modelled in the Bullet simulator and a simple state machine based controller was implemented. It works beautifully as in the following video&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/4Hg29l2iCJo&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;h2 id=&#34;status&#34;&gt;Status&lt;/h2&gt;
&lt;p&gt;As the system is verified in simulation, now it has to be implemented in hardware.
I am working on a small quadruped during freetime and it will be updated later.&lt;/p&gt;
&lt;p&gt;Here are some photos of it in progress.&lt;/p&gt;
&lt;p&gt;




&lt;img src=&#34;https://achuwilson.github.io/project/2017-activespine_quadruped/quad1_hub3d4f15a59a31c668d66105d0dde93e6_333603_450x450_fit_q90_lanczos.jpg&#34; width=&#34;450&#34; height=&#34;338&#34; alt=&#34;quadruped image 1&#34;&gt;






&lt;img src=&#34;https://achuwilson.github.io/project/2017-activespine_quadruped/quad2_hu86232e7e403934dbd3ee3c0182a4e2fe_299174_450x450_fit_q90_lanczos.jpg&#34; width=&#34;450&#34; height=&#34;338&#34; alt=&#34;quadruped image 2&#34;&gt;
&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;p&gt;will be updated later as time permits.&lt;/p&gt;
&lt;/DIV&gt;
</description>
    </item>
    
    <item>
      <title>2048 Solver Robot</title>
      <link>https://achuwilson.github.io/project/2015-2048/</link>
      <pubDate>Tue, 27 Oct 2015 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2015-2048/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#How&#34;&gt;How&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#result&#34;&gt;Result&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;/h2&gt;
&lt;p&gt;2048
&lt;a href=&#34;https://en.wikipedia.org/wiki/2048_%28video_game%29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;[1]&lt;/a&gt;  is a single-player sliding block puzzle game. The objective of the game is to slide numbered tiles on a grid to combine them to create a tile with the number 2048.&lt;/p&gt;
&lt;p&gt;This was a weekend hobby project, to make use of a robot arm, computer vision and AI techniques to solve and play the game.&lt;/p&gt;
&lt;h2 id=&#34;how&#34;&gt;How&lt;/h2&gt;
&lt;p&gt;The state of the game is inferred from the smartphone screen. Screenshots are perodically captured via the ADB interface. The image is pre-processed and an OCR is done to detect the digits on each tile.&lt;/p&gt;
&lt;p&gt;Given the current game state, an alpha-beta pruning algorithm predicts the next best action. The action space is swipe in Left/Right/Up/Bottom directions. The robot arm then would move towards the screen and make the swipe&lt;/p&gt;
&lt;h2 id=&#34;result&#34;&gt;Result&lt;/h2&gt;
&lt;p&gt;See for yourself!.&lt;/p&gt;

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/hUNTeZJSUWE&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;

&lt;p&gt;It not only wins the game, but also scores unbelievable scores.&lt;/p&gt;
&lt;/DIV&gt;
</description>
    </item>
    
    <item>
      <title>Autonomous Drone Landing</title>
      <link>https://achuwilson.github.io/project/2015-quadrotor_landing/</link>
      <pubDate>Mon, 27 Apr 2015 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2015-quadrotor_landing/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#results&#34;&gt;Results&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;/h2&gt;
&lt;p&gt;This was a small project done for the qualification round of MBZIRC Challenge 2015. The drone would lift off, move in a circular pattern with incrasing readius and searching for the marker using the downward facing camera. Once the marker is localized, a simple path planner is used to plan the trajectory to land the drone on the rover.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;Even though we didnt make it to the finals, it was a good learning experience.&lt;/p&gt;
&lt;p&gt;Here is a video of it.

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/QIklrzh8k6U&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/DIV&gt;
</description>
    </item>
    
    <item>
      <title>Smart Notice Board</title>
      <link>https://achuwilson.github.io/project/2009-smart_notice_board/</link>
      <pubDate>Sat, 27 Mar 2010 00:00:00 +0000</pubDate>
      <guid>https://achuwilson.github.io/project/2009-smart_notice_board/</guid>
      <description>&lt;DIV align=&#34;justify&#34;&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;a href=&#34;#what-and-why&#34;&gt;What and why&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#how&#34;&gt;How&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;
&lt;a href=&#34;#results&#34;&gt;Results&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id=&#34;what-and-why&#34;&gt;What and Why&lt;/h2&gt;
&lt;p&gt;This is another hobby project. Any display solution with a VGA input can be used a the notice board. Users text the messages to be displayed, along with an access code to a predefined number. The system would them display the messgae on the display&lt;/p&gt;
&lt;h2 id=&#34;how&#34;&gt;How&lt;/h2&gt;
&lt;p&gt;The system is built around an AVR microcontroller and a GSM modem. The GSM modem has a SIM card provided by the netork provider and can receive text messages. The microcontroller controls the modem and reads the incomming messages through AT commands over a serial port. The user has to send the text message prepended with the security access code. This prevents unauthorized people from spamming the system,&lt;/p&gt;
&lt;p&gt;Once the text to be displayed is received and authenticated. The AVR microcontroller sends it over to the display as a VGA signal. Even though the tiny AVR controller does not have a VGA port, VGA signals are generated by bitbanging through the SPI port.&lt;/p&gt;
&lt;h2 id=&#34;results&#34;&gt;Results&lt;/h2&gt;
&lt;p&gt;Here is a video of the system in operation.

&lt;div style=&#34;position: relative; padding-bottom: 56.25%; height: 0; overflow: hidden;&#34;&gt;
  &lt;iframe src=&#34;https://www.youtube.com/embed/sg7snEejMzI&#34; style=&#34;position: absolute; top: 0; left: 0; width: 100%; height: 100%; border:0;&#34; allowfullscreen title=&#34;YouTube Video&#34;&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/p&gt;
&lt;/DIV&gt;
</description>
    </item>
    
  </channel>
</rss>
